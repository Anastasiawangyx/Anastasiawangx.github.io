<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>改善神经网络的学习方法（交叉熵函数，过度拟合，规范化，初始化权重） | Ana's blog</title><meta name="keywords" content="network,python"><meta name="author" content="Anastasia"><meta name="copyright" content="Anastasia"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="写在前面  此文是我学习ML入门的笔记。学习教材《neural networks and deep learning》，作者Michael Nielsen。 这是一本免费的书籍，网址在这里。 此文是第三章的学习笔记。 初学者入门， 可能会有错误，请大家指正。 第一章笔记和第二章笔记的链接。 ———————————————————————————————————  为什么要改善？ 要改善我们之前的神">
<meta property="og:type" content="article">
<meta property="og:title" content="改善神经网络的学习方法（交叉熵函数，过度拟合，规范化，初始化权重）">
<meta property="og:url" content="http://example.com/2020/09/11/2020-09-11-%E6%94%B9%E5%96%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%A4%E5%8F%89%E7%86%B5%E5%87%BD%E6%95%B0%EF%BC%8C%E8%BF%87%E5%BA%A6%E6%8B%9F%E5%90%88%EF%BC%8C%E8%A7%84%E8%8C%83%E5%8C%96%EF%BC%8C%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9D%83%E9%87%8D%EF%BC%89/index.html">
<meta property="og:site_name" content="Ana&#39;s blog">
<meta property="og:description" content="写在前面  此文是我学习ML入门的笔记。学习教材《neural networks and deep learning》，作者Michael Nielsen。 这是一本免费的书籍，网址在这里。 此文是第三章的学习笔记。 初学者入门， 可能会有错误，请大家指正。 第一章笔记和第二章笔记的链接。 ———————————————————————————————————  为什么要改善？ 要改善我们之前的神">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://mdimage001.oss-cn-beijing.aliyuncs.com/img/v2-fc6b5bc5ed17621fa52acf7a19b2e379_1440w.jpg">
<meta property="article:published_time" content="2020-09-11T08:34:00.000Z">
<meta property="article:modified_time" content="2020-10-03T12:39:28.899Z">
<meta property="article:author" content="Anastasia">
<meta property="article:tag" content="network">
<meta property="article:tag" content="python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://mdimage001.oss-cn-beijing.aliyuncs.com/img/v2-fc6b5bc5ed17621fa52acf7a19b2e379_1440w.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2020/09/11/2020-09-11-%E6%94%B9%E5%96%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%A4%E5%8F%89%E7%86%B5%E5%87%BD%E6%95%B0%EF%BC%8C%E8%BF%87%E5%BA%A6%E6%8B%9F%E5%90%88%EF%BC%8C%E8%A7%84%E8%8C%83%E5%8C%96%EF%BC%8C%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9D%83%E9%87%8D%EF%BC%89/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><script>var GLOBAL_CONFIG = { 
  root: '/',
  hexoversion: '5.1.1',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  ClickShowText: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2020-10-03 20:39:28'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {
  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }

  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }
})()</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.1.1"><link rel="alternate" href="/atom.xml" title="Ana's blog" type="application/atom+xml">
</head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">16</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">7</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">4</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div></div></div><div id="body-wrap"><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">写在前面</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">为什么要改善？</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%9C%A8%E4%B8%80%E5%AE%9A%E6%9D%A1%E4%BB%B6%E4%B8%8B%E5%AD%A6%E4%B9%A0%E7%BC%93%E6%85%A2%E3%80%82"><span class="toc-number">2.1.</span> <span class="toc-text">1. 在一定条件下学习缓慢。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E8%BF%87%E5%BA%A6%E6%8B%9F%E5%90%88%EF%BC%88overfitting%EF%BC%89-%E8%BF%87%E5%BA%A6%E8%AE%AD%E7%BB%83%EF%BC%88overtraining%EF%BC%89"><span class="toc-number">2.2.</span> <span class="toc-text">2. 过度拟合（overfitting）&#x2F;过度训练（overtraining）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">2.3.</span> <span class="toc-text">3.权重初始化</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">如何改善？</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%BA%A4%E5%8F%89%E7%86%B5%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%EF%BC%88cross-entropy-cost-function%EF%BC%89"><span class="toc-number">3.1.</span> <span class="toc-text">1.交叉熵代价函数（cross-entropy cost function）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%9F%94%E6%80%A7%E6%9C%80%E5%A4%A7%E5%80%BC%EF%BC%88softmax%EF%BC%89"><span class="toc-number">3.2.</span> <span class="toc-text">2.柔性最大值（softmax）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%AF%B9%E6%95%B0%E4%BC%BC%E7%84%B6%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%EF%BC%88log-likelihood-cost-function%EF%BC%89"><span class="toc-number">3.3.</span> <span class="toc-text">3.对数似然代价函数（log-likelihood cost function）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E6%8F%90%E5%89%8D%E5%81%9C%E6%AD%A2%EF%BC%88early-stopping%EF%BC%89%E5%92%8Chold-out%E6%96%B9%E6%B3%95"><span class="toc-number">3.4.</span> <span class="toc-text">4.提前停止（early stopping）和hold-out方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-L2%E8%A7%84%E8%8C%83%E5%8C%96%EF%BC%88regularization%EF%BC%89"><span class="toc-number">3.5.</span> <span class="toc-text">5. L2规范化（regularization）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9B%B4%E5%B0%8F%E7%9A%84%E6%9D%83%E9%87%8D%E5%8F%AF%E4%BB%A5%E5%87%8F%E8%BD%BB%E8%BF%87%E5%BA%A6%E6%8B%9F%E5%90%88%EF%BC%9F"><span class="toc-number">3.5.1.</span> <span class="toc-text">为什么更小的权重可以减轻过度拟合？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-L1%E8%A7%84%E8%8C%83%E5%8C%96"><span class="toc-number">3.6.</span> <span class="toc-text">6. L1规范化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E5%BC%83%E6%9D%83%EF%BC%88dropout%EF%BC%89"><span class="toc-number">3.7.</span> <span class="toc-text">7.弃权（dropout）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-%E4%BA%BA%E4%B8%BA%E6%89%A9%E5%B1%95%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%EF%BC%88Artificially-expanding-the-training-data%EF%BC%89"><span class="toc-number">3.8.</span> <span class="toc-text">8.人为扩展训练数据（Artificially expanding the training data）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">3.9.</span> <span class="toc-text">9.权重初始化</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">如何选择神经网络的超参数（hyper-paramets）？</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%BD%E6%B3%9B%E7%AD%96%E7%95%A5%EF%BC%88broad-strategy%EF%BC%89"><span class="toc-number">4.1.</span> <span class="toc-text">宽泛策略（broad strategy）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%AE%80%E5%8C%96%E9%9C%80%E8%A6%81%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">4.1.1.</span> <span class="toc-text">1.简化需要解决的问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E7%AE%80%E5%8C%96%E7%BD%91%E7%BB%9C%E6%9D%A5%E5%8A%A0%E9%80%9F%E5%AE%9E%E9%AA%8C"><span class="toc-number">4.1.2.</span> <span class="toc-text">2.简化网络来加速实验</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%87%8F%E5%B0%91%E9%AA%8C%E8%AF%81%E5%9B%BE%E5%83%8F%E6%95%B0"><span class="toc-number">4.1.3.</span> <span class="toc-text">3.减少验证图像数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E9%80%9F%E7%8E%87"><span class="toc-number">4.2.</span> <span class="toc-text">学习速率</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E6%8F%90%E5%89%8D%E5%81%9C%E6%AD%A2%E6%9D%A5%E7%A1%AE%E5%AE%9A%E8%AE%AD%E7%BB%83%E7%9A%84%E8%BF%AD%E4%BB%A3%E6%9C%9F%E6%95%B0%E9%87%8F"><span class="toc-number">4.3.</span> <span class="toc-text">使用提前停止来确定训练的迭代期数量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%AF%E5%8F%98%E5%AD%A6%E4%B9%A0%E9%80%9F%E7%8E%87"><span class="toc-number">4.4.</span> <span class="toc-text">可变学习速率</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%84%E8%8C%83%E5%8C%96%E5%8F%82%E6%95%B0%CE%BB"><span class="toc-number">4.5.</span> <span class="toc-text">规范化参数λ</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E6%89%B9%E9%87%8F%E6%95%B0%E6%8D%AE%E5%A4%A7%E5%B0%8F%EF%BC%88mini-batch-size%EF%BC%89"><span class="toc-number">4.6.</span> <span class="toc-text">小批量数据大小（mini-batch size）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">其他技术</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Hessian%E6%8A%80%E6%9C%AF"><span class="toc-number">5.1.</span> <span class="toc-text">1.Hessian技术</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%9F%BA%E4%BA%8E-momentum-%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">5.2.</span> <span class="toc-text">2.基于 momentum 的梯度下降</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">6.</span> <span class="toc-text">人工神经元的其他模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-tanh%E7%A5%9E%E7%BB%8F%E5%85%83"><span class="toc-number">6.1.</span> <span class="toc-text">1.tanh神经元</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E4%BF%AE%E6%AD%A3%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E5%85%83%EF%BC%88rectified-linear-neuron%EF%BC%89"><span class="toc-number">6.2.</span> <span class="toc-text">2.修正线性神经元（rectified linear neuron）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">7.</span> <span class="toc-text">小总结</span></a></li></ol></div></div></div><header class="post-bg" id="page-header" style="background-image: url(https://mdimage001.oss-cn-beijing.aliyuncs.com/img/v2-fc6b5bc5ed17621fa52acf7a19b2e379_1440w.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Ana's blog</a></span><span id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">改善神经网络的学习方法（交叉熵函数，过度拟合，规范化，初始化权重）</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-09-11T08:34:00.000Z" title="发表于 2020-09-11 16:34:00">2020-09-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2020-10-03T12:39:28.899Z" title="更新于 2020-10-03 20:39:28">2020-10-03</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/network/">network</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>13分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h1>写在前面</h1>
<ol>
<li>此文是我学习ML入门的笔记。学习教材《neural networks and deep learning》，作者Michael Nielsen。</li>
<li>这是一本免费的书籍，网址<a target="_blank" rel="noopener" href="http://neuralnetworksanddeeplearning.com/">在这里</a>。</li>
<li>此文是第三章的学习笔记。</li>
<li>初学者入门， 可能会有错误，请大家指正。</li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/Anastasiawangyx/article/details/102649158">第一章笔记</a>和<a target="_blank" rel="noopener" href="https://blog.csdn.net/Anastasiawangyx/article/details/102753879">第二章笔记</a>的链接。<br>
———————————————————————————————————</li>
</ol>
<h1>为什么要改善？</h1>
<p>要改善我们之前的神经网络，是因为它存在着以下几个问题：</p>
<h2 id="1-在一定条件下学习缓慢。">1. 在一定条件下学习缓慢。</h2>
<p>在我们之前用二次代价函数、随机梯度下降和反向传播算法定义的神经网络中，网络的学习速率主要取决于<strong>权重的偏导数</strong>，其表达式为</p>
<p><img src="https://img-blog.csdnimg.cn/20191102224201707.JPG" alt="权重偏导数"></p>
<p>因此权重的偏导数正比于<strong>S型函数的一阶导数</strong>。而其一阶导数的图像如下所示</p>
<p><img src="https://img-blog.csdnimg.cn/20191102224449106.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="S型函数的导数图像"></p>
<p>可以看出，当z的值很大或者很小的时候，学习速率会非常缓慢。</p>
<h2 id="2-过度拟合（overfitting）-过度训练（overtraining）">2. 过度拟合（overfitting）/过度训练（overtraining）</h2>
<p>如何理解过度拟合呢？</p>
<p>打一个简单的比方：你很喜欢吃土豆丝这道菜，所以妈妈会你如何把土豆去皮切成丝。你每天很勤奋的练习，最后可以切出粗细均匀的完美的土豆丝了。<br>
然后有一天你吃腻了土豆丝，想吃胡萝卜。<br>
但是你看着眼前的胡萝卜，不知道该如何下刀。<br>
因为，切丝这个学习行为，你只能把它运用在土豆这个<strong>特定的训练对象</strong>上，而不能<strong>泛化(generalization)</strong> 到其他诸如胡萝卜等的身上，这样的学习是非常无效的。</p>
<p>回到神经网络。<br>
我的理解，过度拟合指的是<strong>训练得出的权重、偏置只能在训练数据上得到很好的表现，而在真实数据（如测试数据、验证集）上则无法体现出学习的行为。</strong><br>
过度拟合常常发生在<strong>拥有大量自由参数</strong>的模型上，比如神经网络。</p>
<p>举个例子。<br>
我们只使用前1000个训练数据，设置400个迭代期，其他条件不变，则得到的代价函数看起来还不错。<br>
<img src="https://img-blog.csdnimg.cn/2019110313434342.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="训练集"></p>
<p>但分类准确率在测试集上的表现，就…<br>
<img src="https://img-blog.csdnimg.cn/20191103134807526.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="测试集准确率"><br>
过度拟合现象发生了。</p>
<h2 id="3-权重初始化">3.权重初始化</h2>
<p>在前面的学习中，我们对权重的初始化一直是用<strong>高斯随机变量</strong>，使其被归一化为均值为0，标准差为1。而这种情况下，带权输入z的值会如下分布<br>
<img src="https://img-blog.csdnimg.cn/20191103140436426.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
可以看出，z的绝对值会很大，所以神经元会趋向<strong>饱和</strong>。<br>
———————————————————————————————————</p>
<h1>如何改善？</h1>
<h2 id="1-交叉熵代价函数（cross-entropy-cost-function）">1.交叉熵代价函数（cross-entropy cost function）</h2>
<p>对于单个如下形式的神经元<br>
<img src="https://img-blog.csdnimg.cn/20191103141740224.png" alt="在这里插入图片描述"></p>
<p>其对应的交叉熵代价函数为<br>
<img src="https://img-blog.csdnimg.cn/20191103141856584.JPG" alt="在这里插入图片描述"><br>
其中 n 是训练数据的总数，求和是在所有的训练输⼊ x 上进⾏的，y 是对应的⽬标输出。<br>
而由此计算出的权重的偏导数的形式为<br>
<img src="https://img-blog.csdnimg.cn/20191103143113849.JPG" alt="在这里插入图片描述"><br>
由此避免了学习速率下降的问题。</p>
<p>定义在<strong>神经网络</strong>上的交叉熵函数的形式为：<br>
<img src="https://img-blog.csdnimg.cn/20191103144048460.JPG" alt="在这里插入图片描述"><br>
可以看成在所有的输出神经元上，对上上式求和。</p>
<p>其权重的偏导数形式为<br>
<img src="https://img-blog.csdnimg.cn/20191103145211408.JPG" alt="在这里插入图片描述"><br>
很好的避免了学习速率下降的问题。</p>
<h2 id="2-柔性最大值（softmax）">2.柔性最大值（softmax）</h2>
<p>柔性最大值的想法是为神经网络定义一种新式的输出层。刚开始在输入层和隐藏层上都是一样的，但是在输出层上应用一种<strong>柔性最大值函数</strong>作用在带权输入zjL上。根据这个函数，第j个神经元的激活值ajL就是<br>
<img src="https://img-blog.csdnimg.cn/2019110316302323.JPG" alt="在这里插入图片描述"><br>
其中，分母是对所有输出层上的神经元的求和。因此输出层上<strong>所有激活值和为1.</strong> 换言之，柔性最大层的输出可以用看作是一个<strong>概率分布</strong>。</p>
<h2 id="3-对数似然代价函数（log-likelihood-cost-function）">3.对数似然代价函数（log-likelihood cost function）</h2>
<p>我们使⽤ x 表⽰⽹络的训练输⼊，y 表⽰对应的⽬标输出。然后关<br>
联这个训练输⼊的对数似然代价函数就是<br>
<img src="https://img-blog.csdnimg.cn/20191103164239993.JPG" alt="在这里插入图片描述"><br>
通过代数运算我们可以得到<br>
<img src="https://img-blog.csdnimg.cn/20191103164445572.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
运用柔性最大值函数与对数似然代价函数的结合，也可以很好的解决学习速率下降的问题。</p>
<h2 id="4-提前停止（early-stopping）和hold-out方法">4.提前停止（early stopping）和hold-out方法</h2>
<p>我们之前定义过一个<strong>validation_data（验证集）</strong>，它是从测试集中单独拿出的一批数据，我们还没有使用过它。</p>
<p>现在我们使用验证集来解决过度拟合问题：</p>
<p>在每轮迭代期之后，我们使用validation_data来测试其分类准确率，一旦准确率达到饱和，我们就停止训练。这个策略叫做<strong>提前停止</strong>。</p>
<p>当然，实际应⽤中，我们不会⽴即知道什么时候准确率会饱和。相反，我们会⼀直训练直到我们确信准确率已经饱和。</p>
<p>就是为何⽤ validation_data 取代 test_data 来设置更好的超参数？因为如果我们设置超参数是基于 test_data 的话，可能最终我们就会得到过度拟合于test_data 的超参数。也就是说，我们可能会找到那些符合test_data 特点的超参数，但是<strong>⽹络的性能并不能够泛化到其他数据集合上</strong>。我们借助 validation_data 来克服这个问题。然后⼀旦获得了想要的超参数，最终我们就使⽤ test_data 进⾏准确率测量。<br>
换⾔之，你可以将验证集看成是⼀种特殊的训练数据集能够帮助我们学习好的超参数。这种寻找好的超参数的⽅法有时候被称为 <strong>hold out ⽅法</strong>，因为 validation_data是从traning_data 训练集中留出或者“拿出”的⼀部分。</p>
<h2 id="5-L2规范化（regularization）">5. L2规范化（regularization）</h2>
<p>所谓的L2规范化，就是在代价函数上增加一个额外的项，这个项叫<strong>规范项</strong>。下面以交叉熵代价函数举例说明。<br>
规范化后的交叉熵代价函数为<br>
<img src="https://img-blog.csdnimg.cn/2019110320264041.JPG" alt="在这里插入图片描述"><br>
其中第⼀个项就是常规的交叉熵的表达式。第⼆个现在加⼊的就是<strong>所有权重的平⽅的和</strong>。然后使⽤⼀个因⼦ λ/2n 进⾏量化调整，其中 λ &gt; 0 可以称为<strong>规范化参数</strong>，⽽ n 就是训练集合的大小。<br>
在这种情况下，求得的权重和偏置的偏导数为<br>
<img src="https://img-blog.csdnimg.cn/20191103203444593.JPG" alt="在这里插入图片描述"><br>
因此每次更新权重时的公式为<br>
<img src="https://img-blog.csdnimg.cn/20191103203520414.JPG" alt="在这里插入图片描述"><br>
这和通常的梯度下降的规则相似，除了通过一个因子（1−ηλ÷n1-ηλ \div n1−<em>η**λ</em>÷<em>n</em>）调整了权重w。这种做法有时也叫做权重下降（<strong>weights decay</strong>），因为它使权重变得更小。</p>
<h3 id="为什么更小的权重可以减轻过度拟合？">为什么更小的权重可以减轻过度拟合？</h3>
<p>我的理解</p>
<ol>
<li>小的权重在某种程度上，意味着更低的复杂性，也就对数据做出了一种更简单，更强大的解释。</li>
<li>更小的权重意味着网络的行为不会因为我们随便改变了一个输入而改变太大，因此能够抵抗训练数据中噪声的影响。</li>
<li>小权重神经网络给出的是一个更“一般”而“通用”的解释，能够根据已经学到的内容进行更好的泛化。</li>
</ol>
<h2 id="6-L1规范化">6. L1规范化</h2>
<p>与L2规范化类似，L1规范化是在在未规范化的代价函数上加上⼀个权重绝对值的和：<br>
<img src="https://img-blog.csdnimg.cn/20191103205038212.JPG" alt="在这里插入图片描述"><br>
在两种情形下，规范化的效果就是缩小权重。但权重缩小的方式不同。</p>
<ul>
<li>在 L1 规范化中，权重通过⼀个常量向 0 进行缩小。</li>
<li>在 L2 规范化中，权重通过⼀个和 w 成比例的量进行缩小的。</li>
<li>所以，当⼀个特定的权重绝对值 |w| 很⼤时，L1 规范化的权重缩⼩得远⽐ L2 规范化要⼩得多。</li>
<li>相反，当⼀个特定的权重绝对值 |w| 很⼩时，L1 规范化的权重缩⼩得要⽐ L2 规范化⼤得多。</li>
<li>最终的结果就是：L1 规范化倾向于聚集⽹络的权重在相对少量的⾼重要度连接上，⽽其他权重就会被驱使向 0 接近。</li>
</ul>
<h2 id="7-弃权（dropout）">7.弃权（dropout）</h2>
<p><img src="https://img-blog.csdnimg.cn/20191103210038717.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
原理：随机删除一半的隐藏神经元，用这个‘’新‘’的网络进行学习。</p>
<p>注：输入层和输出层不变。</p>
<p><strong>理解</strong>：</p>
<ul>
<li>弃权，是一种构造大量不同神经网络的过程，让数据在不同的网络中学习，最后求平均值。</li>
<li>确保模型对一部分证据丢失健壮。</li>
</ul>
<p><strong>注：</strong></p>
<ol>
<li>输入层和输出层不变</li>
<li>最后运用实际整个神经网络时，隐藏神经元的权重要减半。</li>
</ol>
<h2 id="8-人为扩展训练数据（Artificially-expanding-the-training-data）">8.人为扩展训练数据（Artificially expanding the training data）</h2>
<p>很容易理解，神经网络的训练数据越多，网络能够学习到的变化就越大，性能自然就越好，但是在真实状态下，<strong>好的训练数据是很难拿到的。</strong></p>
<p>但是我们可以<strong>人为扩展训练数据</strong>。</p>
<p>以手写识别数字为例。</p>
<p>我们的训练数据是MNIST训练图像<br>
<img src="https://imgconvert.csdnimg.cn/aHR0cDovL25ldXJhbG5ldHdvcmtzYW5kZGVlcGxlYXJuaW5nLmNvbS9pbWFnZXMvbW9yZV9kYXRhXzUucG5n?x-oss-process=image/format,png" alt="在这里插入图片描述"></p>
<p>将其进行<strong>旋转</strong>，比如说15度。<br>
<img src="http://neuralnetworksanddeeplearning.com/images/more_data_rotated_5.png" alt="img"></p>
<p>这还是会被设别为同样的数字的。但是在<strong>像素层级</strong>这和任何⼀幅在 MNIST 训练数据中的图像都不相同。所以将这样的样本加⼊到训练数据中是很可能帮助我们的⽹络学会更多如何分类数字。<br>
不只<strong>旋转</strong>，还可以用<strong>转换</strong>和<strong>扭曲</strong>图像来扩展训练数据。</p>
<h2 id="9-权重初始化">9.权重初始化</h2>
<p>我们选用一种新的方法初始化权重。</p>
<p>假设我们有⼀个有 n个输⼊权重的神经元。我们会使⽤<strong>均值为 0 标准差为 1/√n</strong>的⾼斯随机分布初始化这些权重。</p>
<p>也就是说，我们会向下挤压⾼斯分布，让我们的神经元更不可能饱和。</p>
<p><img src="https://img-blog.csdnimg.cn/20191103234259338.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
这样的⼀个神经元<strong>更不可能饱和</strong>，因此也不⼤可能遇到学习速度下降的问题。</p>
<p>———————————————————————————————————</p>
<h1>如何选择神经网络的超参数（hyper-paramets）？</h1>
<h2 id="宽泛策略（broad-strategy）"><em>宽泛策略（broad strategy）</em></h2>
<h3 id="1-简化需要解决的问题">1.简化需要解决的问题</h3>
<p>丢开训练和验证集合中的那些除了 0 和 1的那些图像。然后试着训练⼀个⽹络来区分 0 和 1。不仅仅问题⽐ 10 个分类的情况简化了，同样也会减少 80% 的训练数据，这样就给出了 5 倍的加速。</p>
<h3 id="2-简化网络来加速实验">2.简化网络来加速实验</h3>
<p>如果你相信 [784, 10] 的⽹络更可能⽐随机更加好的分类效果，那么就从这个⽹络开始实验。这会比训练⼀个 [784, 30, 10] 的⽹络更快，你可以进⼀步尝试后⼀个。</p>
<h3 id="3-减少验证图像数">3.减少验证图像数</h3>
<p>减少每一轮迭代期的验证图像数，能够通过更加频繁的监控准确率而获得反馈。</p>
<p>下面对具体的参数进行分析。</p>
<h2 id="学习速率">学习速率</h2>
<p>首先要清楚，我们使用随机梯度下降算法的目的，是希望我们能够逐渐抵达<strong>代价函数谷底</strong>的。<br>
<img src="https://imgconvert.csdnimg.cn/aHR0cDovL25ldXJhbG5ldHdvcmtzYW5kZGVlcGxlYXJuaW5nLmNvbS9pbWFnZXMvdGlrejMzLnBuZw?x-oss-process=image/format,png" alt="在这里插入图片描述"></p>
<ul>
<li>学习速率太小：会导致学习速率过慢，造成浪费。</li>
<li>学习速率过大：可能会导致<strong>算法在接近最小值的时候又越过了谷底</strong>，造成震荡。<br>
那应该如何设置学习速率呢？</li>
</ul>
<ol>
<li>
<p>我们选择在训练数据上的代价<strong>立即开始下降</strong>而非震荡或者增加时作为 η 的阈值的估计。</p>
<p>因为此时我们还在“山坡上”，还未到达谷底。</p>
</li>
<li>
<p>如果代价在训练的前⾯若⼲回合开始下降，你就可以逐步地尝试增加 η，直到你找到⼀个 η 的值使得在开始若⼲回合代价就开始震荡或者增加。</p>
</li>
</ol>
<p>为了避免疑惑，我将英文原版书中的原句放在此处。</p>
<blockquote>
<p>With this picture in mind, we can set η as follows. First, we estimate the threshold value for η at which the cost on the training data immediately begins decreasing, instead of oscillating or increasing. This estimate doesn’t need to be too accurate. You can estimate the order of magnitude by starting with η=0.01. If the cost decreases during the first few epochs, then you should successively try η=0.1,1.0,… until you find a value for η where the cost oscillates or increases during the first few epochs. Alternately, if the cost oscillates or increases during the first few epochs when η=0.01, then try η=0.001,0.0001,… until you find a value for η where the cost decreases during the first few epochs. Following this procedure will give us an order of magnitude estimate for the threshold value of η. You may optionally refine your estimate, to pick out the largest value of η at which the cost decreases during the first few epochs, say η=0.5 or η=0.2 (there’s no need for this to be super-accurate). This gives us an estimate for the threshold value of η.</p>
</blockquote>
<h2 id="使用提前停止来确定训练的迭代期数量">使用提前停止来确定训练的迭代期数量</h2>
<p>正如我们在前⾯讨论的那样，提前停止表⽰在每个迭代期的最后，我们都要计算验证集上的分类准确率。当准确率不再提升，就终止它。这让选择迭代期数变得很简单。</p>
<h2 id="可变学习速率">可变学习速率</h2>
<p>我们⼀直都将学习速率设置为常量。但是，通常采⽤<strong>可变的学习速率</strong>更加有<br>
效。在学习的前期，权重可能⾮常糟糕。所以最好是使⽤⼀个<strong>较大的学习速率让权重变化得更快</strong>。越往后，我们可以降低学习速率，这样可以作出<strong>更加精良的调整</strong>。<br>
关于如何设置可变学习速率，⼀种观点是使用提前终止的想法。就是保持学习速率为⼀个常量知道验证准确率开始变差。然后按照某个量下降学习速率，比如说按照 10 或者 2。我们重复此过程若⼲次，直到学习速率是初始值的 1/1024（或者 1/1000）。那时就终止。</p>
<h2 id="规范化参数λ">规范化参数λ</h2>
<p>书作者建议，开始时不包含规范化（λ = 0.0），<strong>确定 η 的值</strong>。使用确定出来的 η，我们可以使用验证数据来选择好的 λ。从尝试 λ = 1.0 开始，然后根据验证集上的性能按照因子10增加或减少其值。⼀旦我已经找到⼀个好的量级，你可以改进 λ 的值。这⾥搞定后，你就可以返回再重新优化 η。</p>
<h2 id="小批量数据大小（mini-batch-size）">小批量数据大小（mini-batch size）</h2>
<p>首先要指出，我们在网络中是使用矩阵技术来对所有在⼩批量数据中的样本同时计算梯度更新，而不是进行单个循环累加。</p>
<ul>
<li>size太小：没有办法发挥矩阵的优势。</li>
<li>size太大：无法频繁快速的更新权重。</li>
</ul>
<p>因此我们需要一个折中的方法。</p>
<p>幸运的是，小批量数据大小的选择其实是相对<strong>独立</strong>的⼀个超参数（网络整体架<br>
构外的参数），所以你不需要优化其他参数来寻找好的小批量数据大小。</p>
<p>所以，使用一个（可以接受的）初始值，然后进行不同小批量数据大小的尝试。画出验证准确率的值随时间（⾮迭代期）变化的图，选择哪个得到最快性能的提升的小批量数据大小。<br>
———————————————————————————————————</p>
<h1>其他技术</h1>
<h2 id="1-Hessian技术">1.Hessian技术</h2>
<p>Hessian技术是一个用来<strong>最小化代价函数</strong>的方法。<br>
假设 C 是一个有多个参数的函数，w = w1, w2, …，所以 C = C(w)。<br>
因此得到<br>
<img src="https://img-blog.csdnimg.cn/20191104003356887.JPG" alt="在这里插入图片描述"><br>
其中 ∇C 是通常的梯度向量，H就是Hessian矩阵。<br>
运用微积分知识，我们可以得到，当代价函数最小化的时候，Δw的取值为<br>
<strong>∆w = -H-1∇C</strong></p>
<p>因此一个优化代价函数的方法如下：</p>
<ul>
<li>选择开始点，w</li>
<li>更新 w 到新点 w′ = w -H-1∇C，其中 Hessian H 和 ∇C 是在 w 处计算出来的。</li>
<li>不断更新……</li>
</ul>
<p><strong>缺点</strong>：Hessian 矩阵的太⼤了，因此计算 H-1∇C就变得极其困难。</p>
<h2 id="2-基于-momentum-的梯度下降">2.基于 momentum 的梯度下降</h2>
<p>我们引⼊<strong>速度</strong>变量 v = v1, v2, . . .，其中每⼀个对应 wj 变 量。<br>
修改梯度下降的更新规则如下：<br>
<img src="https://img-blog.csdnimg.cn/20191104004520227.JPG" alt="在这里插入图片描述"><br>
在这些⽅程中，µ 是⽤来控制<strong>阻碍</strong>或者<strong>摩擦力</strong>的量的超参数。<br>
准确地说，你应该将 1 - µ 看成是摩擦力的量。</p>
<ul>
<li>当 µ = 1 时，没有摩擦，速度完全由梯度 ∇C 决定。</li>
<li>若是 µ = 0，就存在很⼤的摩擦，速度⽆法叠加，上述公式就变成了通常的梯<br>
度下降。</li>
<li>在实践中，使⽤ 0 和 1 之间的 µ 值可以给我们避免过量⽽⼜能够叠加速度的好处。</li>
<li>我们可以使⽤ hold out 验证数据集来选择合适的 µ 值，就像我们之前选择 η 和 λ 那样。<br>
———————————————————————————————————</li>
</ul>
<h1>人工神经元的其他模型</h1>
<h2 id="1-tanh神经元">1.tanh神经元</h2>
<p>表达式：<br>
<img src="https://img-blog.csdnimg.cn/20191104005038231.JPG" alt="在这里插入图片描述"><br>
图像：<br>
<img src="https://img-blog.csdnimg.cn/20191104005204562.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="2-修正线性神经元（rectified-linear-neuron）">2.修正线性神经元（rectified linear neuron）</h2>
<p>修正线性神经元（rectified linear neuron）或者<strong>修正线性单元（rectified<br>
linear unit）</strong>，简记为 ReLU。输⼊为 x，权重向量为 w，偏置为 b 的 ReLU 神经元的输出是：max(0, w · x + b)</p>
<p>图像是：<br>
<img src="https://img-blog.csdnimg.cn/20191104005500786.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
———————————————————————————————————</p>
<h1>小总结</h1>
<p>这一章讲的内容很杂，提到了改善神经网络的很多技术和方法，以及超参数选择的方法。但是要注意的是，至今没有针对超参数设置的“金科玉律”，很有可能花费了大量的时间和精力，设置出来的参数依旧不满意，要做好准备。</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Anastasia</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2020/09/11/2020-09-11-%E6%94%B9%E5%96%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%A4%E5%8F%89%E7%86%B5%E5%87%BD%E6%95%B0%EF%BC%8C%E8%BF%87%E5%BA%A6%E6%8B%9F%E5%90%88%EF%BC%8C%E8%A7%84%E8%8C%83%E5%8C%96%EF%BC%8C%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9D%83%E9%87%8D%EF%BC%89/">http://example.com/2020/09/11/2020-09-11-%E6%94%B9%E5%96%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%A4%E5%8F%89%E7%86%B5%E5%87%BD%E6%95%B0%EF%BC%8C%E8%BF%87%E5%BA%A6%E6%8B%9F%E5%90%88%EF%BC%8C%E8%A7%84%E8%8C%83%E5%8C%96%EF%BC%8C%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9D%83%E9%87%8D%EF%BC%89/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Ana's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/network/">network</a><a class="post-meta__tags" href="/tags/python/">python</a></div><div class="post_share"><div class="social-share" data-image="https://mdimage001.oss-cn-beijing.aliyuncs.com/img/v2-fc6b5bc5ed17621fa52acf7a19b2e379_1440w.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/09/11/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%EF%BC%88backpropagation%EF%BC%89%E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%88SGD%EF%BC%89%E2%80%94%E2%80%94AI%E5%85%A5%E9%97%A8/"><img class="prev-cover" src="https://mdimage001.oss-cn-beijing.aliyuncs.com/img/v2-fc6b5bc5ed17621fa52acf7a19b2e379_1440w.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">反向传播算法(backpropagation)计算梯度下降(SGD)详解</div></div></a></div><div class="next-post pull-right"><a href="/2020/09/10/2020-09-10-%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%ABMNIST%EF%BC%88python3%EF%BC%89/"><img class="next-cover" src="https://mdimage001.oss-cn-beijing.aliyuncs.com/img/v2-834cdc1e1f29e1f92ec25185fe1e964e_1440w.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">手写数字识别MNIST（python3）</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2020/09/10/2020-09-10-手写数字识别MNIST（python3）/" title="手写数字识别MNIST（python3）"><img class="cover" src="https://mdimage001.oss-cn-beijing.aliyuncs.com/img/v2-834cdc1e1f29e1f92ec25185fe1e964e_1440w.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-09-10</div><div class="title">手写数字识别MNIST（python3）</div></div></a></div><div><a href="/2020/09/11/反向传播算法（backpropagation）计算梯度下降（SGD）——AI入门/" title="反向传播算法(backpropagation)计算梯度下降(SGD)详解"><img class="cover" src="https://mdimage001.oss-cn-beijing.aliyuncs.com/img/v2-fc6b5bc5ed17621fa52acf7a19b2e379_1440w.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-09-11</div><div class="title">反向传播算法(backpropagation)计算梯度下降(SGD)详解</div></div></a></div><div><a href="/2020/09/11/深度神经网络为何很难训练？——AI入门/" title="神经网络为何很难训练？"><img class="cover" src="https://mdimage001.oss-cn-beijing.aliyuncs.com/img/v2-fc6b5bc5ed17621fa52acf7a19b2e379_1440w.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-09-11</div><div class="title">神经网络为何很难训练？</div></div></a></div><div><a href="/2020/09/11/神经网络可以计算任何函数的可视化证明——AI入门/" title="神经网络可以计算任何函数的可视化证明"><img class="cover" src="https://mdimage001.oss-cn-beijing.aliyuncs.com/img/v2-fc6b5bc5ed17621fa52acf7a19b2e379_1440w.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-09-11</div><div class="title">神经网络可以计算任何函数的可视化证明</div></div></a></div><div><a href="/2020/09/11/卷积神经网络CNN+识别手写数字MNIST（附代码）——AI入门/" title="卷积神经网络(CNN)识别手写数字MNIST(附代码)"><img class="cover" src="https://mdimage001.oss-cn-beijing.aliyuncs.com/img/v2-fc6b5bc5ed17621fa52acf7a19b2e379_1440w.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-09-11</div><div class="title">卷积神经网络(CNN)识别手写数字MNIST(附代码)</div></div></a></div></div></div></article></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Anastasia</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">hello stranger</div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  var script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script></div></body></html>