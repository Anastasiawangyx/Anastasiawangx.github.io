<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ana&#39;s blog</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-06-24T03:07:30.893Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Anastasia</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>十大经典排序算法C++版</title>
    <link href="http://example.com/2021/06/24/%E6%8E%92%E5%BA%8F/"/>
    <id>http://example.com/2021/06/24/%E6%8E%92%E5%BA%8F/</id>
    <published>2021-06-24T04:00:00.000Z</published>
    <updated>2021-06-24T03:07:30.893Z</updated>
    
    <content type="html"><![CDATA[<h1>十大经典排序算法C++版</h1><p>对排序的一些术语进行说明。</p><ul><li>稳定：如果在原数组中，a在b之前且a=b，排序后a也要在b之前。</li><li>不稳定：如果子啊原数组中国，a在b之前且a=b，排序后a可能会出现在b之后。</li><li>内排序：所有的排序操作都在内存中完成。</li><li>外排序：由于数据太大，因此把数据放在磁盘中，而排序通过磁盘和内存的数据传输才能进行；</li></ul><ul><li>时间复杂度： 一个算法执行所耗费的时间。</li><li>空间复杂度：运行完一个程序所需内存的大小。</li></ul><p>具体原理：<a href="https://blog.csdn.net/weixin_41190227/article/details/86600821">超详细十大经典排序算法总结</a></p><h1>1、冒泡排序</h1><p>两两比较，将大数逐渐交换到数列到最后方。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">bubble_sort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;<span class="built_in">array</span>)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n = <span class="built_in">array</span>.size();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; n-i<span class="number">-1</span>; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(<span class="built_in">array</span>[j]&gt;<span class="built_in">array</span>[j+<span class="number">1</span>])&#123;</span><br><span class="line">                <span class="keyword">int</span> temp = <span class="built_in">array</span>[j];</span><br><span class="line">                <span class="built_in">array</span>[j] = <span class="built_in">array</span>[j+<span class="number">1</span>];</span><br><span class="line">                <span class="built_in">array</span>[j+<span class="number">1</span>] = temp;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">array</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>算法分析</p><ul><li>最佳情况：T(n) = O(n)   使用一个flag，当遍历一遍都不交换时，直接返回</li><li>最差情况：T(n) = O(n2)</li><li>平均情况：T(n) = O(n2)</li><li>稳定</li></ul><h1>2、选择排序</h1><p>从左往右，依次寻找当前数列中最小的数字，并交换到最前面。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">sort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;<span class="built_in">array</span>)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n = <span class="built_in">array</span>.size();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> minIndex = i;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = i; j &lt; n; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(<span class="built_in">array</span>[j]&lt;<span class="built_in">array</span>[minIndex])    minIndex = j;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> temp = <span class="built_in">array</span>[minIndex];</span><br><span class="line">        <span class="built_in">array</span>[minIndex] = <span class="built_in">array</span>[i];</span><br><span class="line">        <span class="built_in">array</span>[i] = temp;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">array</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>最佳情况：T(n) = O(n2)</li><li>最差情况：T(n) = O(n2)</li><li>平均情况：T(n) = O(n2)</li><li>不稳定</li></ul><h1>3、插入排序</h1><p>从左往右，依次将本数字插入到第一个小于它的数字后。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">sort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;<span class="built_in">array</span>)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n = <span class="built_in">array</span>.size();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> current = <span class="built_in">array</span>[i];</span><br><span class="line">        <span class="keyword">int</span> j = i<span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">while</span>(current&lt;<span class="built_in">array</span>[j]&amp;&amp;j&gt;=<span class="number">0</span>)&#123;</span><br><span class="line">            <span class="built_in">array</span>[j+<span class="number">1</span>] = <span class="built_in">array</span>[j];</span><br><span class="line">            j--;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">array</span>[j+<span class="number">1</span>] = current;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">array</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>最佳情况：T(n) = O(n)</li><li>最坏情况：T(n) = O(n2)</li><li>平均情况：T(n) = O(n2)</li><li>稳定</li></ul><h1>4、希尔排序</h1><p>相对于插入排序，希尔排序通过分组的方式将排序的范围拉的更宽，使靠后的小数字可以更快地被换到前面。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">sort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;<span class="built_in">array</span>)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n = <span class="built_in">array</span>.size();</span><br><span class="line">    <span class="keyword">int</span> gap = n/<span class="number">2</span>;</span><br><span class="line">    <span class="keyword">int</span> temp,preIndex;</span><br><span class="line">    <span class="keyword">while</span>(gap&gt;<span class="number">0</span>)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = gap;i&lt;n;i++)&#123;</span><br><span class="line">            preIndex = i-gap;</span><br><span class="line">            temp = <span class="built_in">array</span>[i];</span><br><span class="line">            <span class="keyword">while</span>(preIndex&gt;=<span class="number">0</span>&amp;&amp;<span class="built_in">array</span>[preIndex]&gt;temp)&#123;</span><br><span class="line">                <span class="built_in">array</span>[preIndex+gap] = <span class="built_in">array</span>[preIndex];</span><br><span class="line">                preIndex -= gap;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="built_in">array</span>[preIndex+gap] = temp;</span><br><span class="line">        &#125;</span><br><span class="line">        gap /= <span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">array</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>最佳情况：$T(n) = O(n)$</li><li>最坏情况：$T(n) = O(log_2n)$</li><li>平均情况：$T(n) = O(log_2n)$</li><li>不稳定</li></ul><h1>5、归并排序</h1><p>使用分治算法，将待排序的序列二分，然后再进行合并。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">merge</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;left, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;right)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> l = left.size();</span><br><span class="line">    <span class="keyword">int</span> r = right.size();</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;res;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> index = <span class="number">0</span>, i = <span class="number">0</span>, j = <span class="number">0</span>; index&lt;l+r; index++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(i&gt;=l)&#123;</span><br><span class="line">            res.push_back(right[j++]);</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(j&gt;=r)&#123;</span><br><span class="line">            res.push_back(left[i++]);</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(left[i]&lt;right[j])&#123;</span><br><span class="line">            res.push_back(left[i++]);</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            res.push_back(right[j++]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">sort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;<span class="built_in">array</span>)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n = <span class="built_in">array</span>.size();</span><br><span class="line">    <span class="keyword">if</span>(n&lt;<span class="number">2</span>) <span class="keyword">return</span> <span class="built_in">array</span>;</span><br><span class="line">    <span class="keyword">int</span> mid = n/<span class="number">2</span>;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;left(<span class="built_in">array</span>.begin(),<span class="built_in">array</span>.begin()+mid);</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;right(<span class="built_in">array</span>.begin()+mid,<span class="built_in">array</span>.end());</span><br><span class="line">    <span class="keyword">return</span> merge(sort(left),sort(right));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>最佳情况：$T(n) = O(log_2n)$</li><li>最坏情况：$T(n) = O(log_2n)$</li><li>平均情况：$T(n) = O(log_2n)$</li><li>不稳定</li></ul><h1>6、快速排序</h1><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">partition</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp;<span class="built_in">array</span>, <span class="keyword">int</span> low, <span class="keyword">int</span> high)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> temp = <span class="built_in">array</span>[low];</span><br><span class="line">    <span class="keyword">while</span>(low&lt;high)&#123;</span><br><span class="line">        <span class="keyword">while</span>(<span class="built_in">array</span>[high]&gt;temp&amp;&amp;low&lt;high)   high--;</span><br><span class="line">        <span class="built_in">array</span>[low] = <span class="built_in">array</span>[high];</span><br><span class="line">        <span class="keyword">while</span>(<span class="built_in">array</span>[low]&lt;temp&amp;&amp;low&lt;high)    low++;</span><br><span class="line">        <span class="built_in">array</span>[high] = <span class="built_in">array</span>[low];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">array</span>[low] = temp;</span><br><span class="line">    <span class="keyword">return</span> low;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp;<span class="built_in">array</span>, <span class="keyword">int</span> low, <span class="keyword">int</span> high)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(low&lt;high)&#123;</span><br><span class="line">    <span class="keyword">int</span> index = partition(<span class="built_in">array</span>,low,high);</span><br><span class="line">    sort(<span class="built_in">array</span>,low,index<span class="number">-1</span>);</span><br><span class="line">    sort(<span class="built_in">array</span>,index+<span class="number">1</span>,high);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>分治的思想，设置一个“基准”数据，将所有大于该数据的都放到后面，所有小于该数据的都放到前面，再分别对前后数据进行分治操作。</p><ul><li>最佳情况：$T(n) = O(log_2n)$</li><li>最坏情况：$T(n) = O(log_2n)$</li><li>平均情况：$T(n) = O(n^2)$</li></ul><h1>7、堆排序</h1><p>构建最大堆（顶点数据比子数据大），再将顶点交换到最后即可。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">adjustHeap</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp;<span class="built_in">array</span>,<span class="keyword">int</span> i,<span class="keyword">int</span> len)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> maxIndex = i;</span><br><span class="line">    <span class="keyword">if</span>(<span class="number">2</span>*i+<span class="number">1</span>&lt;len&amp;&amp;<span class="built_in">array</span>[<span class="number">2</span>*i+<span class="number">1</span>]&gt;<span class="built_in">array</span>[maxIndex])    maxIndex = <span class="number">2</span>*i+<span class="number">1</span>;</span><br><span class="line">    <span class="keyword">if</span>(<span class="number">2</span>*i+<span class="number">2</span>&lt;len&amp;&amp;<span class="built_in">array</span>[<span class="number">2</span>*i+<span class="number">2</span>]&gt;<span class="built_in">array</span>[maxIndex])    maxIndex = <span class="number">2</span>*i+<span class="number">2</span>;</span><br><span class="line">    <span class="keyword">if</span>(maxIndex!=i)&#123;</span><br><span class="line">        <span class="keyword">int</span> temp = <span class="built_in">array</span>[i];</span><br><span class="line">        <span class="built_in">array</span>[i] = <span class="built_in">array</span>[maxIndex];</span><br><span class="line">        <span class="built_in">array</span>[maxIndex] = temp;</span><br><span class="line">        adjustHeap(<span class="built_in">array</span>,i,len<span class="number">-1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp;<span class="built_in">array</span>, <span class="keyword">int</span> low, <span class="keyword">int</span> high)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> len = <span class="built_in">array</span>.size();</span><br><span class="line">    <span class="keyword">if</span>(len&lt;<span class="number">2</span>)<span class="keyword">return</span>;</span><br><span class="line">    <span class="comment">//构造初始堆</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = len/<span class="number">2</span><span class="number">-1</span>;i&gt;=<span class="number">0</span>;i--)&#123;</span><br><span class="line">        adjustHeap(<span class="built_in">array</span>,i,len);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//将最大值交换到最后，继续调整</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = len<span class="number">-1</span>;i&gt;<span class="number">0</span>;i--)&#123;</span><br><span class="line">        <span class="keyword">int</span> temp = <span class="built_in">array</span>[i];</span><br><span class="line">        <span class="built_in">array</span>[i] = <span class="built_in">array</span>[<span class="number">0</span>];</span><br><span class="line">        <span class="built_in">array</span>[<span class="number">0</span>] = temp;</span><br><span class="line">        adjustHeap(<span class="built_in">array</span>,<span class="number">0</span>,i);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>最佳情况：$T(n) = O(log_2n)$</li><li>最差情况：$T(n) = O(log_2n)$</li><li>平均情况：$T(n) = O(log_2n)$</li></ul><h1>8、计数排序</h1><p>使用一个额外的数组来存储数组中每一个数字出现的次数。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp;<span class="built_in">array</span>)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n = <span class="built_in">array</span>.size();</span><br><span class="line">    <span class="keyword">if</span>(n&lt;<span class="number">2</span>) <span class="keyword">return</span>;</span><br><span class="line">    <span class="keyword">int</span> maxValue = <span class="built_in">array</span>[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">int</span> minValue = <span class="built_in">array</span>[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>;i&lt;n;i++)&#123;</span><br><span class="line">        maxValue = max(maxValue,<span class="built_in">array</span>[i]);</span><br><span class="line">        minValue = min(minValue,<span class="built_in">array</span>[i]);</span><br><span class="line">    &#125;</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">int</span> len = maxValue-minValue+<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//构建大小为10的桶</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;bucket(len);</span><br><span class="line">    <span class="comment">//向桶中放数据</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i&lt;len;i++)&#123;</span><br><span class="line">        <span class="keyword">int</span> num = <span class="built_in">array</span>[i]-minValue;</span><br><span class="line">        bucket[num]++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//将桶中的数据放回原数据</span></span><br><span class="line">    <span class="keyword">int</span> index = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i&lt;len;i++)&#123;</span><br><span class="line">        <span class="keyword">while</span>(bucket[i]!=<span class="number">0</span>)&#123;</span><br><span class="line">            bucket[i]--;</span><br><span class="line">            <span class="built_in">array</span>[index] = i+minValue;</span><br><span class="line">            index++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1>9、桶排序</h1><p>使用桶进行数据的存储，其中桶的长度和数量依照数组的范围自动确定。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp;<span class="built_in">array</span>)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n = <span class="built_in">array</span>.size();</span><br><span class="line">    <span class="keyword">if</span>(n&lt;<span class="number">2</span>) <span class="keyword">return</span>;</span><br><span class="line">    <span class="keyword">int</span> maxValue = <span class="built_in">array</span>[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">int</span> minValue = <span class="built_in">array</span>[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>;i&lt;n;i++)&#123;</span><br><span class="line">        maxValue = max(maxValue,<span class="built_in">array</span>[i]);</span><br><span class="line">        minValue = min(minValue,<span class="built_in">array</span>[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//构建桶</span></span><br><span class="line">    <span class="keyword">int</span> len = (maxValue-minValue)/n+<span class="number">1</span>;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt;bucket(len);</span><br><span class="line">    <span class="comment">//将数据填入桶中</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">        <span class="keyword">int</span> num = (<span class="built_in">array</span>[i]-minValue)/n;</span><br><span class="line">        bucket[num].push_back(<span class="built_in">array</span>[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i&lt;len;i++)&#123;</span><br><span class="line">        sort(bucket[i].begin(),bucket[i].end());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//将桶中的数据放回原数据</span></span><br><span class="line">    <span class="keyword">int</span> index = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i&lt;len;i++)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;bucket[i].size();j++)&#123;</span><br><span class="line">            <span class="built_in">array</span>[index] = bucket[i][j];</span><br><span class="line">            index++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1>10、基数排序</h1><p>将桶的大小固定为10，再对每个数字按照其个位、十位等的顺序依次入桶出桶。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp;<span class="built_in">array</span>)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n = <span class="built_in">array</span>.size();</span><br><span class="line">    <span class="keyword">if</span>(n&lt;<span class="number">2</span>) <span class="keyword">return</span>;</span><br><span class="line">    <span class="keyword">int</span> maxValue = <span class="built_in">array</span>[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>;i&lt;n;i++)&#123;</span><br><span class="line">        maxValue = max(maxValue,<span class="built_in">array</span>[i]);</span><br><span class="line">    &#125;</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">int</span> len = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(maxValue!=<span class="number">0</span>)&#123;</span><br><span class="line">        maxValue/=<span class="number">10</span>;</span><br><span class="line">        len++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//构建大小为10的桶</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt;bucket(<span class="number">10</span>);</span><br><span class="line">    <span class="comment">//向桶中放数据</span></span><br><span class="line">    <span class="keyword">int</span> mod = <span class="number">10</span>, div = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i&lt;len;i++)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>;j&lt;n;j++)&#123;</span><br><span class="line">            <span class="keyword">int</span> num = (<span class="built_in">array</span>[j]%mod)/div;</span><br><span class="line">            bucket[num].push_back(<span class="built_in">array</span>[j]);</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="comment">//将桶中的数据放回原数据</span></span><br><span class="line">    <span class="keyword">int</span> index = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i&lt;<span class="number">10</span>;i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;bucket[i].size();j++)&#123;</span><br><span class="line">                <span class="built_in">array</span>[index] = bucket[i][j];</span><br><span class="line">                index++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;十大经典排序算法C++版&lt;/h1&gt;
&lt;p&gt;对排序的一些术语进行说明。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;稳定：如果在原数组中，a在b之前且a=b，排序后a也要在b之前。&lt;/li&gt;
&lt;li&gt;不稳定：如果子啊原数组中国，a在b之前且a=b，排序后a可能会出现在b之后。&lt;/li&gt;
&lt;l</summary>
      
    
    
    
    <category term="算法" scheme="http://example.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="算法" scheme="http://example.com/tags/%E7%AE%97%E6%B3%95/"/>
    
    <category term="C++" scheme="http://example.com/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>c++纯虚函数与抽象基类</title>
    <link href="http://example.com/2020/12/15/c-%E7%BA%AF%E8%99%9A%E5%87%BD%E6%95%B0%E4%B8%8E%E6%8A%BD%E8%B1%A1%E5%9F%BA%E7%B1%BB/"/>
    <id>http://example.com/2020/12/15/c-%E7%BA%AF%E8%99%9A%E5%87%BD%E6%95%B0%E4%B8%8E%E6%8A%BD%E8%B1%A1%E5%9F%BA%E7%B1%BB/</id>
    <published>2020-12-15T06:55:24.000Z</published>
    <updated>2020-12-15T09:05:27.508Z</updated>
    
    <content type="html"><![CDATA[<p>c++面向对象编程的思想之一是可以使用继承。继承中一个重要的思想是使用抽象基类（abstract base class，ABC）。</p><p>假设我们开发一个程序，需要使用椭圆和圆两种图形。因为圆是椭圆的一种特殊情形，根据继承‘is-a’的思想，自然会想到先定义一个Eclipse类，再将Circle类继承自Eclipse类。</p><p>但这样实现有不好的地方。</p><p>椭圆中，需要表示长半轴和短半轴、angle（长轴和水平线的夹角），还可以有rotate方法将椭圆进行旋转，这些都是圆所不需要的，将圆继承自椭圆，是十分笨拙的。</p><p>一种更好的办法是，使用抽象基类（ABC），将椭圆和圆的共性部分，例如坐标x、y，共同的方法例如移动move、计算面积Area抽象出来放到一个基类shape中，再将Eclipse和Circle类分别继承shape，这样就可以使用基类指针数组同时管理Eclipse和Circle对象。</p><p>当类声明中包含纯虚函数时，不能创建该类的对象，此类称为抽象基类。</p><p>纯虚函数的声明如下（以Area为例）：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">double</span> <span class="title">Area</span><span class="params">()</span> <span class="keyword">const</span> </span>=<span class="number">0</span> ;</span><br></pre></td></tr></table></figure><p>ABC类shape的定义如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Shape</span>&#123;</span></span><br><span class="line">  <span class="keyword">private</span>:</span><br><span class="line">  <span class="keyword">double</span> x;</span><br><span class="line">  <span class="keyword">double</span> y;</span><br><span class="line">  <span class="keyword">public</span>:</span><br><span class="line">  Shape(<span class="keyword">double</span> x0=<span class="number">0</span>,y0=<span class="number">0</span>):x(x0),y(y0)&#123;&#125;<span class="comment">//构造函数</span></span><br><span class="line">  <span class="keyword">virtual</span> ~Shape()&#123;&#125;<span class="comment">//析构函数</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">move</span><span class="params">(<span class="keyword">int</span> nx,ny)</span></span>&#123;x=nx;y=ny;&#125;<span class="comment">//move函数</span></span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">double</span> <span class="title">Area</span><span class="params">()</span> <span class="keyword">const</span> </span>=<span class="number">0</span>;<span class="comment">//纯虚函数</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>此后，就可以从Shape类派生出Eclipse类和Circle类，在各自的派生类中加入需要的属性</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;c++面向对象编程的思想之一是可以使用继承。继承中一个重要的思想是使用抽象基类（abstract base class，ABC）。&lt;/p&gt;
&lt;p&gt;假设我们开发一个程序，需要使用椭圆和圆两种图形。因为圆是椭圆的一种特殊情形，根据继承‘is-a’的思想，自然会想到先定义一个Ec</summary>
      
    
    
    
    
    <category term="c++" scheme="http://example.com/tags/c/"/>
    
  </entry>
  
  <entry>
    <title>c++重载运算符</title>
    <link href="http://example.com/2020/12/12/c-%E9%87%8D%E8%BD%BD%E8%BF%90%E7%AE%97%E7%AC%A6/"/>
    <id>http://example.com/2020/12/12/c-%E9%87%8D%E8%BD%BD%E8%BF%90%E7%AE%97%E7%AC%A6/</id>
    <published>2020-12-12T03:35:42.000Z</published>
    <updated>2020-12-12T04:14:42.800Z</updated>
    
    <content type="html"><![CDATA[<p>运算符的重载使得我们可以更加方便的使用常见的运算符进行操作。</p><p>重载运算符的函数格式如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">operatorop(argement-<span class="built_in">list</span>)</span><br></pre></td></tr></table></figure><p>接下来的例子中，我们将定义一个Time类，并通过重载运算符+和&lt;&lt;来实现时间的加法和输出显示，其中会使用友元函数（friend）</p><h2 id="time-h">time.h</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> TIME_H</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> TIME_H</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Time</span>&#123;</span></span><br><span class="line">  <span class="keyword">private</span>:</span><br><span class="line">  <span class="keyword">int</span> minute;</span><br><span class="line">  <span class="keyword">int</span> hour;</span><br><span class="line">  <span class="keyword">public</span>:</span><br><span class="line">  Time();</span><br><span class="line">  Time(<span class="keyword">int</span> h=<span class="number">0</span>,<span class="keyword">int</span> m=<span class="number">0</span>);</span><br><span class="line">Time <span class="keyword">operator</span>+(<span class="keyword">const</span> Time &amp; t)<span class="keyword">const</span>;<span class="comment">//重载运算符+</span></span><br><span class="line">  <span class="keyword">friend</span> <span class="built_in">std</span>::ostream &amp; <span class="keyword">operator</span>&lt;&lt;(<span class="built_in">std</span>::ostream &amp; os, <span class="keyword">const</span> Time &amp; t);<span class="comment">//重载运算符&lt;&lt;，使用了友元函数</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#end <span class="meta-keyword">if</span></span></span><br></pre></td></tr></table></figure><h2 id="time-cpp">time.cpp</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&quot;time.h&quot;</span></span></span><br><span class="line">Time::Time()&#123;</span><br><span class="line">  hour=minute=<span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line">Time::Time(<span class="keyword">int</span> h,<span class="keyword">int</span> m)&#123;</span><br><span class="line">  hour=h;</span><br><span class="line">  minute=m;</span><br><span class="line">&#125;</span><br><span class="line">Time <span class="keyword">operator</span>+(<span class="keyword">const</span> Time &amp; t)<span class="keyword">const</span>&#123;</span><br><span class="line">  Time sum;</span><br><span class="line">  sum.minute=minute+t.minute;</span><br><span class="line">  sum.hour=hour+t.hour;</span><br><span class="line">  sum.minute%=<span class="number">60</span>;</span><br><span class="line">  <span class="keyword">return</span> sum;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//友元函数只需要在原型处使用friend，在定义时不需要使用friend</span></span><br><span class="line"><span class="built_in">std</span>::ostream &amp; <span class="keyword">operator</span>&lt;&lt;(<span class="built_in">std</span>::ostream &amp; os, <span class="keyword">const</span> Time &amp;t)&#123;</span><br><span class="line">  os&lt;&lt;t.hour&lt;&lt;<span class="string">&quot;hours,&quot;</span>&lt;&lt;t.minute&lt;&lt;<span class="string">&quot;minutes&quot;</span>;</span><br><span class="line">  <span class="keyword">return</span> os;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="main-cpp">main.cpp</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&quot;time.h&quot;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">  <span class="keyword">using</span> <span class="built_in">std</span>::<span class="built_in">cout</span>;</span><br><span class="line">  <span class="keyword">using</span> <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">  <span class="function">Time <span class="title">aida</span><span class="params">(<span class="number">3</span>,<span class="number">35</span>)</span></span>;</span><br><span class="line">  <span class="function">Time <span class="title">tosca</span><span class="params">(<span class="number">2</span>,<span class="number">48</span>)</span></span>;</span><br><span class="line">  <span class="comment">//&lt;&lt;重载时使用友元函数，是因为&lt;&lt;操作符的调用对象并不是time，而是cout（ostream对象）</span></span><br><span class="line">  <span class="comment">//使用友元函数可以让非成员函数调用类对象。</span></span><br><span class="line">  <span class="built_in">cout</span>&lt;&lt;<span class="string">&quot;aida:&quot;</span>&lt;&lt;aida&lt;&lt;<span class="string">&quot;;tosca:&quot;</span>&lt;&lt;tosca&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">  <span class="built_in">cout</span>&lt;&lt;<span class="string">&quot;aida+tosca=&quot;</span>&lt;&lt;aida+tosca&lt;&lt;<span class="built_in">endl</span>;<span class="comment">//直接使用+完成时间的相加</span></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//output</span></span><br><span class="line">aida:<span class="number">3</span> hours,<span class="number">35</span> minutes</span><br><span class="line">tosca:<span class="number">2</span> hours,<span class="number">48</span> minutes</span><br><span class="line">aida+tosca=<span class="number">4</span> hours,<span class="number">11</span> minutes</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;运算符的重载使得我们可以更加方便的使用常见的运算符进行操作。&lt;/p&gt;
&lt;p&gt;重载运算符的函数格式如下：&lt;/p&gt;
&lt;figure class=&quot;highlight c++&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;lin</summary>
      
    
    
    
    
    <category term="c++" scheme="http://example.com/tags/c/"/>
    
  </entry>
  
  <entry>
    <title>c++引用变量详解</title>
    <link href="http://example.com/2020/12/09/c-%E5%BC%95%E7%94%A8%E5%8F%98%E9%87%8F%E8%AF%A6%E8%A7%A3/"/>
    <id>http://example.com/2020/12/09/c-%E5%BC%95%E7%94%A8%E5%8F%98%E9%87%8F%E8%AF%A6%E8%A7%A3/</id>
    <published>2020-12-09T06:32:31.000Z</published>
    <updated>2020-12-09T09:52:29.672Z</updated>
    
    <content type="html"><![CDATA[<h2 id="创建引用变量">创建引用变量</h2><p>引用变量，是已定义变量的别名。</p><p>c++中使用&amp;符号来创建引用变量。</p><p>例如，将rodents定义为rats的别名，则可以使用以下代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> rats;</span><br><span class="line"><span class="keyword">int</span> &amp; rodents=rats;</span><br></pre></td></tr></table></figure><p>此时，rodents和rats指向相同的值和地址。</p><p>可以通过下面的代码来测试：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> rats=<span class="number">101</span>;</span><br><span class="line"><span class="keyword">int</span> &amp; rodents=rats;</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;<span class="string">&quot;rats=&quot;</span>&lt;&lt;rats&lt;&lt;<span class="string">&quot;,rodents=&quot;</span>&lt;&lt;rodents&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">rodents++;</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;<span class="string">&quot;rats=&quot;</span>&lt;&lt;rats&lt;&lt;<span class="string">&quot;,rodents=&quot;</span>&lt;&lt;rodents&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//output</span></span><br><span class="line">rats=<span class="number">101</span>,rodents=<span class="number">101</span></span><br><span class="line">rats=<span class="number">102</span>,rodents=<span class="number">102</span></span><br></pre></td></tr></table></figure><p>从这方面来说，引用和指针很像，但是有几点不同：</p><ul><li>引用必须在声明的时候初始化，而不能先声明，再初始化。</li><li>引用一旦与某个定义关联起来，就讲一直效忠于它，不再改变。</li></ul><h2 id="将引用作为函数参数">将引用作为函数参数</h2><p>这种参数传递的方法叫按引用传递，不同于按值传递，按引用传递可以直接修改实参的值。</p><p>以下为一个例子：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iosteam&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">add1</span><span class="params">(<span class="keyword">int</span> a)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">add2</span><span class="params">(<span class="keyword">int</span> &amp; a)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">  <span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line">  <span class="keyword">int</span> a=<span class="number">1</span>;</span><br><span class="line">  add1(a);</span><br><span class="line">  <span class="built_in">cout</span>&lt;&lt;a&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">  add2(a);</span><br><span class="line">  <span class="built_in">cout</span>&lt;&lt;a&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">add1</span><span class="params">(<span class="keyword">int</span> a)</span></span>&#123;</span><br><span class="line">  a++;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">add2</span><span class="params">(<span class="keyword">int</span> &amp; a)</span></span>&#123;</span><br><span class="line">  a++</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//output</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure><p>我们也可以选择不修改原始数据的值，可以在函数中使用<code>const</code></p><p>下面这个例子用来计算参数的立方：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iosteam&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">cube1</span><span class="params">(<span class="keyword">int</span> a)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">cube2</span><span class="params">(<span class="keyword">int</span> &amp; a)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">cube3</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> &amp; a)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">  <span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line">  <span class="keyword">int</span> a=<span class="number">3</span>;</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;cube1(a)&lt;&lt;<span class="string">&quot;=cube of &quot;</span>&lt;&lt;a&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">  a=<span class="number">3</span>;</span><br><span class="line">  <span class="built_in">cout</span>&lt;&lt;cube2(a)&lt;&lt;<span class="string">&quot;=cube of &quot;</span>&lt;&lt;a&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">  a=<span class="number">3</span>;</span><br><span class="line">  <span class="built_in">cout</span>&lt;&lt;cube3(a)&lt;&lt;<span class="string">&quot;=cube of &quot;</span>&lt;&lt;a&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">cube1</span><span class="params">(<span class="keyword">int</span> an)</span></span>&#123;</span><br><span class="line">  an*=an*an;</span><br><span class="line">  <span class="keyword">return</span> an;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">cube2</span><span class="params">(<span class="keyword">int</span> &amp; an)</span></span>&#123;</span><br><span class="line">  an*=an*an;</span><br><span class="line">  <span class="keyword">return</span> an;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">cube3</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> &amp; an)</span></span>&#123;</span><br><span class="line">  an*=an*an;</span><br><span class="line">  <span class="keyword">return</span> an;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//output</span></span><br><span class="line"><span class="number">27</span>=cube of <span class="number">3</span></span><br><span class="line"><span class="number">27</span>=cube of <span class="number">27</span></span><br><span class="line"><span class="number">27</span>=cube of <span class="number">3</span></span><br></pre></td></tr></table></figure><p>使用const修饰时，main函数中的值没有发生改变。</p><p><strong>在上面的例子中，cube1和cube3实现的功能类似，为什么还要多此一举使用引用类型呢？</strong></p><p>这是因为，在按值传递的函数中，编译器会自动生成一个形参的副本，也就是一个临时变量，来存储实参的值。当我们需要传递的参数内容较大时，会造成一定的内存浪费。</p><p>使用引用传递，则可以直接将实参的地址传递到函数中，不需要生产临时变量，节省空间和时间。</p><p>在结构体和类对象中，引用就显得十分必要。</p><h2 id="将引用用于结构">将引用用于结构</h2><p>使用结构引用参数的方式与使用基本变量相同，只需在声明结构参数的时候使用引用运算符&amp;即可。</p><p>假设有如下结构体定义：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">person</span>&#123;</span></span><br><span class="line">  <span class="built_in">string</span> name;</span><br><span class="line">  <span class="keyword">int</span> age;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//函数原型如下</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">show</span><span class="params">(person &amp; p1)</span></span>;</span><br><span class="line"><span class="comment">//如果不希望函数修改传入的结构，可以使用const</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">show</span><span class="params">(<span class="keyword">const</span> person &amp; p1)</span></span>;</span><br></pre></td></tr></table></figure><p>下面用一个例子来说明：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">person</span>&#123;</span></span><br><span class="line">  <span class="built_in">string</span> name;</span><br><span class="line">  <span class="keyword">int</span> age;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function">person &amp; <span class="title">older</span><span class="params">(<span class="keyword">const</span> person &amp; p1,<span class="keyword">const</span> person &amp; p2)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">show</span><span class="params">(<span class="keyword">const</span> person &amp; p3)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">  person father=&#123;&#x27;James&#x27;,40&#125;;</span><br><span class="line">  person son=&#123;&#x27;Simon&#x27;,18&#125;;</span><br><span class="line">  show(older(father,son));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">person &amp; <span class="title">older</span><span class="params">(<span class="keyword">const</span> person &amp; p1,<span class="keyword">const</span> person &amp;p2)</span></span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(p1.age&gt;p2.age)</span><br><span class="line">    <span class="keyword">return</span> p1;</span><br><span class="line">  <span class="keyword">else</span> <span class="keyword">return</span> p2;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">show</span><span class="params">(<span class="keyword">const</span> person &amp; p3)</span></span>&#123;</span><br><span class="line">  <span class="built_in">cout</span>&lt;&lt;<span class="string">&quot;name:&quot;</span>&lt;&lt;p3.name&lt;&lt;<span class="string">&quot;,age:&quot;</span>&lt;&lt;p3.age&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//output</span></span><br><span class="line">name:James,ages:<span class="number">40</span></span><br></pre></td></tr></table></figure><p>此程序是找出person中年龄较大的那一个。</p><p>关于语句<code>show(older(father,son));</code>，有下面的解释。</p><p>首先，father作为第一个参数传递给了older()，这意味着，在older()中，p1指向的是father。</p><p>同理，son作为第二个参数传递给了older()，在older()中，p2指向的是son。</p><p>将两者的age属性进行比较后，将年龄较大的person结构体的引用返回，在此次的例子中，返回的是father，也就是说，返回的是<strong>最初传进来</strong>的father对象。</p><p>接下来，将older()的返回值作为参数传递给了show()，这意味着将father传递给了show()。</p><h3 id="为什么要返回引用类型？">为什么要返回引用类型？</h3><p>假设有这么一条语句：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p3=older(father,son);</span><br></pre></td></tr></table></figure><p>如果older返回的是一个值，而不是引用，则编译器会把整个结构体复制到一个临时变量，再将这个临时变量拷贝复制给p3。但在返回值是引用时，将直接把father复制到p3，其效率更高。</p><p>还有一点要注意：</p><p>上述代码中，show函数的参数使用了const修饰，但older的返回值类型并没有const修饰，但程序依旧可以运行不报错，这是因为，在c++中，可以将一个非const变量赋给const类型，但不可以把一个const变量赋给非const类型。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;创建引用变量&quot;&gt;创建引用变量&lt;/h2&gt;
&lt;p&gt;引用变量，是已定义变量的别名。&lt;/p&gt;
&lt;p&gt;c++中使用&amp;amp;符号来创建引用变量。&lt;/p&gt;
&lt;p&gt;例如，将rodents定义为rats的别名，则可以使用以下代码：&lt;/p&gt;
&lt;figure class=&quot;highl</summary>
      
    
    
    
    
    <category term="c++" scheme="http://example.com/tags/c/"/>
    
  </entry>
  
  <entry>
    <title>c++将函数作为函数参数（函数指针）</title>
    <link href="http://example.com/2020/12/09/c-%E5%B0%86%E5%87%BD%E6%95%B0%E4%BD%9C%E4%B8%BA%E5%87%BD%E6%95%B0%E5%8F%82%E6%95%B0%EF%BC%88%E5%87%BD%E6%95%B0%E6%8C%87%E9%92%88%EF%BC%89/"/>
    <id>http://example.com/2020/12/09/c-%E5%B0%86%E5%87%BD%E6%95%B0%E4%BD%9C%E4%B8%BA%E5%87%BD%E6%95%B0%E5%8F%82%E6%95%B0%EF%BC%88%E5%87%BD%E6%95%B0%E6%8C%87%E9%92%88%EF%BC%89/</id>
    <published>2020-12-09T03:29:48.000Z</published>
    <updated>2020-12-09T05:10:33.759Z</updated>
    
    <content type="html"><![CDATA[<h2 id="如何获取函数的地址">如何获取函数的地址</h2><p>函数名本身就是函数的地址。</p><p>假设有一个函数<code>think()</code>，则<code>think</code>就是该函数的地址。</p><p>要将函数作为参数进行传递，必须传递<strong>函数名</strong>。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">process(think);<span class="comment">//传递的是函数地址</span></span><br><span class="line">process(think());<span class="comment">//传递的是函数的返回值</span></span><br></pre></td></tr></table></figure><h2 id="声明函数指针">声明函数指针</h2><p>在声明指向函数的指针时，要声明指针指向的函数的类型，即声明应指定函数的返回值类型和参数列表。</p><p>假设有一函数，原型如下：<code>double pam(int);</code></p><p>则正确的指针声明如下：<code>double (*pt)(int)</code></p><p>其中，<code>(*pt)</code>是函数，<code>pt</code>是函数指针。</p><p>在正确的声明了指针之后，我们就可以将相应的函数的地址赋给它：</p><p><code>pt=pam</code></p><h2 id="使用指针来调用函数">使用指针来调用函数</h2><p>前面指出，<code>(*pt)</code>是函数，与pam作用相同，因此使用的时候直接将它看作函数名即可。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">pam</span><span class="params">(<span class="keyword">int</span>)</span></span>;</span><br><span class="line"><span class="keyword">double</span> (*pt)(<span class="keyword">int</span>);</span><br><span class="line">pt=pam;</span><br><span class="line"><span class="keyword">double</span> x=pam(<span class="number">10</span>);</span><br><span class="line"><span class="keyword">double</span> y=(*pt)(<span class="number">10</span>);</span><br><span class="line"><span class="keyword">double</span> z=pt(<span class="number">10</span>);<span class="comment">//also valid</span></span><br></pre></td></tr></table></figure><h2 id="例子">例子</h2><p>假设我们需要设计一个名为<code>estimate()</code>的函数，用来计算运行指定行数代码所需要的时间，其中的算法由程序员自己设计决定。</p><p>假设程序员设计了两种计算时间的算法，分别为<code>pam()</code>和<code>betsy()</code>，其定义如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">pam</span> <span class="params">(<span class="keyword">int</span> lens)</span></span>&#123;</span><br><span class="line">  <span class="keyword">return</span> lens*<span class="number">1.5</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">betsy</span><span class="params">(<span class="keyword">int</span> lens)</span></span>&#123;</span><br><span class="line">  <span class="keyword">return</span> lens*<span class="number">2.0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>则函数<code>estimate()</code>直接将上面两个函数作为参数，使用如下格式：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">estimate(pam,code);</span><br><span class="line">estimate(betsy,code);</span><br></pre></td></tr></table></figure><p>而函数<code>estimate()</code>的声明和定义如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">estimate</span><span class="params">(<span class="keyword">double</span> (*pf)(<span class="keyword">int</span>),<span class="keyword">int</span> lens)</span></span>&#123;</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;lines&lt;&lt;<span class="string">&quot;lines will take:&quot;</span>;</span><br><span class="line">  <span class="built_in">cout</span>&lt;&lt;(*pf)(lens)&lt;&lt;<span class="string">&quot;hours&quot;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中，pf是指向函数的指针，(*pf)就是调用传进来的函数参数。</p><h2 id="深入探讨函数指针">深入探讨函数指针</h2><p>假设有下面这些函数原型，他们的参数列表和返回值类型都是相同的。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">const</span> <span class="keyword">double</span> *<span class="title">f1</span><span class="params">(<span class="keyword">const</span> <span class="keyword">double</span> ar[],<span class="keyword">int</span> n)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">const</span> <span class="keyword">double</span> *<span class="title">f2</span><span class="params">(<span class="keyword">const</span> <span class="keyword">double</span> [],<span class="keyword">int</span> n)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">const</span> <span class="keyword">double</span> *<span class="title">f3</span><span class="params">(<span class="keyword">const</span> <span class="keyword">double</span> *,<span class="keyword">int</span> n)</span></span>;<span class="comment">//在函数原型中可以省略标识符ar</span></span><br></pre></td></tr></table></figure><p>如果要声明一个指针p1，指向上面三个函数之一，则声明如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">double</span> *(*p1)(<span class="keyword">const</span> <span class="keyword">double</span> *,<span class="keyword">int</span>)=f1;<span class="comment">//声明的同时初始化</span></span><br></pre></td></tr></table></figure><p>也就是将函数名f1换成了(*p1)。</p><p>使用C++11的自动类型推断功能，代码要简单的多：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> p2=f2;</span><br></pre></td></tr></table></figure><p>也可以声明一个指针数组，同时初始化：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">double</span> *(*pa[<span class="number">3</span>])(<span class="keyword">const</span> <span class="keyword">double</span> *,<span class="keyword">int</span>)=&#123;f1,f2,f3&#125;;</span><br></pre></td></tr></table></figure><p>注意，此时不可以使用auto进行声明，因为auto只能用于单值初始化。</p><p>但是声明完pa后，可以用auto声明同样类型的数组：<code>auto pb=pa;</code></p><h2 id="用typedef进行简化">用typedef进行简化</h2><p>可以使用<strong>typedef</strong>在程序的开头为函数指针声明一个别名：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="keyword">const</span> <span class="keyword">double</span> *(*p_fun)(<span class="keyword">const</span> <span class="keyword">double</span> *,<span class="keyword">int</span>);</span><br><span class="line"><span class="comment">//使用p_fun别名来简化代码</span></span><br><span class="line">p_fun p1=f1;</span><br><span class="line">p_fun pa[<span class="number">3</span>]=&#123;f1,f2,f3&#125;;</span><br></pre></td></tr></table></figure><h2 id="参考资料">参考资料</h2><p>C++ Primer Plus 第六版 中文版</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;如何获取函数的地址&quot;&gt;如何获取函数的地址&lt;/h2&gt;
&lt;p&gt;函数名本身就是函数的地址。&lt;/p&gt;
&lt;p&gt;假设有一个函数&lt;code&gt;think()&lt;/code&gt;，则&lt;code&gt;think&lt;/code&gt;就是该函数的地址。&lt;/p&gt;
&lt;p&gt;要将函数作为参数进行传递，必须传递&lt;</summary>
      
    
    
    
    
    <category term="c++" scheme="http://example.com/tags/c/"/>
    
  </entry>
  
  <entry>
    <title>OpenGL模型加载</title>
    <link href="http://example.com/2020/10/13/OpenGL%E6%A8%A1%E5%9E%8B%E5%8A%A0%E8%BD%BD/"/>
    <id>http://example.com/2020/10/13/OpenGL%E6%A8%A1%E5%9E%8B%E5%8A%A0%E8%BD%BD/</id>
    <published>2020-10-13T14:00:00.000Z</published>
    <updated>2020-10-14T10:38:56.664Z</updated>
    
    <content type="html"><![CDATA[<p>是最近学OpenGL的阶段性总结。</p><p>学习网址：<a href="https://learnopengl-cn.github.io">learnopengl-cn</a></p><p>源码地址：<a href="https://github.com/Anastasiawangyx/learnOpenGL-demo/tree/main/demo3">github</a></p><p>对前三节，入门、光照、模型加载的总结。</p><p>关于如何使用OpenGL加载一个3D模型并显示在窗口中。</p><h1>使用OpenGL实现模型加载</h1><h2 id="初始化GLFW">初始化GLFW</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">glfwInit();</span><br><span class="line">    glfwWindowHint(GLFW_CONTEXT_VERSION_MAJOR, <span class="number">3</span>);</span><br><span class="line">    glfwWindowHint(GLFW_CONTEXT_VERSION_MINOR, <span class="number">3</span>);</span><br><span class="line">    glfwWindowHint(GLFW_OPENGL_PROFILE, GLFW_OPENGL_CORE_PROFILE);</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> __APPLE__</span></span><br><span class="line">    glfwWindowHint(GLFW_OPENGL_FORWARD_COMPAT, GL_TRUE);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure><h2 id="创建GLFW窗口">创建GLFW窗口</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">GLFWwindow* window = glfwCreateWindow(SCR_WIDTH, SCR_HEIGHT, <span class="string">&quot;LearnOpenGL&quot;</span>, <span class="literal">NULL</span>, <span class="literal">NULL</span>);</span><br><span class="line">    <span class="keyword">if</span> (window == <span class="literal">NULL</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Failed to create GLFW window&quot;</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">        glfwTerminate();</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="设置上下文">设置上下文</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">glfwMakeContextCurrent(window);</span><br></pre></td></tr></table></figure><h2 id="注册回调函数">注册回调函数</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">glfwSetFramebufferSizeCallback(window, framebuffer_size_callback);</span><br><span class="line"><span class="comment">//鼠标操作</span></span><br><span class="line">glfwSetCursorPosCallback(window, mouse_callback);</span><br><span class="line"><span class="comment">//鼠标滚轮</span></span><br><span class="line">glfwSetScrollCallback(window, scroll_callback);</span><br></pre></td></tr></table></figure><h2 id="加载GLAD模块">加载GLAD模块</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (!gladLoadGLLoader((GLADloadproc)glfwGetProcAddress))</span><br><span class="line">  &#123;</span><br><span class="line">      <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Failed to initialize GLAD&quot;</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">      <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h3 id=""></h3><h2 id="其他设置">其他设置</h2><h3 id="捕捉鼠标">捕捉鼠标</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">glfwSetInputMode(window, GLFW_CURSOR, GLFW_CURSOR_DISABLED);</span><br></pre></td></tr></table></figure><h3 id="y轴翻转">y轴翻转</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stbi_set_flip_vertically_on_load(<span class="literal">true</span>);</span><br></pre></td></tr></table></figure><h3 id="开启深度测试">开启深度测试</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">glEnable(GL_DEPTH_TEST);</span><br></pre></td></tr></table></figure><h2 id="设置Shader">设置Shader</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Shader <span class="title">ourShader</span><span class="params">(<span class="string">&quot;model_loading.vs&quot;</span>, <span class="string">&quot;model_loading.fs&quot;</span>)</span></span>;</span><br></pre></td></tr></table></figure><p>其中，vs代表顶点着色器，fs代表片段着色器。</p><h2 id="设置Model">设置Model</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Model <span class="title">ourModel</span><span class="params">(<span class="string">&quot;../backpack.obj&quot;</span>)</span></span>;</span><br></pre></td></tr></table></figure><h2 id="渲染循环">渲染循环</h2><p>我们需要一个循环，不断刷新屏幕，绘制图案。</p><p>涉及到一下几个操作：</p><ul><li>设置时间：为了平衡画面，我们会把每次渲染的时间记下来。</li><li>处理输入函数</li><li>清屏</li><li>激活Shader</li><li>设置MVP变换矩阵</li><li>Draw</li><li>交换缓存，处理IO事件</li></ul><p>代码如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// per-frame time logic</span></span><br><span class="line"><span class="keyword">float</span> currentFrame = glfwGetTime();</span><br><span class="line">deltaTime = currentFrame - lastFrame;</span><br><span class="line">lastFrame = currentFrame;</span><br><span class="line"></span><br><span class="line"><span class="comment">// input</span></span><br><span class="line"><span class="comment">// -----</span></span><br><span class="line">processInput(window);</span><br><span class="line"></span><br><span class="line"><span class="comment">// render</span></span><br><span class="line"><span class="comment">// ------</span></span><br><span class="line">glClearColor(<span class="number">0.05f</span>, <span class="number">0.05f</span>, <span class="number">0.05f</span>, <span class="number">1.0f</span>);</span><br><span class="line">glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);</span><br><span class="line"></span><br><span class="line"><span class="comment">// don&#x27;t forget to enable shader before setting uniforms</span></span><br><span class="line">ourShader.use();</span><br><span class="line"></span><br><span class="line"><span class="comment">// view/projection transformations</span></span><br><span class="line">glm::mat4 projection = glm::perspective(glm::radians(camera.Zoom), (<span class="keyword">float</span>)SCR_WIDTH / (<span class="keyword">float</span>)SCR_HEIGHT, <span class="number">0.1f</span>, <span class="number">100.0f</span>);</span><br><span class="line">glm::mat4 view = camera.GetViewMatrix();</span><br><span class="line">ourShader.setMat4(<span class="string">&quot;projection&quot;</span>, projection);</span><br><span class="line">ourShader.setMat4(<span class="string">&quot;view&quot;</span>, view);</span><br><span class="line"></span><br><span class="line"><span class="comment">// render the loaded model</span></span><br><span class="line">glm::mat4 model = glm::mat4(<span class="number">1.0f</span>);</span><br><span class="line">model = glm::translate(model, glm::vec3(<span class="number">0.0f</span>, <span class="number">0.0f</span>, <span class="number">0.0f</span>)); <span class="comment">// translate it down so it&#x27;s at the center of the scene</span></span><br><span class="line">model = glm::scale(model, glm::vec3(<span class="number">1.0f</span>, <span class="number">1.0f</span>, <span class="number">1.0f</span>));<span class="comment">// it&#x27;s a bit too big for our scene, so scale it down</span></span><br><span class="line">ourShader.setMat4(<span class="string">&quot;model&quot;</span>, model);</span><br><span class="line">ourModel.Draw(ourShader);</span><br><span class="line"></span><br><span class="line"><span class="comment">// glfw: swap buffers and poll IO events (keys pressed/released, mouse moved etc.)</span></span><br><span class="line"><span class="comment">// -------------------------------------------------------------------------------</span></span><br><span class="line">glfwSwapBuffers(window);</span><br><span class="line">glfwPollEvents();</span><br></pre></td></tr></table></figure><h2 id="关闭GLFW">关闭GLFW</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">glfwTerminate();</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br></pre></td></tr></table></figure><p>再来看看其他几个类。</p><h1>Shader.h</h1><p>着色器类（shader）的作用是读入着色器的源代码，并生成对应的着色器，如顶点着色器（vertex shader）、片段着色器（fragment shader），并将着色器编译链接到一个程序（program）上，使主程序可直接调用shader.use函数。</p><p>同时还要完成对着色器中变量的复制操作，这里使用uniform全局变量，这样就可以从程序中向着色器中赋值。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">Class Shader&#123;</span><br><span class="line">  <span class="keyword">public</span>:</span><br><span class="line">  <span class="keyword">unsigned</span> <span class="keyword">int</span> ID;</span><br><span class="line">  Shader(<span class="keyword">const</span> <span class="keyword">char</span>* vertexPath, <span class="keyword">const</span> <span class="keyword">char</span>* fragmentPath)&#123;</span><br><span class="line">      <span class="comment">//....</span></span><br><span class="line">      <span class="comment">//vertex shader</span></span><br><span class="line">      <span class="keyword">unsigned</span> <span class="keyword">int</span> vertex,fragment;</span><br><span class="line">      vertex = glCreateShader(GL_VERTEX_SHADER);</span><br><span class="line">      glShaderSource(vertex,<span class="number">1</span>,&amp;sourceCode,<span class="literal">NULL</span>);</span><br><span class="line">      glCompilerShader(vertex);</span><br><span class="line">    <span class="comment">//fragment shader</span></span><br><span class="line">      <span class="comment">//...</span></span><br><span class="line">      <span class="comment">//shader program</span></span><br><span class="line">      ID = glCreateProgram();</span><br><span class="line">      glAttachShader(ID,vertex);</span><br><span class="line">      glAttachShader(ID,fragment);</span><br><span class="line">      glLinkProgram(ID);</span><br><span class="line">      <span class="comment">//link成功即可删除shader</span></span><br><span class="line">      glDeleteShader(vertex);</span><br><span class="line">      glDeleteShader(fragment);</span><br><span class="line">    &#125;</span><br><span class="line">  <span class="comment">//use函数</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">use</span><span class="params">()</span></span>&#123;</span><br><span class="line">    glUseProgram(ID);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//设置各种格式的unnifrom</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">setInt</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span> &amp;name, <span class="keyword">int</span> value)</span> <span class="keyword">const</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        glUniform1i(glGetUniformLocation(ID, name.c_str()), value);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1>camera.h</h1><p>摄像机类定义了我们视角相关的变量和函数，它决定了我们如何“看向”窗口“里的物体，以及在我们的视角发生变化时（如移动鼠标、键盘），图形将会发生什么样的变化，这个变化主要是在MVP矩阵中的view矩阵体现出来的。</p><p>不多做介绍。</p><h1>Mesh.h</h1><p>网格(Mesh)代表的是单个的可绘制实体，也就是一个“单独”的物体。</p><p>一个网格应该至少需要一系列的顶点，每个顶点包含一个位置向量、一个法向量和一个纹理坐标向量。</p><p>一个网格还应该包含用于索引绘制的索引以及纹理形式的材质数据（漫反射/镜面光贴图）。</p><p>我们在网格类中将顶点和纹理这两个结构抽出来，定义两个结构体。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Vertex</span> &#123;</span></span><br><span class="line">    glm::vec3 Position;</span><br><span class="line">    glm::vec3 Normal;</span><br><span class="line">    glm::vec2 TexCoords;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Texture</span> &#123;</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> id;</span><br><span class="line">    <span class="built_in">string</span> type;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>网格的结构也就可以如下定义：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mesh</span> &#123;</span></span><br><span class="line">    <span class="keyword">public</span>:</span><br><span class="line">        <span class="comment">/*  网格数据  */</span></span><br><span class="line">        <span class="built_in">vector</span>&lt;Vertex&gt; vertices;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">unsigned</span> <span class="keyword">int</span>&gt; indices;</span><br><span class="line">        <span class="built_in">vector</span>&lt;Texture&gt; textures;</span><br><span class="line">        <span class="comment">/*  函数  */</span></span><br><span class="line">        Mesh(<span class="built_in">vector</span>&lt;Vertex&gt; vertices, <span class="built_in">vector</span>&lt;<span class="keyword">unsigned</span> <span class="keyword">int</span>&gt; indices, <span class="built_in">vector</span>&lt;Texture&gt; textures)&#123;</span><br><span class="line">          <span class="keyword">this</span>-&gt;vertices = vertices;</span><br><span class="line">    <span class="keyword">this</span>-&gt;indices = indices;</span><br><span class="line">    <span class="keyword">this</span>-&gt;textures = textures;</span><br><span class="line"></span><br><span class="line">    setupMesh();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="function"><span class="keyword">void</span> <span class="title">Draw</span><span class="params">(Shader shader)</span></span>;</span><br><span class="line">    <span class="keyword">private</span>:</span><br><span class="line">        <span class="comment">/*  渲染数据  */</span></span><br><span class="line">        <span class="keyword">unsigned</span> <span class="keyword">int</span> VAO, VBO, EBO;</span><br><span class="line">        <span class="comment">/*  函数  */</span></span><br><span class="line">        <span class="function"><span class="keyword">void</span> <span class="title">setupMesh</span><span class="params">()</span></span>;</span><br><span class="line">&#125;; </span><br></pre></td></tr></table></figure><p>在setupMesh中，完成缓冲的配置，并通过顶点属性指针定义顶点着色器的布局。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">setupMesh</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    glGenVertexArrays(<span class="number">1</span>, &amp;VAO);</span><br><span class="line">    glGenBuffers(<span class="number">1</span>, &amp;VBO);</span><br><span class="line">    glGenBuffers(<span class="number">1</span>, &amp;EBO);</span><br><span class="line"></span><br><span class="line">    glBindVertexArray(VAO);</span><br><span class="line">    glBindBuffer(GL_ARRAY_BUFFER, VBO);</span><br><span class="line"></span><br><span class="line">    glBufferData(GL_ARRAY_BUFFER, vertices.size() * <span class="keyword">sizeof</span>(Vertex), &amp;vertices[<span class="number">0</span>], GL_STATIC_DRAW);  </span><br><span class="line"></span><br><span class="line">    glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, EBO);</span><br><span class="line">    glBufferData(GL_ELEMENT_ARRAY_BUFFER, indices.size() * <span class="keyword">sizeof</span>(<span class="keyword">unsigned</span> <span class="keyword">int</span>), </span><br><span class="line">                 &amp;indices[<span class="number">0</span>], GL_STATIC_DRAW);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 顶点位置</span></span><br><span class="line">    glEnableVertexAttribArray(<span class="number">0</span>);   </span><br><span class="line">    glVertexAttribPointer(<span class="number">0</span>, <span class="number">3</span>, GL_FLOAT, GL_FALSE, <span class="keyword">sizeof</span>(Vertex), (<span class="keyword">void</span>*)<span class="number">0</span>);</span><br><span class="line">    <span class="comment">// 顶点法线</span></span><br><span class="line">    glEnableVertexAttribArray(<span class="number">1</span>);   </span><br><span class="line">    glVertexAttribPointer(<span class="number">1</span>, <span class="number">3</span>, GL_FLOAT, GL_FALSE, <span class="keyword">sizeof</span>(Vertex), (<span class="keyword">void</span>*)offsetof(Vertex, Normal));</span><br><span class="line">    <span class="comment">// 顶点纹理坐标</span></span><br><span class="line">    glEnableVertexAttribArray(<span class="number">2</span>);   </span><br><span class="line">    glVertexAttribPointer(<span class="number">2</span>, <span class="number">2</span>, GL_FLOAT, GL_FALSE, <span class="keyword">sizeof</span>(Vertex), (<span class="keyword">void</span>*)offsetof(Vertex, TexCoords));</span><br><span class="line"></span><br><span class="line">    glBindVertexArray(<span class="number">0</span>);</span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure><p>这里提一下VBO、VAO、EBO整体流程。</p><ul><li><p>声明：<code>unsigned int VAO, VBO, EBO</code></p></li><li><p>生成：<code>glGenBuffers(1, &amp;VBO);</code>，<code>glGenVertexArrays(1, &amp;VAO)</code></p></li><li><p>绑定：<code> glBindVertexArray(VAO); glBindBuffer(GL_ARRAY_BUFFER, VBO);glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, EBO);</code></p></li><li><p>传送数据：<code>glBufferData(GL_ARRAY_BUFFER, vertices.size() * sizeof(Vertex), &amp;vertices[0], GL_STATIC_DRAW); </code></p></li><li><p>设置顶点属性：<code>glEnableVertexAttribArray(0); </code></p><p><code>glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, sizeof(Vertex), (void*)0);</code></p></li><li><p>解绑：<code>glBindVertexArray(0);</code></p></li></ul><p>Draw函数是我们在渲染的时候真正调用的，用来绘制图形的函数。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Draw</span><span class="params">(Shader &amp;shader)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="comment">// bind appropriate textures</span></span><br><span class="line">        <span class="keyword">unsigned</span> <span class="keyword">int</span> diffuseNr  = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">unsigned</span> <span class="keyword">int</span> specularNr = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">unsigned</span> <span class="keyword">int</span> normalNr   = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">unsigned</span> <span class="keyword">int</span> heightNr   = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">unsigned</span> <span class="keyword">int</span> i = <span class="number">0</span>; i &lt; textures.size(); i++)</span><br><span class="line">        &#123;</span><br><span class="line">            glActiveTexture(GL_TEXTURE0 + i); <span class="comment">// active proper texture unit before binding</span></span><br><span class="line">            <span class="comment">// retrieve texture number (the N in diffuse_textureN)</span></span><br><span class="line">            <span class="built_in">string</span> number;</span><br><span class="line">            <span class="built_in">string</span> name = textures[i].type;</span><br><span class="line">            <span class="keyword">if</span>(name == <span class="string">&quot;texture_diffuse&quot;</span>)</span><br><span class="line">                number = <span class="built_in">std</span>::to_string(diffuseNr++);</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span>(name == <span class="string">&quot;texture_specular&quot;</span>)</span><br><span class="line">                number = <span class="built_in">std</span>::to_string(specularNr++); <span class="comment">// transfer unsigned int to stream</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span>(name == <span class="string">&quot;texture_normal&quot;</span>)</span><br><span class="line">                number = <span class="built_in">std</span>::to_string(normalNr++); <span class="comment">// transfer unsigned int to stream</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span>(name == <span class="string">&quot;texture_height&quot;</span>)</span><br><span class="line">                number = <span class="built_in">std</span>::to_string(heightNr++); <span class="comment">// transfer unsigned int to stream</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">// now set the sampler to the correct texture unit</span></span><br><span class="line">            glUniform1i(glGetUniformLocation(shader.ID, (name + number).c_str()), i);</span><br><span class="line">            <span class="comment">// and finally bind the texture</span></span><br><span class="line">            glBindTexture(GL_TEXTURE_2D, textures[i].id);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//上面都是纹理相关</span></span><br><span class="line">        <span class="comment">// 下面开始画一个mesh</span></span><br><span class="line">        glBindVertexArray(VAO);</span><br><span class="line">        glDrawElements(GL_TRIANGLES, indices.size(), GL_UNSIGNED_INT, <span class="number">0</span>);</span><br><span class="line">        glBindVertexArray(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// always good practice to set everything back to defaults once configured.</span></span><br><span class="line">        glActiveTexture(GL_TEXTURE0);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>可以看到VAO有一个<strong>绑定——画——解绑</strong>的过程。</p><h1>Model.h</h1><p>一个模型类完整地表示一个模型，或者说是包含多个网格，甚至是多个物体的模型。我们会使用Assimp来加载模型，并将它转换(Translate)至多个Mesh对象。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// model data</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;Texture&gt; textures_loaded;<span class="comment">// stores all the textures loaded so far, optimization to make sure textures aren&#x27;t loaded more than once.</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;Mesh&gt;    meshes;</span><br><span class="line">    <span class="built_in">string</span> directory;</span><br><span class="line">    <span class="keyword">bool</span> gammaCorrection;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// constructor, expects a filepath to a 3D model.</span></span><br><span class="line">    Model(<span class="built_in">string</span> <span class="keyword">const</span> &amp;path, <span class="keyword">bool</span> gamma = <span class="literal">false</span>) : gammaCorrection(gamma)</span><br><span class="line">    &#123;</span><br><span class="line">        loadModel(path);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// draws the model, and thus all its meshes</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Draw</span><span class="params">(Shader &amp;shader)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">unsigned</span> <span class="keyword">int</span> i = <span class="number">0</span>; i &lt; meshes.size(); i++)</span><br><span class="line">            meshes[i].Draw(shader);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="comment">// loads a model with supported ASSIMP extensions from file and stores the resulting meshes in the meshes vector.</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">loadModel</span><span class="params">(<span class="built_in">string</span> <span class="keyword">const</span> &amp;path)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="comment">// read file via ASSIMP</span></span><br><span class="line">        Assimp::Importer importer;</span><br><span class="line">        <span class="keyword">const</span> aiScene* scene = importer.ReadFile(path, aiProcess_Triangulate | aiProcess_GenSmoothNormals | aiProcess_FlipUVs | aiProcess_CalcTangentSpace);</span><br><span class="line">        <span class="comment">// check for errors</span></span><br><span class="line">        <span class="keyword">if</span>(!scene || scene-&gt;mFlags &amp; AI_SCENE_FLAGS_INCOMPLETE || !scene-&gt;mRootNode) <span class="comment">// if is Not Zero</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;ERROR::ASSIMP:: &quot;</span> &lt;&lt; importer.GetErrorString() &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// retrieve the directory path of the filepath</span></span><br><span class="line">        directory = path.substr(<span class="number">0</span>, path.find_last_of(<span class="string">&#x27;/&#x27;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// process ASSIMP&#x27;s root node recursively</span></span><br><span class="line">        processNode(scene-&gt;mRootNode, scene);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// processes a node in a recursive fashion. Processes each individual mesh located at the node and repeats this process on its children nodes (if any).</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">processNode</span><span class="params">(aiNode *node, <span class="keyword">const</span> aiScene *scene)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="comment">// process each mesh located at the current node</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">unsigned</span> <span class="keyword">int</span> i = <span class="number">0</span>; i &lt; node-&gt;mNumMeshes; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// the node object only contains indices to index the actual objects in the scene.</span></span><br><span class="line">            <span class="comment">// the scene contains all the data, node is just to keep stuff organized (like relations between nodes).</span></span><br><span class="line">            aiMesh* mesh = scene-&gt;mMeshes[node-&gt;mMeshes[i]];</span><br><span class="line">            meshes.push_back(processMesh(mesh, scene));</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// after we&#x27;ve processed all of the meshes (if any) we then recursively process each of the children nodes</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">unsigned</span> <span class="keyword">int</span> i = <span class="number">0</span>; i &lt; node-&gt;mNumChildren; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            processNode(node-&gt;mChildren[i], scene);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">Mesh <span class="title">processMesh</span><span class="params">(aiMesh *mesh, <span class="keyword">const</span> aiScene *scene)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="comment">// data to fill</span></span><br><span class="line">        <span class="built_in">vector</span>&lt;Vertex&gt; vertices;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">unsigned</span> <span class="keyword">int</span>&gt; indices;</span><br><span class="line">        <span class="built_in">vector</span>&lt;Texture&gt; textures;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// walk through each of the mesh&#x27;s vertices</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">unsigned</span> <span class="keyword">int</span> i = <span class="number">0</span>; i &lt; mesh-&gt;mNumVertices; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            Vertex vertex;</span><br><span class="line">            glm::vec3 <span class="built_in">vector</span>; <span class="comment">// we declare a placeholder vector since assimp uses its own vector class that doesn&#x27;t directly convert to glm&#x27;s vec3 class so we transfer the data to this placeholder glm::vec3 first.</span></span><br><span class="line">            <span class="comment">// positions</span></span><br><span class="line">            <span class="built_in">vector</span>.x = mesh-&gt;mVertices[i].x;</span><br><span class="line">            <span class="built_in">vector</span>.y = mesh-&gt;mVertices[i].y;</span><br><span class="line">            <span class="built_in">vector</span>.z = mesh-&gt;mVertices[i].z;</span><br><span class="line">            vertex.Position = <span class="built_in">vector</span>;</span><br><span class="line">            <span class="comment">// normals</span></span><br><span class="line">            <span class="keyword">if</span> (mesh-&gt;HasNormals())</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="built_in">vector</span>.x = mesh-&gt;mNormals[i].x;</span><br><span class="line">                <span class="built_in">vector</span>.y = mesh-&gt;mNormals[i].y;</span><br><span class="line">                <span class="built_in">vector</span>.z = mesh-&gt;mNormals[i].z;</span><br><span class="line">                vertex.Normal = <span class="built_in">vector</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// texture coordinates</span></span><br><span class="line">            <span class="keyword">if</span>(mesh-&gt;mTextureCoords[<span class="number">0</span>]) <span class="comment">// does the mesh contain texture coordinates?</span></span><br><span class="line">            &#123;</span><br><span class="line">                glm::vec2 vec;</span><br><span class="line">                <span class="comment">// a vertex can contain up to 8 different texture coordinates. We thus make the assumption that we won&#x27;t</span></span><br><span class="line">                <span class="comment">// use models where a vertex can have multiple texture coordinates so we always take the first set (0).</span></span><br><span class="line">                vec.x = mesh-&gt;mTextureCoords[<span class="number">0</span>][i].x;</span><br><span class="line">                vec.y = mesh-&gt;mTextureCoords[<span class="number">0</span>][i].y;</span><br><span class="line">                vertex.TexCoords = vec;</span><br><span class="line">                <span class="comment">// tangent</span></span><br><span class="line">                <span class="built_in">vector</span>.x = mesh-&gt;mTangents[i].x;</span><br><span class="line">                <span class="built_in">vector</span>.y = mesh-&gt;mTangents[i].y;</span><br><span class="line">                <span class="built_in">vector</span>.z = mesh-&gt;mTangents[i].z;</span><br><span class="line">                vertex.Tangent = <span class="built_in">vector</span>;</span><br><span class="line">                <span class="comment">// bitangent</span></span><br><span class="line">                <span class="built_in">vector</span>.x = mesh-&gt;mBitangents[i].x;</span><br><span class="line">                <span class="built_in">vector</span>.y = mesh-&gt;mBitangents[i].y;</span><br><span class="line">                <span class="built_in">vector</span>.z = mesh-&gt;mBitangents[i].z;</span><br><span class="line">                vertex.Bitangent = <span class="built_in">vector</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                vertex.TexCoords = glm::vec2(<span class="number">0.0f</span>, <span class="number">0.0f</span>);</span><br><span class="line"></span><br><span class="line">            vertices.push_back(vertex);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// now walk through each of the mesh&#x27;s faces (a face is a mesh its triangle) and retrieve the corresponding vertex indices.</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">unsigned</span> <span class="keyword">int</span> i = <span class="number">0</span>; i &lt; mesh-&gt;mNumFaces; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            aiFace face = mesh-&gt;mFaces[i];</span><br><span class="line">            <span class="comment">// retrieve all indices of the face and store them in the indices vector</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">unsigned</span> <span class="keyword">int</span> j = <span class="number">0</span>; j &lt; face.mNumIndices; j++)</span><br><span class="line">                indices.push_back(face.mIndices[j]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// process materials</span></span><br><span class="line">        aiMaterial* material = scene-&gt;mMaterials[mesh-&gt;mMaterialIndex];</span><br><span class="line">        <span class="comment">// we assume a convention for sampler names in the shaders. Each diffuse texture should be named</span></span><br><span class="line">        <span class="comment">// as &#x27;texture_diffuseN&#x27; where N is a sequential number ranging from 1 to MAX_SAMPLER_NUMBER.</span></span><br><span class="line">        <span class="comment">// Same applies to other texture as the following list summarizes:</span></span><br><span class="line">        <span class="comment">// diffuse: texture_diffuseN</span></span><br><span class="line">        <span class="comment">// specular: texture_specularN</span></span><br><span class="line">        <span class="comment">// normal: texture_normalN</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. diffuse maps</span></span><br><span class="line">        <span class="built_in">vector</span>&lt;Texture&gt; diffuseMaps = loadMaterialTextures(material, aiTextureType_DIFFUSE, <span class="string">&quot;texture_diffuse&quot;</span>);</span><br><span class="line">        textures.insert(textures.end(), diffuseMaps.begin(), diffuseMaps.end());</span><br><span class="line">        <span class="comment">// 2. specular maps</span></span><br><span class="line">        <span class="built_in">vector</span>&lt;Texture&gt; specularMaps = loadMaterialTextures(material, aiTextureType_SPECULAR, <span class="string">&quot;texture_specular&quot;</span>);</span><br><span class="line">        textures.insert(textures.end(), specularMaps.begin(), specularMaps.end());</span><br><span class="line">        <span class="comment">// 3. normal maps</span></span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;Texture&gt; normalMaps = loadMaterialTextures(material, aiTextureType_HEIGHT, <span class="string">&quot;texture_normal&quot;</span>);</span><br><span class="line">        textures.insert(textures.end(), normalMaps.begin(), normalMaps.end());</span><br><span class="line">        <span class="comment">// 4. height maps</span></span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;Texture&gt; heightMaps = loadMaterialTextures(material, aiTextureType_AMBIENT, <span class="string">&quot;texture_height&quot;</span>);</span><br><span class="line">        textures.insert(textures.end(), heightMaps.begin(), heightMaps.end());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// return a mesh object created from the extracted mesh data</span></span><br><span class="line">        <span class="keyword">return</span> Mesh(vertices, indices, textures);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// checks all material textures of a given type and loads the textures if they&#x27;re not loaded yet.</span></span><br><span class="line">    <span class="comment">// the required info is returned as a Texture struct.</span></span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;Texture&gt; <span class="title">loadMaterialTextures</span><span class="params">(aiMaterial *mat, aiTextureType type, <span class="built_in">string</span> typeName)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;Texture&gt; textures;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">unsigned</span> <span class="keyword">int</span> i = <span class="number">0</span>; i &lt; mat-&gt;GetTextureCount(type); i++)</span><br><span class="line">        &#123;</span><br><span class="line">            aiString str;</span><br><span class="line">            mat-&gt;GetTexture(type, i, &amp;str);</span><br><span class="line">            <span class="comment">// check if texture was loaded before and if so, continue to next iteration: skip loading a new texture</span></span><br><span class="line">            <span class="keyword">bool</span> skip = <span class="literal">false</span>;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">unsigned</span> <span class="keyword">int</span> j = <span class="number">0</span>; j &lt; textures_loaded.size(); j++)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>(<span class="built_in">std</span>::<span class="built_in">strcmp</span>(textures_loaded[j].path.data(), str.C_Str()) == <span class="number">0</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    textures.push_back(textures_loaded[j]);</span><br><span class="line">                    skip = <span class="literal">true</span>; <span class="comment">// a texture with the same filepath has already been loaded, continue to next one. (optimization)</span></span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(!skip)</span><br><span class="line">            &#123;   <span class="comment">// if texture hasn&#x27;t been loaded already, load it</span></span><br><span class="line">                Texture texture;</span><br><span class="line">                texture.id = TextureFromFile(str.C_Str(), <span class="keyword">this</span>-&gt;directory);</span><br><span class="line">                texture.type = typeName;</span><br><span class="line">                texture.path = str.C_Str();</span><br><span class="line">                textures.push_back(texture);</span><br><span class="line">                textures_loaded.push_back(texture);  <span class="comment">// store it as texture loaded for entire model, to ensure we won&#x27;t unnecesery load duplicate textures.</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> textures;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;是最近学OpenGL的阶段性总结。&lt;/p&gt;
&lt;p&gt;学习网址：&lt;a href=&quot;https://learnopengl-cn.github.io&quot;&gt;learnopengl-cn&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;源码地址：&lt;a href=&quot;https://github.com/Anast</summary>
      
    
    
    
    <category term="opengl" scheme="http://example.com/categories/opengl/"/>
    
    
    <category term="c++" scheme="http://example.com/tags/c/"/>
    
    <category term="opengl" scheme="http://example.com/tags/opengl/"/>
    
    <category term="图形学" scheme="http://example.com/tags/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>OpenGL中的VAO与VBO</title>
    <link href="http://example.com/2020/10/08/OpenGL%E4%B8%AD%E7%9A%84VAO%E4%B8%8EVBO/"/>
    <id>http://example.com/2020/10/08/OpenGL%E4%B8%AD%E7%9A%84VAO%E4%B8%8EVBO/</id>
    <published>2020-10-08T10:45:00.000Z</published>
    <updated>2020-10-08T11:14:07.517Z</updated>
    
    <content type="html"><![CDATA[<p>最近在学习OpenGL的时候，看到了一篇关于讲解VAO和VBO比较好的文章，这里做一下搬运和改良。</p><p>原文地址：<a href="http://www.zwqxin.com/archives/opengl/vao-and-vbo-stuff.html">AB是一家？VAO与VBO</a></p><h2 id="VBO">VBO</h2><p>与其他buffer object一样，VBO归根到底是显卡存储空间里的一块缓存区(Buffer)而已，这个Buffer有它的名字(VBO的ID)，OpenGL在GPU的某处记录着这个ID和对应的显存地址（或者地址偏移，类似内存）。用代码看看吧：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//生成一个Buffer的ID，不管是什么类型的  </span></span><br><span class="line">glGenBuffers(<span class="number">1</span>, &amp;VBO);   </span><br><span class="line"><span class="comment">//绑定ID，同时也指定该ID对应的buffer的信息类型是GL_ARRAY_BUFFER  </span></span><br><span class="line">glBindBuffer(GL_ARRAY_BUFFER, VBO);  </span><br><span class="line"><span class="comment">//为该ID指定一块指定大小的存储区域（区域的位置大抵由末参数影响),  传输数据      </span></span><br><span class="line">glBufferData(GL_ARRAY_BUFFER, <span class="keyword">sizeof</span>(Data), Data, GL_STATIC_DRAW);  </span><br></pre></td></tr></table></figure><p>这里是VBO的初始化阶段。在这里我们看到了这是对位置，还是颜色，还是纹理坐标，还是法线，还是其他顶点属性进行设置的吗？是的，这个信息是：起码在初始化阶段，一个VBO对于交给它存储的数据到底是什么，完全不知道。这些信息都是在渲染的时候确定的。</p><p>以下是渲染代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">glBindBuffer(GL_ARRAY_BUFFER, PositionVBO);  </span><br><span class="line">glEnableVertexAttribArray(VAT_POSITION);  </span><br><span class="line">glVertexAttribPointer(VAT_POSITION, <span class="number">2</span>, GL_INT, GL_FALSE, <span class="number">0</span>, <span class="literal">NULL</span>);  </span><br><span class="line">  </span><br><span class="line">glBindBuffer(GL_ARRAY_BUFFER, TexcoordVBO);  </span><br><span class="line">glEnableVertexAttribArray(VAT_TEXCOORD);  </span><br><span class="line">glVertexAttribPointer(VAT_TEXCOORD, <span class="number">2</span>, GL_INT, GL_FALSE, <span class="number">0</span>, <span class="literal">NULL</span>);  </span><br><span class="line">  </span><br><span class="line">glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, IndexVBO);  </span><br><span class="line"></span><br><span class="line">glDrawElements(GL_TRIANGLES, <span class="number">6</span>, GL_UNSIGNED_SHORT, <span class="literal">NULL</span>);  </span><br><span class="line">  </span><br><span class="line">glDisableVertexAttribArray(VAT_POSITION);  </span><br><span class="line">glDisableVertexAttribArray(VAT_TEXCOORD);  </span><br><span class="line">  </span><br><span class="line">glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, <span class="literal">NULL</span>);  </span><br><span class="line">glBindBuffer(GL_ARRAY_BUFFER, <span class="literal">NULL</span>); </span><br></pre></td></tr></table></figure><p>在这段渲染代码中，<code>glVertexAttribPointer(VAT_POSITION, 2, GL_INT, GL_FALSE, 0, NULL) </code>函数指定了VBO里的是什么数据——顶点位置，int类型，2个int指定一个顶点位置，在区域里无偏移的采集数据等等。</p><p>之后的<code>glDrawElements</code>只不过根据组织模式(GL_TRIANGLES)和索引数据去采集VBO里的这些数据罢了，具体的表现是，它从<code>GL-context</code>获取了<code>glBindBuffer</code>指定的位置等信息，进行绘制。</p><p>总的来说，VBO在渲染阶段才指定数据位置和“顶点信息”(Vertex Specification)，然后根据此信息去解析缓存区里的数据，联系这两者中间的桥梁是GL-Contenxt。</p><p>GL-context整个程序一般只有一个，所以如果一个渲染流程里有两份不同的绘制代码，GL-context就负责在它们之间进行状态切换。这也是为什么要在渲染过程中，在每份绘制代码之中有glBindBuffer/glEnableVertexAttribArray/glVertexAttribPointer。</p><p>那么优化方法就来了——把这些都放到初始化时候完成吧！——这样做的限制条件是“负责记录状态的GL-context整个程序一般只有一个”，那么就不直接用GL-context记录，用别的东西做状态记录吧——这个东西针对&quot;每份绘制代码“有一个，记录该次绘制所需要的所有VBO所需信息，把它保存到GPU特定位置，绘制的时候直接在这个位置取信息绘制。</p><h2 id="VAO">VAO</h2><p>VAO的全名是Vertex Array Object，首先，它不是Buffer-Object，所以不用作存储数据；其次，它针对”顶点“而言，也就是说它跟”顶点的绘制“息息相关。</p><p>按上所述，它的定位是state-object（状态对象，记录存储状态信息）。这明显区别于buffer-object。VAO记录的是一次绘制中做需要的信息，这包括</p><ul><li><p>”数据在哪里-glBindBuffer(GL_ARRAY_BUFFER)“、</p></li><li><p>”数据的格式是怎样的-glVertexAttribPointer“（顶点位置的数据在哪里，顶点位置的数据的格式是怎样的/纹理坐标的数据在哪里，纹理坐标的数据的格式是怎样的…视乎你让它关联多少个VBO、VBO里有多少种数据）</p><p>顺带一提的是，这里的状态还包括这些属性关联的shader-attribute的location的启用（glEnableVertexAttribArray）、这些顶点属性对应的顶点索引数据的位置（glBindBuffer(GL_ELEMENT_ARRAY_BUFFER)，如果你指定了的话）。</p></li></ul><p>使用VAO可以使渲染的部分变得简单。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;最近在学习OpenGL的时候，看到了一篇关于讲解VAO和VBO比较好的文章，这里做一下搬运和改良。&lt;/p&gt;
&lt;p&gt;原文地址：&lt;a href=&quot;http://www.zwqxin.com/archives/opengl/vao-and-vbo-stuff.html&quot;&gt;AB是一</summary>
      
    
    
    
    <category term="opengl" scheme="http://example.com/categories/opengl/"/>
    
    
    <category term="c++" scheme="http://example.com/tags/c/"/>
    
    <category term="opengl" scheme="http://example.com/tags/opengl/"/>
    
  </entry>
  
  <entry>
    <title>OpenGL之GLAD初始化</title>
    <link href="http://example.com/2020/10/08/OpenGL%E4%B9%8BGLAD%E5%88%9D%E5%A7%8B%E5%8C%96/"/>
    <id>http://example.com/2020/10/08/OpenGL%E4%B9%8BGLAD%E5%88%9D%E5%A7%8B%E5%8C%96/</id>
    <published>2020-10-08T08:03:54.000Z</published>
    <updated>2020-10-08T11:13:52.738Z</updated>
    
    <content type="html"><![CDATA[<h1>GLAD初始化</h1><p>在GLAD初始化时，会用到下面的语句:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (!gladLoadGLLoader((GLADloadproc)glfwGetProcAddress))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Failed to initialize GLAD&quot;</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>并且在我的代码中，如果这段初始化代码的位置放置有问题，run的时候就直接输出<code>Fail to initialize GLAD</code>。</p><p>如果按照下面的顺序放置，就会fail</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">if</span> (!gladLoadGLLoader((GLADloadproc)glfwGetProcAddress))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Failed to initialize GLAD&quot;</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">glfwMakeContextCurrent(window);</span><br></pre></td></tr></table></figure><p>其中，<code>glfwMakeContextCurrent(window)</code>函数是为了添加上下文。</p><p>想要正确的运行，要先添加上下文，再初始化GLAD。</p><p>原因未知，正在学习中。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;GLAD初始化&lt;/h1&gt;
&lt;p&gt;在GLAD初始化时，会用到下面的语句:&lt;/p&gt;
&lt;figure class=&quot;highlight c++&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;</summary>
      
    
    
    
    <category term="opengl" scheme="http://example.com/categories/opengl/"/>
    
    
    <category term="c++" scheme="http://example.com/tags/c/"/>
    
    <category term="opengl" scheme="http://example.com/tags/opengl/"/>
    
  </entry>
  
  <entry>
    <title>Mac下Clion配置openGL环境</title>
    <link href="http://example.com/2020/10/08/Mac%E4%B8%8BClion%E9%85%8D%E7%BD%AEopenGL%E7%8E%AF%E5%A2%83/"/>
    <id>http://example.com/2020/10/08/Mac%E4%B8%8BClion%E9%85%8D%E7%BD%AEopenGL%E7%8E%AF%E5%A2%83/</id>
    <published>2020-10-08T07:13:12.000Z</published>
    <updated>2020-10-08T11:13:30.537Z</updated>
    
    <content type="html"><![CDATA[<h1>CMakeLists.txt</h1><hr><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">cmake_minimum_required(VERSION <span class="number">3.17</span>)</span><br><span class="line">project(learn1)</span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span>(CMAKE_CXX_STANDARD <span class="number">14</span>)</span><br><span class="line"></span><br><span class="line"># 添加头文件</span><br><span class="line"><span class="built_in">set</span>(GLFW_H /usr/local/Cellar/glfw/<span class="number">3.3</span><span class="number">.2</span>/include/GLFW)</span><br><span class="line"><span class="built_in">set</span>(GLAD_H /usr/local/include/glad)</span><br><span class="line">include_directories($&#123;GLFW_H&#125; $&#123;GLAD_H&#125;)</span><br><span class="line"></span><br><span class="line"># 添加目标链接</span><br><span class="line"><span class="built_in">set</span>(GLFW_LINK /usr/local/Cellar/glfw/<span class="number">3.3</span><span class="number">.2</span>/lib/libglfw<span class="number">.3</span>.dylib)</span><br><span class="line">link_libraries($&#123;OPENGL&#125; $&#123;GLFW_LINK&#125;)</span><br><span class="line"></span><br><span class="line">link_libraries(/usr/local/lib)</span><br><span class="line">include_directories(/usr/local/include)</span><br><span class="line">add_executable(learn1 main.cpp glad.c)</span><br><span class="line"><span class="keyword">if</span> (APPLE)</span><br><span class="line">    target_link_libraries(learn1 <span class="string">&quot;-framework OpenGL&quot;</span>)</span><br><span class="line">    target_link_libraries(learn1 <span class="string">&quot;-framework GLUT&quot;</span>)</span><br><span class="line">endif()</span><br></pre></td></tr></table></figure><p>其中，learn1是当前项目的名称。</p><p>参考博客：<a href="https://tanwenbo.top/course/mac-shi-yongclion-pei-zhiopenglglewgladglfw.html">链接</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;CMakeLists.txt&lt;/h1&gt;
&lt;hr&gt;
&lt;figure class=&quot;highlight c&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line</summary>
      
    
    
    
    <category term="c++" scheme="http://example.com/categories/c/"/>
    
    
    <category term="c++" scheme="http://example.com/tags/c/"/>
    
    <category term="opengl" scheme="http://example.com/tags/opengl/"/>
    
  </entry>
  
  <entry>
    <title>eigen</title>
    <link href="http://example.com/2020/09/25/Eigen/"/>
    <id>http://example.com/2020/09/25/Eigen/</id>
    <published>2020-09-25T09:45:06.000Z</published>
    <updated>2020-10-08T11:15:40.291Z</updated>
    
    <content type="html"><![CDATA[<p>在使用Eigen库做矩阵变换的时候，有时候会发生错误，而且Eigen库坑爹的一点是他不会把错误在哪一行告诉你，而是只给出一个错误信息，这一点就很烦。</p><p>笔者使用Eigen库的时候，就遇到了下面的报错<code>error: static_assert failed due to requirement’(int(Eigen::internal::size_of_xpr_at_compile_time&lt;Eigen::Matrix&lt;float, 3, 3, 0, 3, 3&gt;…</code></p><p>当然，实际报错的内容比上面要长，我也只截取了一段错误信息。</p><p>根据网上各种资料显示，出现这个错误基本上都是与<strong>矩阵的维度</strong>有关。</p><p>这里贴一下我的代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Eigen::Matrix4f R=Eigen::Matrix4f::Identity();</span><br><span class="line">Eigen::Matrix3f N;</span><br><span class="line">N&lt;&lt;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="number">3</span>;i++)&#123;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;<span class="number">3</span>;j++)&#123;</span><br><span class="line">    R(i,j)=N(i,j);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里的想法很简单，R是一个四维的单位矩阵，N是一个三维的矩阵，现在我想用N的9个数填满R左上角3*3的矩阵，其余的不变，于是用了一个双重循环。</p><p>然后就编译错误。</p><p>这里主要还是错在这一句<code>R(i,j)=N(i,j)</code>，在Eigen中，R与N维数不同，不能这么写。</p><p>虽然我觉得很扯，毕竟我是取了值再去赋值的。但是事实是，确实就是这个错误。</p><p>那没办法，只能自己硬改了。</p><p>我还想过，用一个3*3的<strong>数组</strong>来当中间的赋值媒介，后来依然不行，依旧报上面的错误。</p><p>所以Eigen库虽然好用，但有些时候还是挺坑的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在使用Eigen库做矩阵变换的时候，有时候会发生错误，而且Eigen库坑爹的一点是他不会把错误在哪一行告诉你，而是只给出一个错误信息，这一点就很烦。&lt;/p&gt;
&lt;p&gt;笔者使用Eigen库的时候，就遇到了下面的报错&lt;code&gt;error: static_assert faile</summary>
      
    
    
    
    <category term="c++" scheme="http://example.com/categories/c/"/>
    
    
    <category term="eigen" scheme="http://example.com/tags/eigen/"/>
    
    <category term="c++" scheme="http://example.com/tags/c/"/>
    
  </entry>
  
  <entry>
    <title>被3整除整数的文法表示</title>
    <link href="http://example.com/2020/09/16/2020-09-16-%E8%A2%AB3%E6%95%B4%E9%99%A4%E6%95%B4%E6%95%B0%E7%9A%84%E6%96%87%E6%B3%95%E8%A1%A8%E7%A4%BA/"/>
    <id>http://example.com/2020/09/16/2020-09-16-%E8%A2%AB3%E6%95%B4%E9%99%A4%E6%95%B4%E6%95%B0%E7%9A%84%E6%96%87%E6%B3%95%E8%A1%A8%E7%A4%BA/</id>
    <published>2020-09-16T09:17:00.000Z</published>
    <updated>2020-10-03T12:37:25.216Z</updated>
    
    <content type="html"><![CDATA[<h1>能被3整除整数的文法表示</h1><p>此题为<strong>编译原理</strong>相关题目，考察文法表示相关内容。</p><p>原文地址：<a href="https://anastasiawangyx.github.io/2020/09/16/2020-09-16-%E8%A2%AB3%E6%95%B4%E9%99%A4%E6%95%B4%E6%95%B0%E7%9A%84%E6%96%87%E6%B3%95%E8%A1%A8%E7%A4%BA/">被3整除整数的文法表示</a></p><h2 id="题目">题目</h2><p>写出所有能被3整除的十进制整数的文法表示。（注：为方便表示，规定整数首位可以为0，即018此种表示是合法的，就是整数18。）</p><h2 id="分析">分析</h2><p>由数学知识可以得到，如果整数的每个位上的数字相加之和可以被3整除，则此整数可以被3整除。</p><p>因此，我们用<strong>是否可以被3整除</strong>为标准，将0-9这十个数字分成三组。</p><ul><li>0，3，6，9：可以被3整除</li><li>1，4，7：被3整除余1</li><li>2，5，8：被3整除余2</li></ul><p>文法表达式为：G=（V$_N$，V$_T$，P，S），其中</p><ul><li>V$_N$={S,A,B}，非终结符集合</li><li>V$_T$={0,1,2,3,4,5,6,7,8,9}，终结符集合</li><li>S为开始符</li></ul><p>接下来重点研究的是P中<strong>规则</strong>的集合。</p><p>首先，在数字中加上{0,3,6,9}此类数字时，不影响数字的整除性，即若原本的数字可被3整除，则加上{0,3,6,9}此类数字后，依旧可以被3整除；若原本数字就不能被3整除，则加上{0,3,6,9}此类数字后，依旧不能被3整除。</p><p>其次，将<code>1,4,7</code>中的任意一个数字与<code>2,5,8</code>中的任意一个相加，可以被3整除。</p><ol><li><p>对开始符S</p><ul><li><p>若和<code>0,3,6,9</code>组合，无影响，即<code>S-&gt;（0｜3｜6｜9）S|ε</code>，其中ε为空串，用来结束这个字符串。</p></li><li><p>若和<code>1,4,7</code>组合，我们用一个非终结符A来表示，有<code>S-&gt;（1｜4｜7）A</code></p></li><li><p>若和<code>2,5,8</code>组合，我们用一个非终结符B来表示，则<code>S-&gt;（2｜5｜8）B</code></p></li></ul><p>（注：此处我们可以使用<code>S-&gt;（0｜3｜6｜9）S|ε</code>，也可以使用<code>S-&gt;S（0｜3｜6｜9）|ε</code>，本质上没有区别，为了统一下面我们都将非终结符写到右边。）</p></li><li><p>对非终结符A，其左边是<code>1,4,7</code>三个数字。</p><ul><li><p>若和<code>0,3,6,9</code>组合，无影响，即<code>A-&gt;（0｜3｜6｜9）A</code>，</p></li><li><p>若和<code>2,5,8</code>组合，则正好凑成了可以被3整除的数字，此时的情况与<code>0,3,6,9</code>相同（因为<code>0,3,6,9</code>也可以被3整除），由1中我们可以得到<code>A-&gt;（2｜5｜8）S</code></p></li><li><p>若和<code>1,4,7</code>组合，相加后被三除余2，此时的情况与<code>2,5,8</code>相同（因为<code>2,5,8</code>被3除也余2），于是由1中我们可以得到<code>A-&gt;（1｜4｜7）B</code></p></li></ul></li><li><p>对非终结符B，其左边是<code>2,5,8</code>三个数字，其分析与上述A的分析相似，此处不再赘述。</p></li></ol><h2 id="结果">结果</h2><p>将上面的所有分析结合起来，就可以得到P的规则如下：</p><p><code>S-&gt;（0｜3｜6｜9）S|（1｜4｜7）A｜（2｜5｜8）B｜ε</code></p><p><code>A-&gt;（0｜3｜6｜9）A｜（2｜5｜8）S｜（1｜4｜7）B</code></p><p><code>B-&gt;（0｜3｜6｜9）B｜（1｜4｜7）S｜（2｜5｜8）A</code></p><p>有些类似递归的思想。</p><h2 id="例子">例子</h2><p>任意任意一个可被3整除的整除都可以由上述文法表示，例如17862，文法表示如下</p><p><code>S-&gt; 1A -&gt; 17B -&gt; 178A -&gt; 1786A -&gt; 17862S -&gt; 17862</code></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;能被3整除整数的文法表示&lt;/h1&gt;
&lt;p&gt;此题为&lt;strong&gt;编译原理&lt;/strong&gt;相关题目，考察文法表示相关内容。&lt;/p&gt;
&lt;p&gt;原文地址：&lt;a href=&quot;https://anastasiawangyx.github.io/2020/09/16/2020-09-</summary>
      
    
    
    
    <category term="编译原理" scheme="http://example.com/categories/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"/>
    
    
    <category term="编译原理" scheme="http://example.com/tags/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>神经网络可以计算任何函数的可视化证明</title>
    <link href="http://example.com/2020/09/11/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%AF%E4%BB%A5%E8%AE%A1%E7%AE%97%E4%BB%BB%E4%BD%95%E5%87%BD%E6%95%B0%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AF%81%E6%98%8E%E2%80%94%E2%80%94AI%E5%85%A5%E9%97%A8/"/>
    <id>http://example.com/2020/09/11/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%AF%E4%BB%A5%E8%AE%A1%E7%AE%97%E4%BB%BB%E4%BD%95%E5%87%BD%E6%95%B0%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AF%81%E6%98%8E%E2%80%94%E2%80%94AI%E5%85%A5%E9%97%A8/</id>
    <published>2020-09-11T08:40:00.000Z</published>
    <updated>2021-06-07T01:38:26.236Z</updated>
    
    <content type="html"><![CDATA[<h1>写在前面</h1><ol><li>此文是我入门AI的学习笔记，教材是《nueral networks and deep learning》，作者Michael Nielsen。</li><li>这是一本免费的书籍，网址<a href="http://neuralnetworksanddeeplearning.com/chap4.html">在这里</a>.</li><li>此文是对书中<strong>第四章</strong>内容的整理归纳和分析，关于前几章的知识点，在我之前的博客之中。</li><li>初学者入门，可能会有错误，请大家批评指正。</li></ol><p>本次的内容主要是为了证明一个问题，即<strong>神经网络可以用来计算任何一个函数。</strong><br>这是一种<strong>普遍性的解释</strong></p><blockquote><p>This result tells us that neural networks have a kind of universality. No matter what function we want to compute, we know that there is a neural network which can do the job.</p></blockquote><p>在此次的证明中，我们忽略了函数本身的<strong>数学性质</strong>，而是从<strong>图形</strong>的角度出发，用一种直观、形象、可视化的方式证明了这个普遍性。</p><h1>两个预先声明</h1><p>在进行证明之前，有两点预先声明需要澄清</p><ol><li>神经网络给出的并不是函数的精确值，而是一个近似估计（approximation）。</li><li>我们进行预估的函数必须是<strong>连续函数</strong>，非连续函数是无法使用神经网络进行估计的。（因为原本我们的神经网络对于输入是进行连续型的计算）</li></ol><p>对于第二个问题，一般来说我们也可以用连续的函数去拟合非连续的函数，最终得到的效果也很不错，所以在实践中并不是什么严重的限制。</p><h1>一个输入和一个输出的情况</h1><p><img src="https://img-blog.csdnimg.cn/20200108130355507.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>注：此处使用的神经元是s型神经元。</p><p>在上图所示的具有单个输入和单个输出的神经网络中，我们将隐藏层的底层神经元的权值设为0，而关注顶层神经元。当改变权值w和偏置b时，我们得到的输出图形会发生变化。<a href="http://neuralnetworksanddeeplearning.com/chap4.html#universality_with_one_input_and_one_output">演示网页</a></p><p>最后得出的结论是：改变<strong>权值</strong>会改变图像的<strong>陡峭程度</strong>，改变<strong>偏置</strong>会改变图像的<strong>左右位置</strong></p><p>而我们将<strong>权重w</strong>设置的很大时，图像图像就变成了阶跃函数（step function）。<br><img src="https://imgconvert.csdnimg.cn/aHR0cDovL25ldXJhbG5ldHdvcmtzYW5kZGVlcGxlYXJuaW5nLmNvbS9pbWFnZXMvaGlnaF93ZWlnaHRfZnVuY3Rpb24uanBn?x-oss-process=image/format,png" alt="在这里插入图片描述"><br>由计算（附后）可以得到，阶跃点x的坐标为<kbd>-b/w</kbd><br><img src="https://img-blog.csdnimg.cn/20200108132822772.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>我们用字母s来表示阶跃点的位置，即</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">s</span>=-b/w</span><br><span class="line">下同</span><br></pre></td></tr></table></figure><p>接下来我们加入底层神经元，一起来探究图形的变化。</p><p>根据上面的经验我们可以得到，当我们将底层神经元的权重也设置的很大时，图像将会得到<strong>两个阶跃</strong>，其中s1，s2分别代表了两个阶跃点的坐标。<br><img src="https://img-blog.csdnimg.cn/20200108132954185.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>而上图中，由图形演示我们可以知道，w1和w2分别控制了两个阶跃点的高度。</p><p>当我们将两个阶跃点的高度设置正好<strong>互为相反数</strong>时，就可以得到一个<strong>凸起（bump）</strong>。如下图所演示。<br><img src="https://img-blog.csdnimg.cn/20200108133530556.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>我们用h来表示凸起的高度，即</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">h</span>=w1=-w2</span><br><span class="line">下同</span><br></pre></td></tr></table></figure><p>当我们用编程的角度去看待这个问题，就可以得到一个<kbd>if-then-else</kbd>的陈述。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> input&gt;step point</span><br><span class="line">add <span class="number">1</span> to the weighted output</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">add <span class="number">0</span> to the weighted output</span><br></pre></td></tr></table></figure><p>当隐藏层只有两个神经元时，我们可以得到一个一个凸起。由此可知，当我们在隐藏层加入更多的神经元，就可以得到不同位置，不同高度的凸起。</p><p>两对时：<br><img src="https://img-blog.csdnimg.cn/20200108134346504.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>让我们去尝试拟合一个复杂的一元函数<br><img src="https://img-blog.csdnimg.cn/20200108134652796.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>上图所示的演示使用了五对隐藏神经元 ，对一个复杂函数图像的模拟虽然不是特别的精确，但是它告诉我们了<strong>精确模拟的可能性</strong>，即添加更多的隐藏神经元，改变他们的阶跃点和高度，最终将会得到偏差很小的模拟。</p><h1>两个输入的情况</h1><p>如图，我们将两个输入分别设为x和y，它模拟的是二元函数的情况。<br><img src="https://img-blog.csdnimg.cn/20200108135544997.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>为了方便思考，我们先将y的输入权值设为0，即w2=0，而考虑x轴的情况。</p><p>有了前面单个输入的经验，我们可以知道，当w1很大的时候，函数图像再次出现<strong>阶跃</strong>现象。<br><img src="https://img-blog.csdnimg.cn/20200108140237906.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>我们也可以用同样的方法，在x轴方向得到<strong>凸起</strong>。</p><p><img src="https://img-blog.csdnimg.cn/20200108140504968.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>如上图所示。此时y输入指向隐藏神经元的权重均为0.<br>参数解释：</p><ul><li>x 0.3：第一个阶跃点的位置是x=0.3</li><li>x 0.7：第二个阶跃点的位置是x=0.7</li><li>h：凸起的高度，也是隐藏神经元的输出权重。</li></ul><p>同样的图形也可以在y轴方向演示。</p><p><img src="https://img-blog.csdnimg.cn/202001081409085.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>此时x输入指向隐藏神经元的权重均为0.</p><p>此时我们将x、y方向进行叠加，得到如图所示的函数图像。<br><img src="https://img-blog.csdnimg.cn/20200108141210767.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>此时，四周“高原”的高度为h，中间“塔”的高度为2h。</p><p>为了得到与单个神经元“凸起”相似的结构，我们需要对输出层的偏置进行合理的选择。</p><p>数学计算（附后）与实际图像操作告诉我们，当偏置b的值设定为<kbd>-3/2h</kbd>时，会得到中间凸起的<strong>塔</strong>形函数。<br><img src="https://img-blog.csdnimg.cn/20200108153758301.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>增加隐藏神经元的数量，即可得到不同高度，不同位置的塔，从而对不同的二元函数进行拟合计算。</p><p>如下图所示，两个塔的情况。<br><img src="https://img-blog.csdnimg.cn/20200108154114418.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><em>其中第三层w的值用来控制塔的高度</em></p><p>以及很多塔的情况<br><img src="https://img-blog.csdnimg.cn/20200108154212411.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h1>多个输入的情况</h1><p>根据以上的经验，我们可以得出，对于多个输入的神经网络拟合情况。<br><img src="https://img-blog.csdnimg.cn/2020010815435037.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><ol><li>第一层<strong>权值</strong>设置的很大，同时调整<strong>偏置</strong>，得到类似“阶跃”结构。</li><li>第二层<strong>权值</strong> 交替设置为+h，-h，设置<strong>偏置</strong>为<kbd>(-m+0.5)h</kbd> ,其中m为输入的个数。</li><li>第三层的<strong>权值</strong>设置用来控制高度，，设置<strong>偏置</strong>使其满足<kbd>σ^-1^◦f</kbd> 的函数图像。</li></ol><h1>S型神经元的延伸</h1><p>以上结论都是在s型神经元的基础上得来的。</p><p>如下图是s型神经元的输出函数图。<br><img src="https://img-blog.csdnimg.cn/20200108171148749.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>但是这个结论是可以推广到更多的神经元模型上。</p><p>如下图，激活函数的图形是不规则的形状。</p><p><img src="https://img-blog.csdnimg.cn/20200108171322692.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>当我们按照上面的方法，将<strong>权值增大</strong>，同时调整<strong>偏置</strong>时，依旧可以得到一个很好的阶跃函数，其中阶跃点的值为<kbd>-b/w</kbd>。<a href="http://neuralnetworksanddeeplearning.com/chap4.html#extension_beyond_sigmoid_neurons">演示网页</a><br><img src="https://img-blog.csdnimg.cn/20200108171352370.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>因此我们可以使用这个神经元模型，通过上述所得的技巧来完成对所给函数的拟合。</p><p>但是并非所有的激活函数都可以用来拟合，事实上，函数需要满足条件</p><ul><li>z→−∞和z→∞ 时，函数的值是确定的，也就是我们阶跃函数所取的两个值。</li><li>上述两个值不能相同。</li></ul><h1>修补阶跃函数</h1><p>此处关注的是，我们在用阶跃函数拟合真实函数时，出现的<kbd>故障窗口（window of failure）</kbd><br><img src="https://img-blog.csdnimg.cn/20200108174156890.png" alt="在这里插入图片描述"><br>当然我们可以通过增大权值来使此窗口变得很窄，但是这里作者给出了另一个解决办法。</p><p>假设我们隐藏神经元的加权输出<kbd>σ^-1^◦f</kbd> 图像为<br><img src="https://img-blog.csdnimg.cn/20200108174625547.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>我们采用一个输入，一个输出的方式得到了如下图所示的拟合。<br><img src="https://img-blog.csdnimg.cn/20200108174718676.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>我们将上图的凸起高度减半，得到下图。<br><img src="https://img-blog.csdnimg.cn/20200108174807227.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>第二个操作是，将上述图像向右平移，平移的距离是阶跃长度的一半，得到的图像如下。<br><img src="https://img-blog.csdnimg.cn/20200108174932639.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>将上述两个图像叠加，得到的这个整体的近似仍然在⼀些⼩窗⼝的地⽅有故障。但是问题⽐以前要小很多。原因是在⼀个近似中的故障窗⼝的点，不会在另⼀个的故障窗⼝中。</p><p>我们甚⾄能通过加⼊更多的平移结果，⽤ M 表⽰，重叠的近似 σ^-1^◦f/M 来做得更好。假设故障窗⼝已经⾜够窄了，其中的点只会在⼀个故障窗⼝中。并且假设我们使⽤⼀个 M ⾜够⼤的重叠近似，结果会是⼀个⾮常好的整体近似。</p><h1>结论</h1><ol><li>此章的内容是用可视化的方式，证明了神经网络可以用来拟合计算任意一个函数。</li><li>此章对于普遍性的解释当然不是如何使⽤神经⽹络计算的切实可⾏的⽤法，但是理解此章的内容依旧十分重要。</li><li>正是因为任意函数都是可以用神经网络来拟合计算的，因此真正重要的问题不是这个函数是否存在，而是<strong>找到一个好的计算函数的方法</strong>。</li></ol><h1>证明题</h1><ol><li>阶跃点的位置（以一个输入和一个输出的情况为例）</li></ol><p><img src="https://img-blog.csdnimg.cn/20200108181333707.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><ol start="2"><li>证明中间凸起的<strong>塔</strong>形函数的偏置b的值设定为<kbd>(-m+0.5)h</kbd>，其中m为输入的个数。</li></ol><p><img src="https://img-blog.csdnimg.cn/20200108195421493.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><ol start="3"><li>证明对于两个至多个的情况，上述情况所使用的两层隐藏神经元的情况可以减少为使用单层隐藏神经元。</li></ol><p><img src="https://img-blog.csdnimg.cn/20200108204705469.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;写在前面&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;此文是我入门AI的学习笔记，教材是《nueral networks and deep learning》，作者Michael Nielsen。&lt;/li&gt;
&lt;li&gt;这是一本免费的书籍，网址&lt;a href=&quot;http://neuralne</summary>
      
    
    
    
    <category term="network" scheme="http://example.com/categories/network/"/>
    
    
    <category term="network" scheme="http://example.com/tags/network/"/>
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>神经网络为何很难训练？</title>
    <link href="http://example.com/2020/09/11/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%BA%E4%BD%95%E5%BE%88%E9%9A%BE%E8%AE%AD%E7%BB%83%EF%BC%9F%E2%80%94%E2%80%94AI%E5%85%A5%E9%97%A8/"/>
    <id>http://example.com/2020/09/11/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%BA%E4%BD%95%E5%BE%88%E9%9A%BE%E8%AE%AD%E7%BB%83%EF%BC%9F%E2%80%94%E2%80%94AI%E5%85%A5%E9%97%A8/</id>
    <published>2020-09-11T08:37:00.000Z</published>
    <updated>2021-06-24T03:07:45.603Z</updated>
    
    <content type="html"><![CDATA[<h1>写在前面</h1><ol><li>此文是我学习AI入门的笔记。学习教材《neural networks and deep learning》，作者Michael Nielsen。</li><li>这是一本免费的书籍，网址<a href="http://neuralnetworksanddeeplearning.com/chap5.html">在这里</a>。</li><li>此文是<strong>第五章</strong>内容的学习总结，前几章的内容总结可以见<a href="https://anastasiawangyx.github.io/">我的博客</a>。</li><li>初学者入门，如有错误，请指正。</li></ol><h1>消失的梯度问题（The vanishing gradient probm）</h1><p>当要处理一个比较复杂的问题时，我们往往倾向于将问题分解成一个个简单的问题，逐个解决。</p><p>将这一点运用在神经网络上，我们可以使用<strong>多层的神经网络来处理问题。</strong></p><p>例如，如果我们在进⾏视觉模式识别：</p><p>在第⼀层的神经元可能学会识别<strong>边</strong>。</p><p>在第⼆层的神经元可以在边的基础上学会识别出更加复杂的形状，例如三⻆形或者矩形。</p><p>第三层将能够识别更加复杂的形状。</p><p>依此类推。这些多层的抽象看起来能够赋予深度⽹络⼀种学习解决复杂模式识别问题的能力。</p><p>前面我们所搭建的用来<kbd>识别手写数字</kbd>的网络，是一个三层（输入层-隐藏层-输出层）的浅层神经网络，第一层784个神经元，第二层30个神经元，第三层10个神经元。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> network2</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net=network2.Network([<span class="number">784</span>,<span class="number">30</span>,<span class="number">10</span>])</span><br></pre></td></tr></table></figure><p>我们使用这个神经网络进行手写数字识别时，达到了<strong>96.48</strong>%的正确率。（详情见<a href="https://blog.csdn.net/Anastasiawangyx/article/details/102649158">第一章</a>）</p><p>让我们尝试再加一层具有30个神经元的隐藏层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>net = network2.Network([<span class="number">784</span>, <span class="number">30</span>, <span class="number">30</span>, <span class="number">10</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net.SGD(training_data, <span class="number">30</span>, <span class="number">10</span>, <span class="number">0.1</span>, lmbda=<span class="number">5.0</span>, </span><br><span class="line"><span class="meta">... </span>evaluation_data=validation_data,monitor_evaluation_accuracy=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>最终结果是，准确率升到了<strong>96.90</strong>%。</p><p>再加一层隐藏层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>net = network2.Network([<span class="number">784</span>, <span class="number">30</span>, <span class="number">30</span>, <span class="number">30</span>, <span class="number">10</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net.SGD(training_data, <span class="number">30</span>, <span class="number">10</span>, <span class="number">0.1</span>, lmbda=<span class="number">5.0</span>, </span><br><span class="line"><span class="meta">... </span>evaluation_data=validation_data, monitor_evaluation_accuracy=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>准确率降到了<strong>96.57</strong>%。</p><p>再加一层隐藏层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>net = network2.Network([<span class="number">784</span>, <span class="number">30</span>, <span class="number">30</span>, <span class="number">30</span>, <span class="number">30</span>, <span class="number">10</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net.SGD(training_data, <span class="number">30</span>, <span class="number">10</span>, <span class="number">0.1</span>, lmbda=<span class="number">5.0</span>, </span><br><span class="line"><span class="meta">... </span>evaluation_data=validation_data, monitor_evaluation_accuracy=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>在具有五层隐藏层的神经网络中，准确度降到了<strong>96.53</strong>%.</p><p>虽然不是一个很明显的下降，但是却告诉我们，深度神经网络在解决问题的时候效率似乎并没有比浅层神经网络好到哪里去。</p><p>这显然与我们之前的假设相悖。</p><p>问题其实是出现在，我们的<strong>学习算法并没有发现正确的权值和偏置。</strong></p><p>下面我们将这个问题可视化。</p><p>我们选取具有<strong>两层有隐藏层</strong>的神经网络[784,30,30,10]，下图的条状图形给出的是<strong>隐藏层最顶部六个神经元学习改变速率</strong><kbd> ∂C/∂b</kbd></p><p>即<strong>代价函数关于权重改变的速率</strong>。</p><p><img src="https://img-blog.csdnimg.cn/20200110192201217.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>因为神经网络的权重和偏置是被<strong>随机初始化</strong>的，所以此处的条状长短不一。</p><p>但是还是可以很明显的看出，第二隐藏层的改变速率普遍比第一层要大。</p><p>实际的数学计算也表明了这个事实:</p><p>如果我们令<kbd>δ^l^~j~ = ∂C/∂b^l^~j~</kbd>表示第 l 层的第 j 个神经元的梯度，我们可以将 δ^1^看做是⼀个向量其中元素表⽰第⼀层隐藏层的学习速度，δ^2^ 则是第⼆层隐藏层的学习速度。接着使⽤这些向量的⻓度作为全局衡量这些隐藏层的学习速度的度量。</p><p>因此，||δ^1^|| 就代表第⼀层隐藏层学习速度，⽽ ||δ^2^|| 就代表第⼆层隐藏层学习速度。</p><p>借助这些定义，在和上图同样的配置下，||δ^1^|| = 0.07 ⽽ ||δ^2^|| = 0.31.</p><p>如果我们有三个隐藏层，⽐如说在⼀个 [784, 30, 30, 10] 的⽹络中，那么对应的学习速度就是 <strong>0.012, 0.060, 0.283</strong>。这⾥前⾯的隐藏层学习速度还是要低于最<br>后的隐藏层。假设我们增加另⼀个包含 30 个隐藏神经元的隐藏层。那么，对应的学习速度就是：<strong>0.003, 0.017, 0.070, 0.285</strong>。</p><p>还是⼀样的模式：前⾯的层学习速度低于后⾯的层。</p><p>以上所述均为训练刚开始时的学习速率，那么，这个速率会随着学习的推移发生什么变化呢？</p><p>只有两个隐藏层时的情况：</p><p><img src="https://img-blog.csdnimg.cn/20200110193331822.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>三层隐藏层的情况：</p><p><img src="https://img-blog.csdnimg.cn/20200110193424785.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>四层隐藏层的情况：<br><img src="https://img-blog.csdnimg.cn/2020011019343939.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>由此可以得出的一个结论：</p><p><strong>在某些深度神经网络中，我们在隐藏层反向传播的时候，梯度倾向于变小</strong>。</p><p>这意味着前面隐藏层的神经元的学习速率要低于后面隐藏层的神经元。</p><h1>深度神经网络中梯度的不稳定性</h1><h2 id="1-消失梯度的解释">1. 消失梯度的解释</h2><p>为了弄清楚为何会出现消失的梯度，来看看⼀个极简单的深度神经⽹络：每⼀层都只有⼀个单⼀的神经元。下图就是有三层隐藏层的神经⽹络：<br><img src="https://img-blog.csdnimg.cn/20200110201313490.png" alt="在这里插入图片描述"><br>这⾥，w1, w2, . . . 是权重，⽽ b1, b2, . . . 是偏置，C 则是某个代价函数。</p><p>我们在这个神经网络的基础上，计算<strong>第一个隐藏神经元</strong>的梯度∂C/∂b~1~</p><p>这里直接给出结论，具体证明过程<a href="http://neuralnetworksanddeeplearning.com/chap5.html#what%27s_causing_the_vanishing_gradient_problem_unstable_gradients_in_deep_neural_nets">演示。</a><br><img src="https://img-blog.csdnimg.cn/2020011020162496.png" alt="在这里插入图片描述"><br>除了最后⼀项，该表达式是⼀系列形如 w~j~σ′(z~j~ ) 的乘积。为了理解每个项的⾏为，先看看下⾯的 sigmoid 函数导数的图像：</p><p><img src="https://img-blog.csdnimg.cn/20200111111548748.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>该导数在 σ′(0) = 1/4 时达到最⾼，所以σ′(0)&lt;= 1/4。</p><p>我们使⽤标准⽅法来初始化⽹络中的<strong>权重</strong>，那么会使⽤⼀个<strong>均值为 0 标准差为 1 的⾼斯分布</strong>，因此此所有的权重通常会满⾜ |w~j~ | &lt; 1.</p><p>因此，我们发现会有 w~j~σ′(z~j~ ) &lt; 1/4。并且在我们进⾏了所有这些项的乘积时，最终结果肯定会指数级下降：项越多，乘积的下降的越快。</p><p><img src="https://img-blog.csdnimg.cn/20200111112035540.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>这是一个对于梯度消失问题的合理解释。</p><h2 id="2-梯度激增问题（The-exploding-gradient-problem）">2. 梯度激增问题（The exploding gradient problem）</h2><p>当我们的权重在学习的过程中不断增大时，通过前面的计算我们可以知道，前面隐藏层的学习速率会在反向传播的时候获得<strong>指数级的增长</strong>，这也称为指数激增问题。</p><h2 id="3-不稳定的梯度问题">3. 不稳定的梯度问题</h2><p>根本的问题其实并⾮是消失的梯度问题或者激增的梯度问题，⽽是<strong>在前⾯的层上的梯度是来⾃后⾯的层上项的乘积</strong>。当存在过多的层次时，就出现了内在本质上的不稳定情况。</p><p>特别的，对于每一层包含更多神经元的复杂的神经网络，此问题依旧存在。实践中，⼀般会发现在sigmoid ⽹络中前⾯的层的梯度<strong>指数级地消失</strong>。所以在这些层上的学习速度就会变得很慢了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;写在前面&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;此文是我学习AI入门的笔记。学习教材《neural networks and deep learning》，作者Michael Nielsen。&lt;/li&gt;
&lt;li&gt;这是一本免费的书籍，网址&lt;a href=&quot;http://neuraln</summary>
      
    
    
    
    <category term="network" scheme="http://example.com/categories/network/"/>
    
    
    <category term="network" scheme="http://example.com/tags/network/"/>
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络(CNN)识别手写数字MNIST(附代码)</title>
    <link href="http://example.com/2020/09/11/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN+%E8%AF%86%E5%88%AB%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97MNIST%EF%BC%88%E9%99%84%E4%BB%A3%E7%A0%81%EF%BC%89%E2%80%94%E2%80%94AI%E5%85%A5%E9%97%A8/"/>
    <id>http://example.com/2020/09/11/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN+%E8%AF%86%E5%88%AB%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97MNIST%EF%BC%88%E9%99%84%E4%BB%A3%E7%A0%81%EF%BC%89%E2%80%94%E2%80%94AI%E5%85%A5%E9%97%A8/</id>
    <published>2020-09-11T08:36:00.000Z</published>
    <updated>2021-06-07T01:37:18.996Z</updated>
    
    <content type="html"><![CDATA[<h1>写在前面</h1><ol><li>此文是我学习AI入门的笔记。学习教材《neural networks and deep learning》，作者Michael Nielsen。</li><li>这是一本免费的书籍，网址<a href="http://neuralnetworksanddeeplearning.com/chap6.html">在这里</a>。</li><li>此文是<strong>第六章</strong>内容的学习总结，前几章的内容总结可以见<a href="https://anastasiawangyx.github.io/">我的博客</a>。</li><li>初学者入门，如有错误，请指正。</li></ol><blockquote></blockquote><h1>简介卷积神经网络</h1><p>在之前的学习中，我们都是使用<strong>全连接</strong>的神经网络来处理问题的。即，⽹络中的神经元与相邻的层上的每个神经元均连接，在此之前我们已经获得了98%的分辨率了。<br><img src="https://img-blog.csdnimg.cn/20200123131431465.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>但是在识别手写数字的问题上，这样连接方式有很大的缺点：</p><p>它忽略了图像本身的<strong>空间结构（spatial structure）</strong>，而以同样的方式对待所有的输入像素。</p><p>今天介绍的卷积神经网络则可以很好的利用图像的空间结构，是处理图像分类问题的一个很好的框架。</p><p>学习CNN框架，就要了解它的三个基本概念：</p><ul><li>Local receptive fields 局部感受野</li><li>shared weights 共享权重</li><li>pooling  池化</li></ul><h2 id="1-局部感受野">1. 局部感受野</h2><p>在识别手写数字的问题中，输入层是28 * 28的图像的像素灰度，因此输入层共有28 * 28=784个神经元。</p><p>在CNN中，我们将这784个神经元看做方形排列。</p><p><img src="https://img-blog.csdnimg.cn/20200123132827447.png" alt="在这里插入图片描述"></p><p>当我们把输入层神经元连接到隐藏神经元层时，我们不会全部映射，而是进行小的、局部的连接。</p><p>例如，我们将输入层一个5 * 5的方形区域连接到隐藏层的一个神经元。</p><p><img src="https://img-blog.csdnimg.cn/20200123133247141.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>这个输⼊图像的区域被称为隐藏神经元的<strong>局部感受野</strong>。它是输入像素上的一个小窗口。每个连接学习⼀个权重。而隐藏神经元同时也学习⼀个总的偏置。你可以把这个特定的隐藏神经元看作是在学习分析它的局部感受野。</p><p>然后我们在输入层上移动局部感受野，而每一个不同的局部感受野对应了隐藏层上的一个神经元。</p><p>例如，输入层左上角的25个神经元对应第一个隐藏神经元。</p><p><img src="https://img-blog.csdnimg.cn/20200123133628651.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>然后我们向右移动一个像素（神经元），对应隐藏层第二个神经元。</p><p><img src="https://img-blog.csdnimg.cn/20200123133736448.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>如此不断移动，我们会得到一个24 * 24个神经元的隐藏层。</p><p>当然我们也可以使用不同的<strong>跨距</strong>（stride length），比如一次移动两个像素。在此处我们取跨距为1。</p><h2 id="2-共享权重和偏置">2. 共享权重和偏置</h2><p>对于每一个隐藏层神经元，它具有一个偏置，并连接了一个5 * 5的局部感受野，所以也有5 * 5个权重。</p><p>对于同一层的每一个隐藏神经元，我们使用<strong>相同的偏置和5* 5权重</strong>，换句话说，对于隐藏层的每一个神经元，其输出公式为<br><img src="https://img-blog.csdnimg.cn/20200123143230150.JPG" alt="在这里插入图片描述"></p><ul><li>σ是神经元的激活函数，可以是我们之前学习过的S型函数。</li><li>b是共享偏置</li><li>w~l,m~是一个共享的5 * 5 的权值数组</li><li>a~x,y~表示位于x，y位置的输入激活值</li></ul><p>对同一个隐藏层使用相同的权重和偏置，就可以让此层的神经元在图像的不同位置学习一个特定的特征。</p><p>也因此，我们将从输入层到隐藏层的映射称为<strong>特征映射（feature map）</strong><br>将定义特征映射的权重和偏置称为<strong>共享权重（shared weights）和共享偏置（shared bias）</strong>，也称作<strong>卷积核（kernel）或滤波器（filter）</strong>，这是一些文献中可能会使用的术语。</p><p>⽬前我描述的⽹络结构只能检测⼀种局部特征的类型。为了完成图像识别我们需要超过⼀个的特征映射。所以⼀个完整的卷积层由几个不同的特征映射组成：<br><img src="https://img-blog.csdnimg.cn/20200123172115130.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>在上图所示的例子中，我们使用了三个特征映射，每个特征映射由一个5 * 5的权值数组和一个偏置，可以探测图像的一个特征。</p><p>而在下面我们的实际开发案例中，我们使用了20个特征映射（卷积核、滤波器），学习了下面的20种图形特征。</p><p>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200123173014806.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70">https://img-blog.csdnimg.cn/20200123173014806.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70</a>  =490x430)</p><p>每个映射有⼀幅 5 × 5 块的图像表示，对应于局部感受野中的 5 × 5 权重。白色块意味着⼀个小（更小的负数）权重，所以这样的特征映射对相应的输⼊像素有更小的响应。更暗的块意味着⼀个更⼤的权重，所以这样的特征映射对相应的输⼊像素有更大的响应。</p><p>共享权重和偏置的⼀个很⼤的优点是，它⼤⼤<strong>减少了参与的卷积网络的参数</strong>。对于每个特征映射我们需要 25 = 5 × 5 个共享权重，加上⼀个共享偏置。所以每个特征映射需要 26 个参数。如果我们有 20 个特征映射，那么总共有 20 × 26 = 520 个参数来定义卷积层。</p><p>作为对比，假设我们有⼀个全连接的第⼀层，具有 784 = 28 × 28 个输⼊神经元，和⼀个相对适中的 30 个隐藏神经元，正如我们在本书之前的很多例⼦中使⽤的。总共有 784 × 30 个权重，加上额外的 30 个偏置，共有 23, 550 个参数。换句话说，这个全连接的层有多达 40 倍于卷基层的参数。</p><h2 id="3-池化">3. 池化</h2><p>卷积神经网络除了卷积层之外，还有池化层。池化层通常在卷积层之后，主要作用是简化从卷积层输出的信息。</p><h3 id="最大值池化（max-pooling）">最大值池化（max-pooling）</h3><p>最大值混合是一个常见的池化方式。一个池化单元将卷积层一个区域（例如2<br>*2）中的<strong>最大激活值</strong>作为其输出：<br><img src="https://img-blog.csdnimg.cn/20200123192648729.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>注意既然从卷积层有 24 × 24 个神经元输出，混合后我们得到 12 × 12 个神经元。</p><p>正如上⾯提到的，卷积层通常包含超过⼀个特征映射。我们将最⼤值混合分别应⽤于每⼀个特征映射。所以如果有三个特征映射，组合在⼀起的卷积层和最⼤值混合层看起来像这样：</p><p><img src="https://img-blog.csdnimg.cn/2020012319313268.png" alt="在这里插入图片描述"><br>通过池化的方式，网络可以得知，对于给定的一种图形特征，它大致分布在图像的哪些位置，而不需要处理大量具体的位置信息，这有助于减少在后面的层中所需的参数的数目。</p><h3 id="L2池化（L2-pooling）">L2池化（L2-pooling）</h3><p>L2-池化则是取 2×2 区域中<strong>激活值的平方和的平方根</strong>。</p><p>虽然细节不同，但是两种池化方式都被广泛应用到实际中。</p><h2 id="综合">综合</h2><p>我们现在可以把这些思想都放在⼀起来构建⼀个完整的卷积神经⽹络。它和我们刚看到的架构相似，但是有额外的⼀层 10 个输出神经元，对应于 10 个可能的 MNIST 数字（’0’，’1’，’2’ 等）：</p><p><img src="https://img-blog.csdnimg.cn/20200123194551914.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>这个⽹络从 28 × 28 个输⼊神经元开始，这些神经元⽤于对 MNIST 图像的像素强度进⾏编码。</p><p>接着的是⼀个卷积层，使⽤⼀个 5×5 局部感受野和 3 个特征映射。其结果是⼀个 3×24×24隐藏特征神经元层。下⼀步是⼀个最大值池化层，应⽤于 2 × 2 区域，遍及 3 个特征映射。结果是⼀个 3 × 12 × 12 隐藏特征神经元层。</p><p>网络中最后连接的层是⼀个<strong>全连接层</strong>。更确切地说，这⼀层将最⼤值混合层的每⼀个神经元连接到每⼀个输出神经元。这个全连接结构和我们之前章节中使用的相同。</p><blockquote></blockquote><h1>代码</h1><p>现在将卷积神经网络应用于手写数字MNIST问题。</p><h2 id="环境配置">环境配置</h2><p>此处提供代码的<a href="https://github.com/mnielsen/neural-networks-and-deep-learning">Github下载地址</a>，我们此次运行的代码是<code><a href="http://network3.py">network3.py</a></code>,是之前运行的<code><a href="http://network.py">network.py</a></code>和<code><a href="http://network2.py">network2.py</a></code>的加强版。</p><p>跟前面的章节相似，我们会使用Anaconda来运行作者的python2 的代码，同时需要<code>numpy</code>库和<code>Theano</code>库。</p><p>Anaconda和numpy库的下载及使用可以参考我之前的一篇博客：<a href="https://blog.csdn.net/Anastasiawangyx/article/details/102649158">使用神经网络实现手写数字识别（MNIST）</a></p><p>我们还需要下载Theano库来运行代码。网上关于Theano的下载和安装教程不多，但是我基本都没怎么看懂（咳咳）。自己倒腾了一下不知道为啥就可以运行了（那就这样吧），此处放上<a href="http://deeplearning.net/software/theano/">Theano的官方文档</a>供大家参考。</p><p>首先在Anaconda中进入自己的python2环境，我的环境名叫<code>deeplearning</code>.</p><p>输入<code>conda install theano</code>,下载theano包。<br><img src="https://img-blog.csdnimg.cn/20200123203927151.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>我因为已经安装过了所以显示是#All requested packages already installed</p><p>在我的电脑上安装时，会首先提示是否安装，输入<code>y</code>继续安装，接着等待5min左右就可以安装完成了。</p><p>完成后我们输入<code>conda list</code>查看<br><img src="https://img-blog.csdnimg.cn/20200123204329287.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>可以看到<code>theano</code>和<code>numpy</code>库，则安装成功。</p><h2 id="报错处理">报错处理</h2><p>将环境目录指向电脑上存储<code>network3.py</code>的目录，</p><p>输入<code>python</code>进入python解释器。</p><p>继续输入</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;<span class="keyword">import</span> network3</span><br></pre></td></tr></table></figure><p>在此处应该会<strong>报错</strong>。</p><p><code>Import Error:cannot import name downsample</code><br><img src="https://img-blog.csdnimg.cn/20200123215036286.JPG" alt="在这里插入图片描述"></p><p>因为教材比较早，代码里使用的<code>downsample</code>在现在的python中已经不再使用了，因此我们需要对代码本身做一点改动。</p><p>随便用一个编辑器打开<code>network3.py</code>的代码，修改其中一行，将<code>downsample</code>改为<code>pool</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#from theano.tensor.signal import downsample</span></span><br><span class="line"><span class="keyword">from</span> theano.tensor.signal <span class="keyword">import</span> pool</span><br></pre></td></tr></table></figure><p>代码中运用到downsample的部分也要改掉<br>在代码中找到下面的一段</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pooled_out = downsample.max_pool_2d(</span><br><span class="line">       input=conv_out,</span><br><span class="line">       ds=poolsize,</span><br><span class="line">       ignore_border=<span class="literal">True</span></span><br><span class="line"> )</span><br></pre></td></tr></table></figure><p>改成</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> pooled_out = pool.pool_2d(</span><br><span class="line">     input=conv_out,</span><br><span class="line">     ws=poolsize,</span><br><span class="line">     ignore_border=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>参考博客：<a href="https://blog.csdn.net/moxiaobeiMM/article/details/75015408">python无法加载downsample模型问题</a></p><p>之后再导入network3就可以顺利进行了。</p><h2 id="代码运行">代码运行</h2> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"> &gt;&gt;&gt; <span class="keyword">import</span> network3</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> network3 <span class="keyword">import</span> Network</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> network3 <span class="keyword">import</span> ConvPoolLayer, FullyConnectedLayer, SoftmaxLayer</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>training_data, validation_data, test_data = network3.load_data_shared()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mini_batch_size = <span class="number">10</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net = Network([</span><br><span class="line">FullyConnectedLayer(n_in=<span class="number">784</span>, n_out=<span class="number">100</span>),</span><br><span class="line">SoftmaxLayer(n_in=<span class="number">100</span>, n_out=<span class="number">10</span>)], mini_batch_size)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net.SGD(training_data, <span class="number">60</span>, mini_batch_size, <span class="number">0.1</span>,</span><br><span class="line">validation_data, test_data)</span><br></pre></td></tr></table></figure><p>此处我们使用的是只有一个包含100个神经元的隐藏层</p><p>同时使用<strong>对数似然代价函数</strong>和<strong>柔性最大值层</strong>，因为这两个方法在现在的图像分类网络中很常见。关于上面两个方法的描述，请见我之前的博客：<a href="https://blog.csdn.net/Anastasiawangyx/article/details/102866483">改善神经网络学习的方法</a></p><p>最终可以达到97.83%的准确率，准确率在不同电脑上会有细微差别，大致在97.8%左右。<br><img src="https://img-blog.csdnimg.cn/20200123221928240.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="一层卷积网络">一层卷积网络</h3><p>使用<strong>卷积神经网络</strong>来解决这个问题。</p><p>让我们从在网络开始位置的右边插⼊⼀个卷积层开始。我们将使用 5 × 5 局部感受野，跨距为 1，同时使用20 个特征映射。</p><p>我们也会插⼊⼀个最⼤值混合层，它⽤⼀个 2 × 2 的混合窗⼝来合并特征。所以总体的⽹络架构看起来很像上⼀节讨论的架构，但是有⼀个额外的全连接层：</p><p><img src="https://img-blog.csdnimg.cn/20200123223208956.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>在这个架构中，我们可以把卷积和混合层看作是在学习输⼊训练图像中的局部感受野，而后面的全连接层则在⼀个更抽象的层次学习，从整个图像整合全局信息。这是⼀种常见的卷积神经网络模式。</p><p>在python解释器中输入下列代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>net = Network([</span><br><span class="line">ConvPoolLayer(image_shape=(mini_batch_size, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>),</span><br><span class="line">filter_shape=(<span class="number">20</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>),</span><br><span class="line">poolsize=(<span class="number">2</span>, <span class="number">2</span>)),</span><br><span class="line">FullyConnectedLayer(n_in=<span class="number">20</span>*<span class="number">12</span>*<span class="number">12</span>, n_out=<span class="number">100</span>),</span><br><span class="line">SoftmaxLayer(n_in=<span class="number">100</span>, n_out=<span class="number">10</span>)], mini_batch_size)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net.SGD(training_data, <span class="number">60</span>, mini_batch_size, <span class="number">0.1</span>,</span><br><span class="line">validation_data, test_data)</span><br></pre></td></tr></table></figure><p>最终达到的准确率在98.78%左右，相比之前已经得到了很大的改善。</p><h3 id="两层卷积网络">两层卷积网络</h3><p>我们试着插⼊<strong>第二个卷积–混合层</strong>。把它插在已有的卷积–混合层和全连接隐藏层之间。我们再次使用⼀个 5 × 5 局部感受野，混合 2 × 2 的区域。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>net = Network([</span><br><span class="line">        ConvPoolLayer(image_shape=(mini_batch_size, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), </span><br><span class="line">                      filter_shape=(<span class="number">20</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>), </span><br><span class="line">                      poolsize=(<span class="number">2</span>, <span class="number">2</span>)),</span><br><span class="line">        ConvPoolLayer(image_shape=(mini_batch_size, <span class="number">20</span>, <span class="number">12</span>, <span class="number">12</span>), </span><br><span class="line">                      filter_shape=(<span class="number">40</span>, <span class="number">20</span>, <span class="number">5</span>, <span class="number">5</span>), </span><br><span class="line">                      poolsize=(<span class="number">2</span>, <span class="number">2</span>)),</span><br><span class="line">        FullyConnectedLayer(n_in=<span class="number">40</span>*<span class="number">4</span>*<span class="number">4</span>, n_out=<span class="number">100</span>),</span><br><span class="line">        SoftmaxLayer(n_in=<span class="number">100</span>, n_out=<span class="number">10</span>)], mini_batch_size)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net.SGD(training_data, <span class="number">60</span>, mini_batch_size, <span class="number">0.1</span>, </span><br><span class="line">            validation_data, test_data) </span><br></pre></td></tr></table></figure><p>这一次我们的准确率可以达到99.06%。</p><h3 id="修正线性单元">修正线性单元</h3><p>但是我们仍然可以通过改进我们的神经网络获得更好的分辨率。</p><p>这次我们使用修正线性单元，而不是S型激活函数。其表达式形式为:</p><p><code>f(z) ≡ max(0, z)</code>。</p><p>关于修正线性单元的信息可以参考我之前的博客：<a href="https://blog.csdn.net/Anastasiawangyx/article/details/102866483">改善神经网络学习的方法。</a></p><p>同时我们加入L2规范化，规范化参数λ = 0.1。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> network3 <span class="keyword">import</span> ReLU</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net = Network([</span><br><span class="line">ConvPoolLayer(image_shape=(mini_batch_size, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>),</span><br><span class="line">filter_shape=(<span class="number">20</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>),</span><br><span class="line">poolsize=(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">activation_fn=ReLU),</span><br><span class="line">ConvPoolLayer(image_shape=(mini_batch_size, <span class="number">20</span>, <span class="number">12</span>, <span class="number">12</span>),</span><br><span class="line">filter_shape=(<span class="number">40</span>, <span class="number">20</span>, <span class="number">5</span>, <span class="number">5</span>),</span><br><span class="line">poolsize=(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">activation_fn=ReLU),</span><br><span class="line">FullyConnectedLayer(n_in=<span class="number">40</span>*<span class="number">4</span>*<span class="number">4</span>, n_out=<span class="number">100</span>, activation_fn=ReLU),</span><br><span class="line">SoftmaxLayer(n_in=<span class="number">100</span>, n_out=<span class="number">10</span>)], mini_batch_size)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net.SGD(training_data, <span class="number">60</span>, mini_batch_size, <span class="number">0.03</span>,</span><br><span class="line">validation_data, test_data, lmbda=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><p>此时得到的分辨率应该为99.23%左右，相比之前又有了很大的提高。</p><h3 id="拓展训练数据">拓展训练数据</h3><p>在上面的基础上，我们以算法的形式拓展训练数据。</p><p>具体做法是将图像平移若干个像素，即可以得到一副新的图像。</p><p>我们在shell提示符中运行程序<code>expend_mnist.py</code>来实现数据的拓展。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ python expend_mnist.py</span><br></pre></td></tr></table></figure><p>运⾏这个程序取得 50, 000 幅 MNIST 训练图像并扩展为具有 250, 000 幅训练图像的训练集。然后我们可以使⽤这些训练图像来训练我们的网络。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>expanded_training_data, _, _ = network3.load_data_shared(</span><br><span class="line"><span class="string">&quot;../data/mnist_expanded.pkl.gz&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net = Network([</span><br><span class="line">ConvPoolLayer(image_shape=(mini_batch_size, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>),</span><br><span class="line">filter_shape=(<span class="number">20</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>),</span><br><span class="line">poolsize=(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">activation_fn=ReLU),</span><br><span class="line">ConvPoolLayer(image_shape=(mini_batch_size, <span class="number">20</span>, <span class="number">12</span>, <span class="number">12</span>),</span><br><span class="line">filter_shape=(<span class="number">40</span>, <span class="number">20</span>, <span class="number">5</span>, <span class="number">5</span>),</span><br><span class="line">poolsize=(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">activation_fn=ReLU),</span><br><span class="line">FullyConnectedLayer(n_in=<span class="number">40</span>*<span class="number">4</span>*<span class="number">4</span>, n_out=<span class="number">100</span>, activation_fn=ReLU),</span><br><span class="line">SoftmaxLayer(n_in=<span class="number">100</span>, n_out=<span class="number">10</span>)], mini_batch_size)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net.SGD(expanded_training_data, <span class="number">60</span>, mini_batch_size, <span class="number">0.03</span>,</span><br><span class="line">validation_data, test_data, lmbda=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><p>最终可以得到99.37%左右的分辨准确率。</p><h3 id="使用弃权的全连接层">使用弃权的全连接层</h3><p>在以上方法的基础上，我们再加一层全连接层，这样我们就有两个含有100个神经元的全连接层。</p><p>同时，将之前接触过的弃权技术运用到最终的全连接层上。</p><p>我们知道，弃权技术的基本思想是，在训练网络时随机的移除单独的激活值，降低网络的依赖性，有助于减轻过度拟合现象。参考博客：<a href="https://blog.csdn.net/Anastasiawangyx/article/details/102866483">改善神经网络学习的方法。</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>net = Network([</span><br><span class="line">        ConvPoolLayer(image_shape=(mini_batch_size, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), </span><br><span class="line">                      filter_shape=(<span class="number">20</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>), </span><br><span class="line">                      poolsize=(<span class="number">2</span>, <span class="number">2</span>), </span><br><span class="line">                      activation_fn=ReLU),</span><br><span class="line">        ConvPoolLayer(image_shape=(mini_batch_size, <span class="number">20</span>, <span class="number">12</span>, <span class="number">12</span>), </span><br><span class="line">                      filter_shape=(<span class="number">40</span>, <span class="number">20</span>, <span class="number">5</span>, <span class="number">5</span>), </span><br><span class="line">                      poolsize=(<span class="number">2</span>, <span class="number">2</span>), </span><br><span class="line">                      activation_fn=ReLU),</span><br><span class="line">        FullyConnectedLayer(</span><br><span class="line">            n_in=<span class="number">40</span>*<span class="number">4</span>*<span class="number">4</span>, n_out=<span class="number">1000</span>, activation_fn=ReLU, p_dropout=<span class="number">0.5</span>),</span><br><span class="line">        FullyConnectedLayer(</span><br><span class="line">            n_in=<span class="number">1000</span>, n_out=<span class="number">1000</span>, activation_fn=ReLU, p_dropout=<span class="number">0.5</span>),</span><br><span class="line">        SoftmaxLayer(n_in=<span class="number">1000</span>, n_out=<span class="number">10</span>, p_dropout=<span class="number">0.5</span>)], </span><br><span class="line">        mini_batch_size)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net.SGD(expanded_training_data, <span class="number">40</span>, mini_batch_size, <span class="number">0.03</span>, </span><br><span class="line">            validation_data, test_data)</span><br></pre></td></tr></table></figure><p>使用弃权的全连接层，我们的神经网络的分辨率提高到了99.60%。</p><h3 id="使用组合网络">使用组合网络</h3><p>借助上面的方法，我们可以将准确率提高到99.60%左右。</p><p>⼀个简单的进⼀步提⾼性能的⽅法是创建几个神经网络，然后让它们投票来决定最好的分类。例如，假设我们使⽤上述的方式训练了 5 个不同的神经⽹络，每个达到了接近于 99.60% 的准确率。尽管网络都会有相似的准确率，他们很可能因为不同的随机初始化产生不同的错误。在这 5 个网络中进行⼀次投票来取得⼀个优于单个网络的分类，能进一步提高准确率。</p><h2 id="源代码">源代码</h2><p>以下是修改过的源代码，运行环境：python2.7，需要的库：<code>numpy</code>、<code>theano</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;network3.py</span></span><br><span class="line"><span class="string">~~~~~~~~~~~~~~</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">A Theano-based program for training and running simple neural</span></span><br><span class="line"><span class="string">networks.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Supports several layer types (fully connected, convolutional, max</span></span><br><span class="line"><span class="string">pooling, softmax), and activation functions (sigmoid, tanh, and</span></span><br><span class="line"><span class="string">rectified linear units, with more easily added).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">When run on a CPU, this program is much faster than network.py and</span></span><br><span class="line"><span class="string">network2.py.  However, unlike network.py and network2.py it can also</span></span><br><span class="line"><span class="string">be run on a GPU, which makes it faster still.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Because the code is based on Theano, the code is different in many</span></span><br><span class="line"><span class="string">ways from network.py and network2.py.  However, where possible I have</span></span><br><span class="line"><span class="string">tried to maintain consistency with the earlier programs.  In</span></span><br><span class="line"><span class="string">particular, the API is similar to network2.py.  Note that I have</span></span><br><span class="line"><span class="string">focused on making the code simple, easily readable, and easily</span></span><br><span class="line"><span class="string">modifiable.  It is not optimized, and omits many desirable features.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">This program incorporates ideas from the Theano documentation on</span></span><br><span class="line"><span class="string">convolutional neural nets (notably,</span></span><br><span class="line"><span class="string">http://deeplearning.net/tutorial/lenet.html ), from Misha Denil&#x27;s</span></span><br><span class="line"><span class="string">implementation of dropout (https://github.com/mdenil/dropout ), and</span></span><br><span class="line"><span class="string">from Chris Olah (http://colah.github.io ).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Written for Theano 0.6 and 0.7, needs some changes for more recent</span></span><br><span class="line"><span class="string">versions of Theano.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Libraries</span></span><br><span class="line"><span class="comment"># Standard library</span></span><br><span class="line"><span class="keyword">import</span> cPickle</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"></span><br><span class="line"><span class="comment"># Third-party libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> theano</span><br><span class="line"><span class="keyword">import</span> theano.tensor <span class="keyword">as</span> T</span><br><span class="line"><span class="keyword">from</span> theano.tensor.nnet <span class="keyword">import</span> conv</span><br><span class="line"><span class="keyword">from</span> theano.tensor.nnet <span class="keyword">import</span> softmax</span><br><span class="line"><span class="keyword">from</span> theano.tensor <span class="keyword">import</span> shared_randomstreams</span><br><span class="line"><span class="comment">#from theano.tensor.signal import downsample</span></span><br><span class="line"><span class="keyword">from</span> theano.tensor.signal <span class="keyword">import</span> pool</span><br><span class="line"></span><br><span class="line"><span class="comment"># Activation functions for neurons</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear</span>(<span class="params">z</span>):</span> <span class="keyword">return</span> z</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ReLU</span>(<span class="params">z</span>):</span> <span class="keyword">return</span> T.maximum(<span class="number">0.0</span>, z)</span><br><span class="line"><span class="keyword">from</span> theano.tensor.nnet <span class="keyword">import</span> sigmoid</span><br><span class="line"><span class="keyword">from</span> theano.tensor <span class="keyword">import</span> tanh</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Constants</span></span><br><span class="line">GPU = <span class="literal">True</span></span><br><span class="line"><span class="keyword">if</span> GPU:</span><br><span class="line">    <span class="keyword">print</span> <span class="string">&quot;Trying to run under a GPU.  If this is not desired, then modify &quot;</span>+\</span><br><span class="line">        <span class="string">&quot;network3.py\nto set the GPU flag to False.&quot;</span></span><br><span class="line">    <span class="keyword">try</span>: theano.config.device = <span class="string">&#x27;gpu&#x27;</span></span><br><span class="line">    <span class="keyword">except</span>: <span class="keyword">pass</span> <span class="comment"># it&#x27;s already set</span></span><br><span class="line">    theano.config.floatX = <span class="string">&#x27;float32&#x27;</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">print</span> <span class="string">&quot;Running with a CPU.  If this is not desired, then the modify &quot;</span>+\</span><br><span class="line">        <span class="string">&quot;network3.py to set\nthe GPU flag to True.&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Load the MNIST data</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_shared</span>(<span class="params">filename=<span class="string">&quot;../data/mnist.pkl.gz&quot;</span></span>):</span></span><br><span class="line">    f = gzip.open(filename, <span class="string">&#x27;rb&#x27;</span>)</span><br><span class="line">    training_data, validation_data, test_data = cPickle.load(f)</span><br><span class="line">    f.close()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">shared</span>(<span class="params">data</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Place the data into shared variables.  This allows Theano to copy</span></span><br><span class="line"><span class="string">        the data to the GPU, if one is available.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        shared_x = theano.shared(</span><br><span class="line">            np.asarray(data[<span class="number">0</span>], dtype=theano.config.floatX), borrow=<span class="literal">True</span>)</span><br><span class="line">        shared_y = theano.shared(</span><br><span class="line">            np.asarray(data[<span class="number">1</span>], dtype=theano.config.floatX), borrow=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> shared_x, T.cast(shared_y, <span class="string">&quot;int32&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> [shared(training_data), shared(validation_data), shared(test_data)]</span><br><span class="line"></span><br><span class="line"><span class="comment">#### Main class used to construct and train networks</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span>(<span class="params">object</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, layers, mini_batch_size</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Takes a list of `layers`, describing the network architecture, and</span></span><br><span class="line"><span class="string">        a value for the `mini_batch_size` to be used during training</span></span><br><span class="line"><span class="string">        by stochastic gradient descent.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.layers = layers</span><br><span class="line">        self.mini_batch_size = mini_batch_size</span><br><span class="line">        self.params = [param <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers <span class="keyword">for</span> param <span class="keyword">in</span> layer.params]</span><br><span class="line">        self.x = T.matrix(<span class="string">&quot;x&quot;</span>)</span><br><span class="line">        self.y = T.ivector(<span class="string">&quot;y&quot;</span>)</span><br><span class="line">        init_layer = self.layers[<span class="number">0</span>]</span><br><span class="line">        init_layer.set_inpt(self.x, self.x, self.mini_batch_size)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(<span class="number">1</span>, len(self.layers)):</span><br><span class="line">            prev_layer, layer  = self.layers[j<span class="number">-1</span>], self.layers[j]</span><br><span class="line">            layer.set_inpt(</span><br><span class="line">                prev_layer.output, prev_layer.output_dropout, self.mini_batch_size)</span><br><span class="line">        self.output = self.layers[<span class="number">-1</span>].output</span><br><span class="line">        self.output_dropout = self.layers[<span class="number">-1</span>].output_dropout</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">SGD</span>(<span class="params">self, training_data, epochs, mini_batch_size, eta,</span></span></span><br><span class="line"><span class="function"><span class="params">            validation_data, test_data, lmbda=<span class="number">0.0</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Train the network using mini-batch stochastic gradient descent.&quot;&quot;&quot;</span></span><br><span class="line">        training_x, training_y = training_data</span><br><span class="line">        validation_x, validation_y = validation_data</span><br><span class="line">        test_x, test_y = test_data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># compute number of minibatches for training, validation and testing</span></span><br><span class="line">        num_training_batches = size(training_data)/mini_batch_size</span><br><span class="line">        num_validation_batches = size(validation_data)/mini_batch_size</span><br><span class="line">        num_test_batches = size(test_data)/mini_batch_size</span><br><span class="line"></span><br><span class="line">        <span class="comment"># define the (regularized) cost function, symbolic gradients, and updates</span></span><br><span class="line">        l2_norm_squared = sum([(layer.w**<span class="number">2</span>).sum() <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers])</span><br><span class="line">        cost = self.layers[<span class="number">-1</span>].cost(self)+\</span><br><span class="line">               <span class="number">0.5</span>*lmbda*l2_norm_squared/num_training_batches</span><br><span class="line">        grads = T.grad(cost, self.params)</span><br><span class="line">        updates = [(param, param-eta*grad)</span><br><span class="line">                   <span class="keyword">for</span> param, grad <span class="keyword">in</span> zip(self.params, grads)]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># define functions to train a mini-batch, and to compute the</span></span><br><span class="line">        <span class="comment"># accuracy in validation and test mini-batches.</span></span><br><span class="line">        i = T.lscalar() <span class="comment"># mini-batch index</span></span><br><span class="line">        train_mb = theano.function(</span><br><span class="line">            [i], cost, updates=updates,</span><br><span class="line">            givens=&#123;</span><br><span class="line">                self.x:</span><br><span class="line">                training_x[i*self.mini_batch_size: (i+<span class="number">1</span>)*self.mini_batch_size],</span><br><span class="line">                self.y:</span><br><span class="line">                training_y[i*self.mini_batch_size: (i+<span class="number">1</span>)*self.mini_batch_size]</span><br><span class="line">            &#125;)</span><br><span class="line">        validate_mb_accuracy = theano.function(</span><br><span class="line">            [i], self.layers[<span class="number">-1</span>].accuracy(self.y),</span><br><span class="line">            givens=&#123;</span><br><span class="line">                self.x:</span><br><span class="line">                validation_x[i*self.mini_batch_size: (i+<span class="number">1</span>)*self.mini_batch_size],</span><br><span class="line">                self.y:</span><br><span class="line">                validation_y[i*self.mini_batch_size: (i+<span class="number">1</span>)*self.mini_batch_size]</span><br><span class="line">            &#125;)</span><br><span class="line">        test_mb_accuracy = theano.function(</span><br><span class="line">            [i], self.layers[<span class="number">-1</span>].accuracy(self.y),</span><br><span class="line">            givens=&#123;</span><br><span class="line">                self.x:</span><br><span class="line">                test_x[i*self.mini_batch_size: (i+<span class="number">1</span>)*self.mini_batch_size],</span><br><span class="line">                self.y:</span><br><span class="line">                test_y[i*self.mini_batch_size: (i+<span class="number">1</span>)*self.mini_batch_size]</span><br><span class="line">            &#125;)</span><br><span class="line">        self.test_mb_predictions = theano.function(</span><br><span class="line">            [i], self.layers[<span class="number">-1</span>].y_out,</span><br><span class="line">            givens=&#123;</span><br><span class="line">                self.x:</span><br><span class="line">                test_x[i*self.mini_batch_size: (i+<span class="number">1</span>)*self.mini_batch_size]</span><br><span class="line">            &#125;)</span><br><span class="line">        <span class="comment"># Do the actual training</span></span><br><span class="line">        best_validation_accuracy = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> xrange(epochs):</span><br><span class="line">            <span class="keyword">for</span> minibatch_index <span class="keyword">in</span> xrange(num_training_batches):</span><br><span class="line">                iteration = num_training_batches*epoch+minibatch_index</span><br><span class="line">                <span class="keyword">if</span> iteration % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">                    print(<span class="string">&quot;Training mini-batch number &#123;0&#125;&quot;</span>.format(iteration))</span><br><span class="line">                cost_ij = train_mb(minibatch_index)</span><br><span class="line">                <span class="keyword">if</span> (iteration+<span class="number">1</span>) % num_training_batches == <span class="number">0</span>:</span><br><span class="line">                    validation_accuracy = np.mean(</span><br><span class="line">                        [validate_mb_accuracy(j) <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_validation_batches)])</span><br><span class="line">                    print(<span class="string">&quot;Epoch &#123;0&#125;: validation accuracy &#123;1:.2%&#125;&quot;</span>.format(</span><br><span class="line">                        epoch, validation_accuracy))</span><br><span class="line">                    <span class="keyword">if</span> validation_accuracy &gt;= best_validation_accuracy:</span><br><span class="line">                        print(<span class="string">&quot;This is the best validation accuracy to date.&quot;</span>)</span><br><span class="line">                        best_validation_accuracy = validation_accuracy</span><br><span class="line">                        best_iteration = iteration</span><br><span class="line">                        <span class="keyword">if</span> test_data:</span><br><span class="line">                            test_accuracy = np.mean(</span><br><span class="line">                                [test_mb_accuracy(j) <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_test_batches)])</span><br><span class="line">                            print(<span class="string">&#x27;The corresponding test accuracy is &#123;0:.2%&#125;&#x27;</span>.format(</span><br><span class="line">                                test_accuracy))</span><br><span class="line">        print(<span class="string">&quot;Finished training network.&quot;</span>)</span><br><span class="line">        print(<span class="string">&quot;Best validation accuracy of &#123;0:.2%&#125; obtained at iteration &#123;1&#125;&quot;</span>.format(</span><br><span class="line">            best_validation_accuracy, best_iteration))</span><br><span class="line">        print(<span class="string">&quot;Corresponding test accuracy of &#123;0:.2%&#125;&quot;</span>.format(test_accuracy))</span><br><span class="line"></span><br><span class="line"><span class="comment">#### Define layer types</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConvPoolLayer</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Used to create a combination of a convolutional and a max-pooling</span></span><br><span class="line"><span class="string">    layer.  A more sophisticated implementation would separate the</span></span><br><span class="line"><span class="string">    two, but for our purposes we&#x27;ll always use them together, and it</span></span><br><span class="line"><span class="string">    simplifies the code, so it makes sense to combine them.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, filter_shape, image_shape, poolsize=(<span class="params"><span class="number">2</span>, <span class="number">2</span></span>),</span></span></span><br><span class="line"><span class="function"><span class="params">                 activation_fn=sigmoid</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;`filter_shape` is a tuple of length 4, whose entries are the number</span></span><br><span class="line"><span class="string">        of filters, the number of input feature maps, the filter height, and the</span></span><br><span class="line"><span class="string">        filter width.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `image_shape` is a tuple of length 4, whose entries are the</span></span><br><span class="line"><span class="string">        mini-batch size, the number of input feature maps, the image</span></span><br><span class="line"><span class="string">        height, and the image width.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `poolsize` is a tuple of length 2, whose entries are the y and</span></span><br><span class="line"><span class="string">        x pooling sizes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.filter_shape = filter_shape</span><br><span class="line">        self.image_shape = image_shape</span><br><span class="line">        self.poolsize = poolsize</span><br><span class="line">        self.activation_fn=activation_fn</span><br><span class="line">        <span class="comment"># initialize weights and biases</span></span><br><span class="line">        n_out = (filter_shape[<span class="number">0</span>]*np.prod(filter_shape[<span class="number">2</span>:])/np.prod(poolsize))</span><br><span class="line">        self.w = theano.shared(</span><br><span class="line">            np.asarray(</span><br><span class="line">                np.random.normal(loc=<span class="number">0</span>, scale=np.sqrt(<span class="number">1.0</span>/n_out), size=filter_shape),</span><br><span class="line">                dtype=theano.config.floatX),</span><br><span class="line">            borrow=<span class="literal">True</span>)</span><br><span class="line">        self.b = theano.shared(</span><br><span class="line">            np.asarray(</span><br><span class="line">                np.random.normal(loc=<span class="number">0</span>, scale=<span class="number">1.0</span>, size=(filter_shape[<span class="number">0</span>],)),</span><br><span class="line">                dtype=theano.config.floatX),</span><br><span class="line">            borrow=<span class="literal">True</span>)</span><br><span class="line">        self.params = [self.w, self.b]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_inpt</span>(<span class="params">self, inpt, inpt_dropout, mini_batch_size</span>):</span></span><br><span class="line">        self.inpt = inpt.reshape(self.image_shape)</span><br><span class="line">        conv_out = conv.conv2d(</span><br><span class="line">            input=self.inpt, filters=self.w, filter_shape=self.filter_shape,</span><br><span class="line">            image_shape=self.image_shape)</span><br><span class="line">        pooled_out = pool.pool_2d(</span><br><span class="line">            input=conv_out, ws=self.poolsize, ignore_border=<span class="literal">True</span>)</span><br><span class="line">        self.output = self.activation_fn(</span><br><span class="line">            pooled_out + self.b.dimshuffle(<span class="string">&#x27;x&#x27;</span>, <span class="number">0</span>, <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;x&#x27;</span>))</span><br><span class="line">        self.output_dropout = self.output <span class="comment"># no dropout in the convolutional layers</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FullyConnectedLayer</span>(<span class="params">object</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_in, n_out, activation_fn=sigmoid, p_dropout=<span class="number">0.0</span></span>):</span></span><br><span class="line">        self.n_in = n_in</span><br><span class="line">        self.n_out = n_out</span><br><span class="line">        self.activation_fn = activation_fn</span><br><span class="line">        self.p_dropout = p_dropout</span><br><span class="line">        <span class="comment"># Initialize weights and biases</span></span><br><span class="line">        self.w = theano.shared(</span><br><span class="line">            np.asarray(</span><br><span class="line">                np.random.normal(</span><br><span class="line">                    loc=<span class="number">0.0</span>, scale=np.sqrt(<span class="number">1.0</span>/n_out), size=(n_in, n_out)),</span><br><span class="line">                dtype=theano.config.floatX),</span><br><span class="line">            name=<span class="string">&#x27;w&#x27;</span>, borrow=<span class="literal">True</span>)</span><br><span class="line">        self.b = theano.shared(</span><br><span class="line">            np.asarray(np.random.normal(loc=<span class="number">0.0</span>, scale=<span class="number">1.0</span>, size=(n_out,)),</span><br><span class="line">                       dtype=theano.config.floatX),</span><br><span class="line">            name=<span class="string">&#x27;b&#x27;</span>, borrow=<span class="literal">True</span>)</span><br><span class="line">        self.params = [self.w, self.b]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_inpt</span>(<span class="params">self, inpt, inpt_dropout, mini_batch_size</span>):</span></span><br><span class="line">        self.inpt = inpt.reshape((mini_batch_size, self.n_in))</span><br><span class="line">        self.output = self.activation_fn(</span><br><span class="line">            (<span class="number">1</span>-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)</span><br><span class="line">        self.y_out = T.argmax(self.output, axis=<span class="number">1</span>)</span><br><span class="line">        self.inpt_dropout = dropout_layer(</span><br><span class="line">            inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)</span><br><span class="line">        self.output_dropout = self.activation_fn(</span><br><span class="line">            T.dot(self.inpt_dropout, self.w) + self.b)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">accuracy</span>(<span class="params">self, y</span>):</span></span><br><span class="line">        <span class="string">&quot;Return the accuracy for the mini-batch.&quot;</span></span><br><span class="line">        <span class="keyword">return</span> T.mean(T.eq(y, self.y_out))</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SoftmaxLayer</span>(<span class="params">object</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_in, n_out, p_dropout=<span class="number">0.0</span></span>):</span></span><br><span class="line">        self.n_in = n_in</span><br><span class="line">        self.n_out = n_out</span><br><span class="line">        self.p_dropout = p_dropout</span><br><span class="line">        <span class="comment"># Initialize weights and biases</span></span><br><span class="line">        self.w = theano.shared(</span><br><span class="line">            np.zeros((n_in, n_out), dtype=theano.config.floatX),</span><br><span class="line">            name=<span class="string">&#x27;w&#x27;</span>, borrow=<span class="literal">True</span>)</span><br><span class="line">        self.b = theano.shared(</span><br><span class="line">            np.zeros((n_out,), dtype=theano.config.floatX),</span><br><span class="line">            name=<span class="string">&#x27;b&#x27;</span>, borrow=<span class="literal">True</span>)</span><br><span class="line">        self.params = [self.w, self.b]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_inpt</span>(<span class="params">self, inpt, inpt_dropout, mini_batch_size</span>):</span></span><br><span class="line">        self.inpt = inpt.reshape((mini_batch_size, self.n_in))</span><br><span class="line">        self.output = softmax((<span class="number">1</span>-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)</span><br><span class="line">        self.y_out = T.argmax(self.output, axis=<span class="number">1</span>)</span><br><span class="line">        self.inpt_dropout = dropout_layer(</span><br><span class="line">            inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)</span><br><span class="line">        self.output_dropout = softmax(T.dot(self.inpt_dropout, self.w) + self.b)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cost</span>(<span class="params">self, net</span>):</span></span><br><span class="line">        <span class="string">&quot;Return the log-likelihood cost.&quot;</span></span><br><span class="line">        <span class="keyword">return</span> -T.mean(T.log(self.output_dropout)[T.arange(net.y.shape[<span class="number">0</span>]), net.y])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">accuracy</span>(<span class="params">self, y</span>):</span></span><br><span class="line">        <span class="string">&quot;Return the accuracy for the mini-batch.&quot;</span></span><br><span class="line">        <span class="keyword">return</span> T.mean(T.eq(y, self.y_out))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Miscellanea</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">size</span>(<span class="params">data</span>):</span></span><br><span class="line">    <span class="string">&quot;Return the size of the dataset `data`.&quot;</span></span><br><span class="line">    <span class="keyword">return</span> data[<span class="number">0</span>].get_value(borrow=<span class="literal">True</span>).shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout_layer</span>(<span class="params">layer, p_dropout</span>):</span></span><br><span class="line">    srng = shared_randomstreams.RandomStreams(</span><br><span class="line">        np.random.RandomState(<span class="number">0</span>).randint(<span class="number">999999</span>))</span><br><span class="line">    mask = srng.binomial(n=<span class="number">1</span>, p=<span class="number">1</span>-p_dropout, size=layer.shape)</span><br><span class="line">    <span class="keyword">return</span> layer*T.cast(mask, theano.config.floatX)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;写在前面&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;此文是我学习AI入门的笔记。学习教材《neural networks and deep learning》，作者Michael Nielsen。&lt;/li&gt;
&lt;li&gt;这是一本免费的书籍，网址&lt;a href=&quot;http://neuraln</summary>
      
    
    
    
    <category term="network" scheme="http://example.com/categories/network/"/>
    
    
    <category term="network" scheme="http://example.com/tags/network/"/>
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>反向传播算法(backpropagation)计算梯度下降(SGD)详解</title>
    <link href="http://example.com/2020/09/11/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%EF%BC%88backpropagation%EF%BC%89%E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%88SGD%EF%BC%89%E2%80%94%E2%80%94AI%E5%85%A5%E9%97%A8/"/>
    <id>http://example.com/2020/09/11/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%EF%BC%88backpropagation%EF%BC%89%E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%88SGD%EF%BC%89%E2%80%94%E2%80%94AI%E5%85%A5%E9%97%A8/</id>
    <published>2020-09-11T08:35:00.000Z</published>
    <updated>2021-06-07T01:33:37.947Z</updated>
    
    <content type="html"><![CDATA[<h1>写在前面</h1><ol><li>此文是我学习AI入门的笔记。学习教材《neural networks and deep learning》，作者Michael Nielsen。</li><li>这是一本免费的书籍，网址在<a href="http://neuralnetworksanddeeplearning.com/">这里</a></li><li>这是我对第二章的反向传播算法的总结笔记，请看完第二章之后再来看这篇文章。</li><li>第一章的学习笔记在<a href="https://blog.csdn.net/Anastasiawangyx/article/details/102649158">这里</a>，会对理解此篇有帮助。</li><li>初学者入门， 可能会有错误，请大家指正。</li></ol><h2 id="概念准备">概念准备</h2><ol><li><strong>带权输入Z</strong></li></ol><blockquote><p>z^l^=w^l^a^l^^-^^1^+b^l^，其中l为层数</p></blockquote><ol start="2"><li><strong>激活值a</strong></li></ol><blockquote><p>a^l^=σ（z^l^），其中σ为S型神经元的输出函数</p></blockquote><ol start="3"><li><strong>误差</strong></li></ol><blockquote><p>δ^L^~j~=$\frac{\partial C}{\partial z}$</p></blockquote><h2 id="四个基本方程">四个基本方程</h2><p><img src="https://img-blog.csdnimg.cn/20191026111307930.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><ol><li><strong>输出层误差方程BP1</strong><br>其中右边第一项是二次代价函数对激活值的偏导数，由二次代价函数的表达式可知此式还可以写成这个形式<br><img src="https://img-blog.csdnimg.cn/20191026111850662.JPG" alt="在这里插入图片描述"></li><li><strong>使用上一层的误差δ^L^^+^^1^来表示当前层的误差δ^L^方程BP2</strong><br>我们可以把矩阵的转置近似于看做反向移动，它提供给了我们通过l层的误差反向传递回来给第l-1层的误差的计算方法。</li><li><strong>代价函数关于网络中任意偏置的改变率方程BP3</strong><br>给了我们由误差计算偏置改变率的方法。注意，此处体现出了反向传播算法的优点，可以<strong>同时计算所有的偏置改变率</strong>，而不用单独计算每一个的改变率。</li><li><strong>代价函数关于网络中任意权重的改变率方程BP4</strong><br>给了我们由误差计算偏置改变率的方法，同样提高了运算效率。</li></ol><h1>代码分析</h1><p>注：<strong>英文注解</strong>为本书原本的文档注解，<strong>中文注解</strong>是我的注解。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span>(<span class="params">object</span>):</span></span><br><span class="line">...</span><br><span class="line"><span class="string">&quot;&quot;&quot;backprop函数实现反向传播算法&quot;&quot;&quot;</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">backprop</span>(<span class="params">self, x, y</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return a tuple &quot;(nabla_b, nabla_w)&quot; representing the</span></span><br><span class="line"><span class="string">        gradient for the cost function C_x.  &quot;nabla_b&quot; and</span></span><br><span class="line"><span class="string">        &quot;nabla_w&quot; are layer-by-layer lists of numpy arrays, similar</span></span><br><span class="line"><span class="string">        to &quot;self.biases&quot; and &quot;self.weights&quot;.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment">#初始化b,w的偏导数，得到相应的结构，数值均为0</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="comment"># feedforward</span></span><br><span class="line">        <span class="comment">#第一层的激活值等于输入值</span></span><br><span class="line">        activation = x</span><br><span class="line">        activations = [x] <span class="comment"># list to store all the activations, layer by layer</span></span><br><span class="line">        zs = [] <span class="comment"># list to store all the z vectors, layer by layer</span></span><br><span class="line">        <span class="comment">#依次计算每层的带权输入和激活值</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">            z = np.dot(w, activation)+b</span><br><span class="line">            zs.append(z)</span><br><span class="line">            activation = sigmoid(z)</span><br><span class="line">            activations.append(activation)</span><br><span class="line">        <span class="comment"># backward pass</span></span><br><span class="line">        <span class="comment">#输出层误差delta</span></span><br><span class="line">        delta = self.cost_derivative(activations[<span class="number">-1</span>], y) * \</span><br><span class="line">            sigmoid_prime(zs[<span class="number">-1</span>])</span><br><span class="line">        <span class="comment">#使用公式BP3和BP4，由误差得到b和w的偏导</span></span><br><span class="line">        nabla_b[<span class="number">-1</span>] = delta</span><br><span class="line">        nabla_w[<span class="number">-1</span>] = np.dot(delta, activations[<span class="number">-2</span>].transpose())</span><br><span class="line">        <span class="comment"># Note that the variable l in the loop below is used a little</span></span><br><span class="line">        <span class="comment"># differently to the notation in Chapter 2 of the book.  Here,</span></span><br><span class="line">        <span class="comment"># l = 1 means the last layer of neurons, l = 2 is the</span></span><br><span class="line">        <span class="comment"># second-last layer, and so on.  It&#x27;s a renumbering of the</span></span><br><span class="line">        <span class="comment"># scheme in the book, used here to take advantage of the fact</span></span><br><span class="line">        <span class="comment"># that Python can use negative indices in lists.</span></span><br><span class="line">        <span class="comment">#反向传播，得到每一层的误差，再得到每一层的偏导</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> xrange(<span class="number">2</span>, self.num_layers):</span><br><span class="line">            z = zs[-l]</span><br><span class="line">            sp = sigmoid_prime(z)</span><br><span class="line">            delta = np.dot(self.weights[-l+<span class="number">1</span>].transpose(), delta) * sp</span><br><span class="line">            nabla_b[-l] = delta</span><br><span class="line">            nabla_w[-l] = np.dot(delta, activations[-l<span class="number">-1</span>].transpose())</span><br><span class="line">        <span class="comment">#返回本次迭代得到的b和w</span></span><br><span class="line">        <span class="keyword">return</span> (nabla_b, nabla_w)</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"><span class="comment">#二次代价函数对激活值a求导</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cost_derivative</span>(<span class="params">self, output_activations, y</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return the vector of partial derivatives \partial C_x /</span></span><br><span class="line"><span class="string">        \partial a for the output activations.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> (output_activations-y) </span><br><span class="line"><span class="comment">#z型神经元的输出函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">z</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;The sigmoid function.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span>+np.exp(-z))</span><br><span class="line"><span class="comment">#输出函数的导数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_prime</span>(<span class="params">z</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Derivative of the sigmoid function.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(z)*(<span class="number">1</span>-sigmoid(z))</span><br></pre></td></tr></table></figure><h1>总结</h1><h2 id="1-反向传播算法描述">1.反向传播算法描述</h2><ol><li><strong>输入x</strong>：为输入层设置对应的激活值a^1^</li><li><strong>向前传播</strong>：对每个l=2，3，…，L，计算相同的带权输入和激活值</li><li><strong>输出层误差</strong>：由公式BP1得到输出层误差</li><li><strong>反向传播误差</strong>：由公式BP2，计算每一层的误差。</li><li><strong>输出</strong>：代价函数的梯度由BP3和BP4两个公式得出。</li></ol><h2 id="2-对给定一个大小为m的mini-batch，计算相应的梯度">2.对给定一个大小为m的mini-batch，计算相应的梯度</h2><ol><li><strong>输入训练样本的集合</strong></li><li><strong>对每个训练样本x</strong>:设置对应的输入激活a，并执行反向传播算法。</li><li><strong>梯度下降</strong>：对每个l=2,3,…,L，根据对上述m个样本求平均值后，依次更新每层的权重和偏置。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;写在前面&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;此文是我学习AI入门的笔记。学习教材《neural networks and deep learning》，作者Michael Nielsen。&lt;/li&gt;
&lt;li&gt;这是一本免费的书籍，网址在&lt;a href=&quot;http://neural</summary>
      
    
    
    
    <category term="network" scheme="http://example.com/categories/network/"/>
    
    
    <category term="network" scheme="http://example.com/tags/network/"/>
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>改善神经网络的学习方法（交叉熵函数，过度拟合，规范化，初始化权重）</title>
    <link href="http://example.com/2020/09/11/2020-09-11-%E6%94%B9%E5%96%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%A4%E5%8F%89%E7%86%B5%E5%87%BD%E6%95%B0%EF%BC%8C%E8%BF%87%E5%BA%A6%E6%8B%9F%E5%90%88%EF%BC%8C%E8%A7%84%E8%8C%83%E5%8C%96%EF%BC%8C%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9D%83%E9%87%8D%EF%BC%89/"/>
    <id>http://example.com/2020/09/11/2020-09-11-%E6%94%B9%E5%96%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%A4%E5%8F%89%E7%86%B5%E5%87%BD%E6%95%B0%EF%BC%8C%E8%BF%87%E5%BA%A6%E6%8B%9F%E5%90%88%EF%BC%8C%E8%A7%84%E8%8C%83%E5%8C%96%EF%BC%8C%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9D%83%E9%87%8D%EF%BC%89/</id>
    <published>2020-09-11T08:34:00.000Z</published>
    <updated>2020-10-03T12:39:28.899Z</updated>
    
    <content type="html"><![CDATA[<h1>写在前面</h1><ol><li>此文是我学习ML入门的笔记。学习教材《neural networks and deep learning》，作者Michael Nielsen。</li><li>这是一本免费的书籍，网址<a href="http://neuralnetworksanddeeplearning.com/">在这里</a>。</li><li>此文是第三章的学习笔记。</li><li>初学者入门， 可能会有错误，请大家指正。</li><li><a href="https://blog.csdn.net/Anastasiawangyx/article/details/102649158">第一章笔记</a>和<a href="https://blog.csdn.net/Anastasiawangyx/article/details/102753879">第二章笔记</a>的链接。<br>———————————————————————————————————</li></ol><h1>为什么要改善？</h1><p>要改善我们之前的神经网络，是因为它存在着以下几个问题：</p><h2 id="1-在一定条件下学习缓慢。">1. 在一定条件下学习缓慢。</h2><p>在我们之前用二次代价函数、随机梯度下降和反向传播算法定义的神经网络中，网络的学习速率主要取决于<strong>权重的偏导数</strong>，其表达式为</p><p><img src="https://img-blog.csdnimg.cn/20191102224201707.JPG" alt="权重偏导数"></p><p>因此权重的偏导数正比于<strong>S型函数的一阶导数</strong>。而其一阶导数的图像如下所示</p><p><img src="https://img-blog.csdnimg.cn/20191102224449106.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="S型函数的导数图像"></p><p>可以看出，当z的值很大或者很小的时候，学习速率会非常缓慢。</p><h2 id="2-过度拟合（overfitting）-过度训练（overtraining）">2. 过度拟合（overfitting）/过度训练（overtraining）</h2><p>如何理解过度拟合呢？</p><p>打一个简单的比方：你很喜欢吃土豆丝这道菜，所以妈妈会你如何把土豆去皮切成丝。你每天很勤奋的练习，最后可以切出粗细均匀的完美的土豆丝了。<br>然后有一天你吃腻了土豆丝，想吃胡萝卜。<br>但是你看着眼前的胡萝卜，不知道该如何下刀。<br>因为，切丝这个学习行为，你只能把它运用在土豆这个<strong>特定的训练对象</strong>上，而不能<strong>泛化(generalization)</strong> 到其他诸如胡萝卜等的身上，这样的学习是非常无效的。</p><p>回到神经网络。<br>我的理解，过度拟合指的是<strong>训练得出的权重、偏置只能在训练数据上得到很好的表现，而在真实数据（如测试数据、验证集）上则无法体现出学习的行为。</strong><br>过度拟合常常发生在<strong>拥有大量自由参数</strong>的模型上，比如神经网络。</p><p>举个例子。<br>我们只使用前1000个训练数据，设置400个迭代期，其他条件不变，则得到的代价函数看起来还不错。<br><img src="https://img-blog.csdnimg.cn/2019110313434342.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="训练集"></p><p>但分类准确率在测试集上的表现，就…<br><img src="https://img-blog.csdnimg.cn/20191103134807526.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="测试集准确率"><br>过度拟合现象发生了。</p><h2 id="3-权重初始化">3.权重初始化</h2><p>在前面的学习中，我们对权重的初始化一直是用<strong>高斯随机变量</strong>，使其被归一化为均值为0，标准差为1。而这种情况下，带权输入z的值会如下分布<br><img src="https://img-blog.csdnimg.cn/20191103140436426.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>可以看出，z的绝对值会很大，所以神经元会趋向<strong>饱和</strong>。<br>———————————————————————————————————</p><h1>如何改善？</h1><h2 id="1-交叉熵代价函数（cross-entropy-cost-function）">1.交叉熵代价函数（cross-entropy cost function）</h2><p>对于单个如下形式的神经元<br><img src="https://img-blog.csdnimg.cn/20191103141740224.png" alt="在这里插入图片描述"></p><p>其对应的交叉熵代价函数为<br><img src="https://img-blog.csdnimg.cn/20191103141856584.JPG" alt="在这里插入图片描述"><br>其中 n 是训练数据的总数，求和是在所有的训练输⼊ x 上进⾏的，y 是对应的⽬标输出。<br>而由此计算出的权重的偏导数的形式为<br><img src="https://img-blog.csdnimg.cn/20191103143113849.JPG" alt="在这里插入图片描述"><br>由此避免了学习速率下降的问题。</p><p>定义在<strong>神经网络</strong>上的交叉熵函数的形式为：<br><img src="https://img-blog.csdnimg.cn/20191103144048460.JPG" alt="在这里插入图片描述"><br>可以看成在所有的输出神经元上，对上上式求和。</p><p>其权重的偏导数形式为<br><img src="https://img-blog.csdnimg.cn/20191103145211408.JPG" alt="在这里插入图片描述"><br>很好的避免了学习速率下降的问题。</p><h2 id="2-柔性最大值（softmax）">2.柔性最大值（softmax）</h2><p>柔性最大值的想法是为神经网络定义一种新式的输出层。刚开始在输入层和隐藏层上都是一样的，但是在输出层上应用一种<strong>柔性最大值函数</strong>作用在带权输入zjL上。根据这个函数，第j个神经元的激活值ajL就是<br><img src="https://img-blog.csdnimg.cn/2019110316302323.JPG" alt="在这里插入图片描述"><br>其中，分母是对所有输出层上的神经元的求和。因此输出层上<strong>所有激活值和为1.</strong> 换言之，柔性最大层的输出可以用看作是一个<strong>概率分布</strong>。</p><h2 id="3-对数似然代价函数（log-likelihood-cost-function）">3.对数似然代价函数（log-likelihood cost function）</h2><p>我们使⽤ x 表⽰⽹络的训练输⼊，y 表⽰对应的⽬标输出。然后关<br>联这个训练输⼊的对数似然代价函数就是<br><img src="https://img-blog.csdnimg.cn/20191103164239993.JPG" alt="在这里插入图片描述"><br>通过代数运算我们可以得到<br><img src="https://img-blog.csdnimg.cn/20191103164445572.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>运用柔性最大值函数与对数似然代价函数的结合，也可以很好的解决学习速率下降的问题。</p><h2 id="4-提前停止（early-stopping）和hold-out方法">4.提前停止（early stopping）和hold-out方法</h2><p>我们之前定义过一个<strong>validation_data（验证集）</strong>，它是从测试集中单独拿出的一批数据，我们还没有使用过它。</p><p>现在我们使用验证集来解决过度拟合问题：</p><p>在每轮迭代期之后，我们使用validation_data来测试其分类准确率，一旦准确率达到饱和，我们就停止训练。这个策略叫做<strong>提前停止</strong>。</p><p>当然，实际应⽤中，我们不会⽴即知道什么时候准确率会饱和。相反，我们会⼀直训练直到我们确信准确率已经饱和。</p><p>就是为何⽤ validation_data 取代 test_data 来设置更好的超参数？因为如果我们设置超参数是基于 test_data 的话，可能最终我们就会得到过度拟合于test_data 的超参数。也就是说，我们可能会找到那些符合test_data 特点的超参数，但是<strong>⽹络的性能并不能够泛化到其他数据集合上</strong>。我们借助 validation_data 来克服这个问题。然后⼀旦获得了想要的超参数，最终我们就使⽤ test_data 进⾏准确率测量。<br>换⾔之，你可以将验证集看成是⼀种特殊的训练数据集能够帮助我们学习好的超参数。这种寻找好的超参数的⽅法有时候被称为 <strong>hold out ⽅法</strong>，因为 validation_data是从traning_data 训练集中留出或者“拿出”的⼀部分。</p><h2 id="5-L2规范化（regularization）">5. L2规范化（regularization）</h2><p>所谓的L2规范化，就是在代价函数上增加一个额外的项，这个项叫<strong>规范项</strong>。下面以交叉熵代价函数举例说明。<br>规范化后的交叉熵代价函数为<br><img src="https://img-blog.csdnimg.cn/2019110320264041.JPG" alt="在这里插入图片描述"><br>其中第⼀个项就是常规的交叉熵的表达式。第⼆个现在加⼊的就是<strong>所有权重的平⽅的和</strong>。然后使⽤⼀个因⼦ λ/2n 进⾏量化调整，其中 λ &gt; 0 可以称为<strong>规范化参数</strong>，⽽ n 就是训练集合的大小。<br>在这种情况下，求得的权重和偏置的偏导数为<br><img src="https://img-blog.csdnimg.cn/20191103203444593.JPG" alt="在这里插入图片描述"><br>因此每次更新权重时的公式为<br><img src="https://img-blog.csdnimg.cn/20191103203520414.JPG" alt="在这里插入图片描述"><br>这和通常的梯度下降的规则相似，除了通过一个因子（1−ηλ÷n1-ηλ \div n1−<em>η**λ</em>÷<em>n</em>）调整了权重w。这种做法有时也叫做权重下降（<strong>weights decay</strong>），因为它使权重变得更小。</p><h3 id="为什么更小的权重可以减轻过度拟合？">为什么更小的权重可以减轻过度拟合？</h3><p>我的理解</p><ol><li>小的权重在某种程度上，意味着更低的复杂性，也就对数据做出了一种更简单，更强大的解释。</li><li>更小的权重意味着网络的行为不会因为我们随便改变了一个输入而改变太大，因此能够抵抗训练数据中噪声的影响。</li><li>小权重神经网络给出的是一个更“一般”而“通用”的解释，能够根据已经学到的内容进行更好的泛化。</li></ol><h2 id="6-L1规范化">6. L1规范化</h2><p>与L2规范化类似，L1规范化是在在未规范化的代价函数上加上⼀个权重绝对值的和：<br><img src="https://img-blog.csdnimg.cn/20191103205038212.JPG" alt="在这里插入图片描述"><br>在两种情形下，规范化的效果就是缩小权重。但权重缩小的方式不同。</p><ul><li>在 L1 规范化中，权重通过⼀个常量向 0 进行缩小。</li><li>在 L2 规范化中，权重通过⼀个和 w 成比例的量进行缩小的。</li><li>所以，当⼀个特定的权重绝对值 |w| 很⼤时，L1 规范化的权重缩⼩得远⽐ L2 规范化要⼩得多。</li><li>相反，当⼀个特定的权重绝对值 |w| 很⼩时，L1 规范化的权重缩⼩得要⽐ L2 规范化⼤得多。</li><li>最终的结果就是：L1 规范化倾向于聚集⽹络的权重在相对少量的⾼重要度连接上，⽽其他权重就会被驱使向 0 接近。</li></ul><h2 id="7-弃权（dropout）">7.弃权（dropout）</h2><p><img src="https://img-blog.csdnimg.cn/20191103210038717.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>原理：随机删除一半的隐藏神经元，用这个‘’新‘’的网络进行学习。</p><p>注：输入层和输出层不变。</p><p><strong>理解</strong>：</p><ul><li>弃权，是一种构造大量不同神经网络的过程，让数据在不同的网络中学习，最后求平均值。</li><li>确保模型对一部分证据丢失健壮。</li></ul><p><strong>注：</strong></p><ol><li>输入层和输出层不变</li><li>最后运用实际整个神经网络时，隐藏神经元的权重要减半。</li></ol><h2 id="8-人为扩展训练数据（Artificially-expanding-the-training-data）">8.人为扩展训练数据（Artificially expanding the training data）</h2><p>很容易理解，神经网络的训练数据越多，网络能够学习到的变化就越大，性能自然就越好，但是在真实状态下，<strong>好的训练数据是很难拿到的。</strong></p><p>但是我们可以<strong>人为扩展训练数据</strong>。</p><p>以手写识别数字为例。</p><p>我们的训练数据是MNIST训练图像<br><img src="https://imgconvert.csdnimg.cn/aHR0cDovL25ldXJhbG5ldHdvcmtzYW5kZGVlcGxlYXJuaW5nLmNvbS9pbWFnZXMvbW9yZV9kYXRhXzUucG5n?x-oss-process=image/format,png" alt="在这里插入图片描述"></p><p>将其进行<strong>旋转</strong>，比如说15度。<br><img src="http://neuralnetworksanddeeplearning.com/images/more_data_rotated_5.png" alt="img"></p><p>这还是会被设别为同样的数字的。但是在<strong>像素层级</strong>这和任何⼀幅在 MNIST 训练数据中的图像都不相同。所以将这样的样本加⼊到训练数据中是很可能帮助我们的⽹络学会更多如何分类数字。<br>不只<strong>旋转</strong>，还可以用<strong>转换</strong>和<strong>扭曲</strong>图像来扩展训练数据。</p><h2 id="9-权重初始化">9.权重初始化</h2><p>我们选用一种新的方法初始化权重。</p><p>假设我们有⼀个有 n个输⼊权重的神经元。我们会使⽤<strong>均值为 0 标准差为 1/√n</strong>的⾼斯随机分布初始化这些权重。</p><p>也就是说，我们会向下挤压⾼斯分布，让我们的神经元更不可能饱和。</p><p><img src="https://img-blog.csdnimg.cn/20191103234259338.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>这样的⼀个神经元<strong>更不可能饱和</strong>，因此也不⼤可能遇到学习速度下降的问题。</p><p>———————————————————————————————————</p><h1>如何选择神经网络的超参数（hyper-paramets）？</h1><h2 id="宽泛策略（broad-strategy）"><em>宽泛策略（broad strategy）</em></h2><h3 id="1-简化需要解决的问题">1.简化需要解决的问题</h3><p>丢开训练和验证集合中的那些除了 0 和 1的那些图像。然后试着训练⼀个⽹络来区分 0 和 1。不仅仅问题⽐ 10 个分类的情况简化了，同样也会减少 80% 的训练数据，这样就给出了 5 倍的加速。</p><h3 id="2-简化网络来加速实验">2.简化网络来加速实验</h3><p>如果你相信 [784, 10] 的⽹络更可能⽐随机更加好的分类效果，那么就从这个⽹络开始实验。这会比训练⼀个 [784, 30, 10] 的⽹络更快，你可以进⼀步尝试后⼀个。</p><h3 id="3-减少验证图像数">3.减少验证图像数</h3><p>减少每一轮迭代期的验证图像数，能够通过更加频繁的监控准确率而获得反馈。</p><p>下面对具体的参数进行分析。</p><h2 id="学习速率">学习速率</h2><p>首先要清楚，我们使用随机梯度下降算法的目的，是希望我们能够逐渐抵达<strong>代价函数谷底</strong>的。<br><img src="https://imgconvert.csdnimg.cn/aHR0cDovL25ldXJhbG5ldHdvcmtzYW5kZGVlcGxlYXJuaW5nLmNvbS9pbWFnZXMvdGlrejMzLnBuZw?x-oss-process=image/format,png" alt="在这里插入图片描述"></p><ul><li>学习速率太小：会导致学习速率过慢，造成浪费。</li><li>学习速率过大：可能会导致<strong>算法在接近最小值的时候又越过了谷底</strong>，造成震荡。<br>那应该如何设置学习速率呢？</li></ul><ol><li><p>我们选择在训练数据上的代价<strong>立即开始下降</strong>而非震荡或者增加时作为 η 的阈值的估计。</p><p>因为此时我们还在“山坡上”，还未到达谷底。</p></li><li><p>如果代价在训练的前⾯若⼲回合开始下降，你就可以逐步地尝试增加 η，直到你找到⼀个 η 的值使得在开始若⼲回合代价就开始震荡或者增加。</p></li></ol><p>为了避免疑惑，我将英文原版书中的原句放在此处。</p><blockquote><p>With this picture in mind, we can set η as follows. First, we estimate the threshold value for η at which the cost on the training data immediately begins decreasing, instead of oscillating or increasing. This estimate doesn’t need to be too accurate. You can estimate the order of magnitude by starting with η=0.01. If the cost decreases during the first few epochs, then you should successively try η=0.1,1.0,… until you find a value for η where the cost oscillates or increases during the first few epochs. Alternately, if the cost oscillates or increases during the first few epochs when η=0.01, then try η=0.001,0.0001,… until you find a value for η where the cost decreases during the first few epochs. Following this procedure will give us an order of magnitude estimate for the threshold value of η. You may optionally refine your estimate, to pick out the largest value of η at which the cost decreases during the first few epochs, say η=0.5 or η=0.2 (there’s no need for this to be super-accurate). This gives us an estimate for the threshold value of η.</p></blockquote><h2 id="使用提前停止来确定训练的迭代期数量">使用提前停止来确定训练的迭代期数量</h2><p>正如我们在前⾯讨论的那样，提前停止表⽰在每个迭代期的最后，我们都要计算验证集上的分类准确率。当准确率不再提升，就终止它。这让选择迭代期数变得很简单。</p><h2 id="可变学习速率">可变学习速率</h2><p>我们⼀直都将学习速率设置为常量。但是，通常采⽤<strong>可变的学习速率</strong>更加有<br>效。在学习的前期，权重可能⾮常糟糕。所以最好是使⽤⼀个<strong>较大的学习速率让权重变化得更快</strong>。越往后，我们可以降低学习速率，这样可以作出<strong>更加精良的调整</strong>。<br>关于如何设置可变学习速率，⼀种观点是使用提前终止的想法。就是保持学习速率为⼀个常量知道验证准确率开始变差。然后按照某个量下降学习速率，比如说按照 10 或者 2。我们重复此过程若⼲次，直到学习速率是初始值的 1/1024（或者 1/1000）。那时就终止。</p><h2 id="规范化参数λ">规范化参数λ</h2><p>书作者建议，开始时不包含规范化（λ = 0.0），<strong>确定 η 的值</strong>。使用确定出来的 η，我们可以使用验证数据来选择好的 λ。从尝试 λ = 1.0 开始，然后根据验证集上的性能按照因子10增加或减少其值。⼀旦我已经找到⼀个好的量级，你可以改进 λ 的值。这⾥搞定后，你就可以返回再重新优化 η。</p><h2 id="小批量数据大小（mini-batch-size）">小批量数据大小（mini-batch size）</h2><p>首先要指出，我们在网络中是使用矩阵技术来对所有在⼩批量数据中的样本同时计算梯度更新，而不是进行单个循环累加。</p><ul><li>size太小：没有办法发挥矩阵的优势。</li><li>size太大：无法频繁快速的更新权重。</li></ul><p>因此我们需要一个折中的方法。</p><p>幸运的是，小批量数据大小的选择其实是相对<strong>独立</strong>的⼀个超参数（网络整体架<br>构外的参数），所以你不需要优化其他参数来寻找好的小批量数据大小。</p><p>所以，使用一个（可以接受的）初始值，然后进行不同小批量数据大小的尝试。画出验证准确率的值随时间（⾮迭代期）变化的图，选择哪个得到最快性能的提升的小批量数据大小。<br>———————————————————————————————————</p><h1>其他技术</h1><h2 id="1-Hessian技术">1.Hessian技术</h2><p>Hessian技术是一个用来<strong>最小化代价函数</strong>的方法。<br>假设 C 是一个有多个参数的函数，w = w1, w2, …，所以 C = C(w)。<br>因此得到<br><img src="https://img-blog.csdnimg.cn/20191104003356887.JPG" alt="在这里插入图片描述"><br>其中 ∇C 是通常的梯度向量，H就是Hessian矩阵。<br>运用微积分知识，我们可以得到，当代价函数最小化的时候，Δw的取值为<br><strong>∆w = -H-1∇C</strong></p><p>因此一个优化代价函数的方法如下：</p><ul><li>选择开始点，w</li><li>更新 w 到新点 w′ = w -H-1∇C，其中 Hessian H 和 ∇C 是在 w 处计算出来的。</li><li>不断更新……</li></ul><p><strong>缺点</strong>：Hessian 矩阵的太⼤了，因此计算 H-1∇C就变得极其困难。</p><h2 id="2-基于-momentum-的梯度下降">2.基于 momentum 的梯度下降</h2><p>我们引⼊<strong>速度</strong>变量 v = v1, v2, . . .，其中每⼀个对应 wj 变 量。<br>修改梯度下降的更新规则如下：<br><img src="https://img-blog.csdnimg.cn/20191104004520227.JPG" alt="在这里插入图片描述"><br>在这些⽅程中，µ 是⽤来控制<strong>阻碍</strong>或者<strong>摩擦力</strong>的量的超参数。<br>准确地说，你应该将 1 - µ 看成是摩擦力的量。</p><ul><li>当 µ = 1 时，没有摩擦，速度完全由梯度 ∇C 决定。</li><li>若是 µ = 0，就存在很⼤的摩擦，速度⽆法叠加，上述公式就变成了通常的梯<br>度下降。</li><li>在实践中，使⽤ 0 和 1 之间的 µ 值可以给我们避免过量⽽⼜能够叠加速度的好处。</li><li>我们可以使⽤ hold out 验证数据集来选择合适的 µ 值，就像我们之前选择 η 和 λ 那样。<br>———————————————————————————————————</li></ul><h1>人工神经元的其他模型</h1><h2 id="1-tanh神经元">1.tanh神经元</h2><p>表达式：<br><img src="https://img-blog.csdnimg.cn/20191104005038231.JPG" alt="在这里插入图片描述"><br>图像：<br><img src="https://img-blog.csdnimg.cn/20191104005204562.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="2-修正线性神经元（rectified-linear-neuron）">2.修正线性神经元（rectified linear neuron）</h2><p>修正线性神经元（rectified linear neuron）或者<strong>修正线性单元（rectified<br>linear unit）</strong>，简记为 ReLU。输⼊为 x，权重向量为 w，偏置为 b 的 ReLU 神经元的输出是：max(0, w · x + b)</p><p>图像是：<br><img src="https://img-blog.csdnimg.cn/20191104005500786.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuYXN0YXNpYXdhbmd5eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>———————————————————————————————————</p><h1>小总结</h1><p>这一章讲的内容很杂，提到了改善神经网络的很多技术和方法，以及超参数选择的方法。但是要注意的是，至今没有针对超参数设置的“金科玉律”，很有可能花费了大量的时间和精力，设置出来的参数依旧不满意，要做好准备。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;写在前面&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;此文是我学习ML入门的笔记。学习教材《neural networks and deep learning》，作者Michael Nielsen。&lt;/li&gt;
&lt;li&gt;这是一本免费的书籍，网址&lt;a href=&quot;http://neuraln</summary>
      
    
    
    
    <category term="network" scheme="http://example.com/categories/network/"/>
    
    
    <category term="network" scheme="http://example.com/tags/network/"/>
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>手写数字识别MNIST（python3）</title>
    <link href="http://example.com/2020/09/10/2020-09-10-%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%ABMNIST%EF%BC%88python3%EF%BC%89/"/>
    <id>http://example.com/2020/09/10/2020-09-10-%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%ABMNIST%EF%BC%88python3%EF%BC%89/</id>
    <published>2020-09-10T13:35:00.000Z</published>
    <updated>2020-10-09T02:59:05.654Z</updated>
    
    <content type="html"><![CDATA[<h1>写在前面</h1><ol><li><p>教材链接：<a href="http://neuralnetworksanddeeplearning.com/arning">Neural Networks and Deep Learning</a></p></li><li><p>本机系统：macOS Catalina 10.15.6，python3.8.3</p></li><li><p>本博客源代码：<a href="https://github.com/Anastasiawangyx/handwritten_digit_recognition_python3">handwritten_digit_recognition_python3</a></p></li><li><p>python2(也是教材官方的github库)：<a href="https://github.com/mnielsen/neural-networks-and-deep-learning">neural-networks-and-deep-learning</a></p></li><li><p>需要的第三方库：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"><span class="keyword">import</span> pickle</span><br></pre></td></tr></table></figure></li></ol><h1>一、数据导入</h1><p>在<code>mnist_loader.py</code>包中，将data目录下的mnist.pkl.gz压缩包解压缩，并以一定的格式传递给python进行计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span>():</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    将MNIST文件解压并打开，存储到变量中。</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    f = gzip.open(<span class="string">&#x27;data/mnist.pkl.gz&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>)</span><br><span class="line">    training_data, validation_data, test_data = pickle.load(f, encoding=<span class="string">&#x27;bytes&#x27;</span>)</span><br><span class="line">    f.close()</span><br><span class="line">    <span class="keyword">return</span> training_data, validation_data, test_data</span><br></pre></td></tr></table></figure><p>使用<code>gzip.open()</code>命令将.gz文件以二进制只读的格式打开，再用<code>pickle.load()</code>函数将数据存入变量中。</p><p>注意，此时各个变量的格式：</p><ul><li>training_data：元祖的形式（x，y），其中x为一个有50000个元素的numpy矩阵，每一个元素又分别有784个元素，代表28*28的单个MNIST图像。y也是一个有50000个元素的numpy矩阵，每一个元素为MNIST图像所代表的digit数字，范围从0到9.</li><li>validation_data、test_data的形式与上述training_data相同，但只包含10000幅图像。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_wrapper</span>():</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将解压后得到的数据转化成更适合python计算的格式</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">    tr_d, va_d, te_d = load_data()</span><br><span class="line">    training_inputs = [np.reshape(x, (<span class="number">784</span>, <span class="number">1</span>)) <span class="keyword">for</span> x <span class="keyword">in</span> tr_d[<span class="number">0</span>]]</span><br><span class="line">    training_results = [vectorized_result(y) <span class="keyword">for</span> y <span class="keyword">in</span> tr_d[<span class="number">1</span>]]</span><br><span class="line">    training_data = list(zip(training_inputs, training_results))</span><br><span class="line">    validation_inputs = [np.reshape(x, (<span class="number">784</span>, <span class="number">1</span>)) <span class="keyword">for</span> x <span class="keyword">in</span> va_d[<span class="number">0</span>]]</span><br><span class="line">    validation_data = list(zip(validation_inputs, va_d[<span class="number">1</span>]))</span><br><span class="line">    test_inputs = [np.reshape(x, (<span class="number">784</span>, <span class="number">1</span>)) <span class="keyword">for</span> x <span class="keyword">in</span> te_d[<span class="number">0</span>]]</span><br><span class="line">    test_data =list(zip(test_inputs, te_d[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">return</span> training_data, validation_data, test_data</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>转换后各个变量的格式：</p><ul><li><p>training_data：一个长度为50000的<strong>列表</strong>，列表中的每一个元素为元祖（x，y），其中，x是一个包含了输入图片的784维的numpy矩阵，y是一个10维的numpy矩阵，表示该数字的期待输出。</p></li><li><p>validation_data/test_data：一个长度为10000的<strong>列表</strong>，列表中的每一个元素为元祖（x，y），其中，x是一个包含了输入图片的784维的numpy矩阵，y是MNIST图像所代表的digit数字，范围从0到9。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vectorized_result</span>(<span class="params">j</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    :param j: 输入图像表示的数字</span></span><br><span class="line"><span class="string">    :return: </span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    e = np.zeros((<span class="number">10</span>, <span class="number">1</span>))</span><br><span class="line">    e[j] = <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">return</span> e</span><br></pre></td></tr></table></figure><p>一个辅助函数，返回一个10维的numpy矩阵。</p><h1>二、网络学习</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">定义一个神经网络类</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, sizes</span>):</span></span><br><span class="line">        self.num_layers = len(sizes)</span><br><span class="line">        self.sizes = sizes</span><br><span class="line">        self.bias = [np.random.randn(y, <span class="number">1</span>) <span class="keyword">for</span> y <span class="keyword">in</span> sizes[<span class="number">1</span>:]]  <span class="comment"># randn返回的是一个0，1之间的随机小数，格式是y行1列。</span></span><br><span class="line">        self.weights = [np.random.randn(y, x) <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(sizes[:<span class="number">-1</span>], sizes[<span class="number">1</span>:])]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">feedforward</span>(<span class="params">self, a</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        :param a: 神经网络的输入</span></span><br><span class="line"><span class="string">        :return a: 输出层</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.bias, self.weights):</span><br><span class="line">            a = sigmoid(np.dot(w, a) + b)</span><br><span class="line">        <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">SGD</span>(<span class="params">self, training_data, epochs, mini_batch_size, eta, test_data=None</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        随机梯度下降算法，将训练数据分成相同大小的mini_batch，在每个batch不断迭代得到权重和偏置。</span></span><br><span class="line"><span class="string">        :param training_data: 训练数据</span></span><br><span class="line"><span class="string">        :param epochs: 迭代周期</span></span><br><span class="line"><span class="string">        :param mini_batch_size: 一个mini_batch的大小</span></span><br><span class="line"><span class="string">        :param eta: 学习速率</span></span><br><span class="line"><span class="string">        :param test_data: 测试数据</span></span><br><span class="line"><span class="string">        :return: None</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> test_data:</span><br><span class="line">            n_test = len(test_data)</span><br><span class="line">        n = len(training_data)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(epochs):</span><br><span class="line">            random.shuffle(training_data)</span><br><span class="line">            mini_batches = [</span><br><span class="line">                training_data[k:k + mini_batch_size]</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>, n, mini_batch_size)</span><br><span class="line">            ]</span><br><span class="line">            <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</span><br><span class="line">                self.update_mini_batch(mini_batch, eta)</span><br><span class="line">            <span class="keyword">if</span> test_data:</span><br><span class="line">                print(<span class="string">&quot;Epoch &#123;0&#125; :&#123;1&#125;/&#123;2&#125;&quot;</span>.format(j, self.evaluate(test_data), n_test))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">&quot;Epoch &#123;0&#125; complete.&quot;</span>.format(j))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_mini_batch</span>(<span class="params">self, mini_batch, eta</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        在每一个mini_batch内的每一个数据使用反向传播算法，得到权重和偏置的在一个mini_batch内的更新。</span></span><br><span class="line"><span class="string">        :param mini_batch: 一个用来训练网络的数据batch，由测试数据打乱后分割而成。</span></span><br><span class="line"><span class="string">        :param eta: 学习速率</span></span><br><span class="line"><span class="string">        :return: None</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.bias]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:</span><br><span class="line">            delta_nabla_b, delta_nabla_w = self.backprop(x, y)</span><br><span class="line">            nabla_b = [nb + dnb <span class="keyword">for</span> nb, dnb <span class="keyword">in</span> zip(nabla_b, delta_nabla_b)]</span><br><span class="line">            nabla_w = [nw + dnw <span class="keyword">for</span> nw, dnw <span class="keyword">in</span> zip(nabla_w, delta_nabla_w)]</span><br><span class="line">        self.bias = [b - (eta / len(mini_batch) * nb) <span class="keyword">for</span> b, nb <span class="keyword">in</span> zip(self.bias, nabla_b)]</span><br><span class="line">        self.weights = [w - (eta / len(mini_batch) * nw) <span class="keyword">for</span> w, nw <span class="keyword">in</span> zip(self.weights, nabla_w)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backprop</span>(<span class="params">self, x, y</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        反向传播算法，利用四个公式，得到一个单独数据输入时，权重和偏置的改变</span></span><br><span class="line"><span class="string">        :param x: 输入的数据</span></span><br><span class="line"><span class="string">        :param y: 期望输出</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.bias]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        activation = x</span><br><span class="line">        activations = [x]</span><br><span class="line">        zs = []</span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.bias, self.weights):</span><br><span class="line">            z = np.dot(w, activation) + b</span><br><span class="line">            zs.append(z)</span><br><span class="line">            activation = sigmoid(z)</span><br><span class="line">            activations.append(activation)</span><br><span class="line">        delta = self.cost_derivative(activations[<span class="number">-1</span>], y) * sigmoid_prime(zs[<span class="number">-1</span>])</span><br><span class="line">        nabla_b[<span class="number">-1</span>] = delta</span><br><span class="line">        nabla_w[<span class="number">-1</span>] = np.dot(delta, activations[<span class="number">-2</span>].transpose())</span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">2</span>, self.num_layers):</span><br><span class="line">            z = zs[-l]</span><br><span class="line">            sp = sigmoid_prime(z)</span><br><span class="line">            delta = np.dot(self.weights[-l + <span class="number">1</span>].transpose(), delta) * sp</span><br><span class="line">            nabla_b[-l] = delta</span><br><span class="line">            nabla_w[-l] = np.dot(delta, activations[-l - <span class="number">1</span>].transpose())</span><br><span class="line">        <span class="keyword">return</span> nabla_b, nabla_w</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span>(<span class="params">self, test_data</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        得到正确分类的个数</span></span><br><span class="line"><span class="string">        :param test_data: 测试数据</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        test_result = [(np.argmax(self.feedforward(x)), y) <span class="keyword">for</span> x, y <span class="keyword">in</span> test_data]</span><br><span class="line">        <span class="keyword">return</span> sum(int(x == y) <span class="keyword">for</span> x, y <span class="keyword">in</span> test_result)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cost_derivative</span>(<span class="params">self,output_activations,y</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        得到代价函数的导数</span></span><br><span class="line"><span class="string">        :param output_activations: 输出激活值</span></span><br><span class="line"><span class="string">        :param y: 期望输出</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> output_activations-y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">z</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    S型神经元的格式</span></span><br><span class="line"><span class="string">    :param z: 中间值</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span> / (<span class="number">1.0</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_prime</span>(<span class="params">z</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    S型神经元的导数</span></span><br><span class="line"><span class="string">    :param z:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(z) * (<span class="number">1</span> - sigmoid(z))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>每个函数的具体作用都写在了函数前的注释中，此处不再赘述。</p><p>再来看看<code>main.py</code>里面的内容。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> network</span><br><span class="line"><span class="keyword">import</span> mnist_loader</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    print(<span class="string">&quot;begin!&quot;</span>)</span><br><span class="line">    training_data,validation_data,test_data=mnist_loader.load_data_wrapper()</span><br><span class="line">    net=network.Network([<span class="number">784</span>,<span class="number">30</span>,<span class="number">10</span>])</span><br><span class="line">    net.SGD(training_data,<span class="number">30</span>,<span class="number">10</span>,<span class="number">3.0</span>,test_data=test_data)</span><br></pre></td></tr></table></figure><p>main函数是python程序运行的主函数，将剩下两个python程序导入后，给各个data赋值，定义一个[784,30,10]的神经网络，最后调用SGD来完成计算。</p><p>最终结果是，在我的三十个迭代周期中，分辨准确率的最大值达到了95.2%。</p><h1>三、步骤总结</h1><p>使用神经网络视线手写数字识别大概分以下几个步骤。</p><h2 id="1-数据导入">1.数据导入</h2><p>使用一个单独的程序将官方提供的MNIST数据导入python程序，以供我们训练使用。</p><h2 id="2-定义网络">2.定义网络</h2><p>此处使用的是一个[784,30,10]的神经网络，隐藏层只有30个神经元，学习速率3.0，选取30个迭代周期，每个batch的大小设置为10。</p><p>此处的超参数也可以自己设定。</p><h2 id="3-随机梯度下降">3.随机梯度下降</h2><p>将训练数据分成相同大小的mini_batch，在每个batch不断迭代得到权重和偏置，称为随机梯度下降算法。</p><p>具体来说，在每一个迭代周期内：</p><ul><li><p>将training_data打乱，按照mini_batch的步长将training_data分成不同的mini_batchs。</p></li><li><p>在每个mini_batch内，一个一个的输入训练数据，使用<strong>反向传播算法</strong>，将得到的关于权重和偏置的改变累加起来，最后利用公式</p><p>$$w^{l} \rightarrow w^{l}-\frac{\eta}{m} \sum_{x} \delta^{x, l}\left(a^{x, l-1}\right)^{T}$$</p><p>$$b^{l} \rightarrow b^{l}-\frac{\eta}{m} \sum_{x} \delta^{x, l})$$</p><p>获得在一个mini_batch内的权重和偏置的改变。</p></li><li><p>遍历完所有的mini_batch，得到一次迭代后w和b的改变。</p></li><li><p>将test_data输入到刚刚训练好的网络中，得到准确率。</p></li><li><p>继续下一次迭代。</p></li></ul><h2 id="4-反向传播算法">4.反向传播算法</h2><p>对于一个单独的元祖输入（x，y），其中x是一个包含了输入图片的784维的numpy矩阵，y是一个10维的numpy矩阵，表示该数字的期待输出，反向传播算法的具体做法如下：</p><ul><li>第一层的激活值a^1^即为输入x。</li><li>根据公式，使用网络原本的w和b，算出每一层的中间值z和激活值a。</li><li>计算输出层误差：$\delta^{x, L}=\nabla_{a} C_{x} \odot \sigma^{\prime}\left(z^{x, L}\right)$，其中$\nabla_{a} C_{x}=a^{L}-y$</li><li>反向传播，计算其他层的误差：$\delta^{l}=\left(\left(w^{l+1}\right)^{T} \delta^{l+1}\right) \odot \sigma^{\prime}\left(z^{l}\right)$</li><li>得到代价函数的梯度下降公式：$\frac{\partial C}{\partial w_{j k}^{l}}=a_{k}^{l-1} \delta_{j}^{l}$，$\frac{\partial C}{\partial b_{j }^{l}}=\delta_{j}^{l}$</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;写在前面&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;教材链接：&lt;a href=&quot;http://neuralnetworksanddeeplearning.com/arning&quot;&gt;Neural Networks and Deep Learning&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;l</summary>
      
    
    
    
    <category term="network" scheme="http://example.com/categories/network/"/>
    
    
    <category term="network" scheme="http://example.com/tags/network/"/>
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
  </entry>
  
</feed>
